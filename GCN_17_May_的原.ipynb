{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvwNJ0udXw2-",
        "outputId": "276b52bd-af9a-4206-e72b-af1b64a210c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.12.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torchvision.datasets import mnist\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torch import optim\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8SVAIR6yJB6"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import time\n",
        "# t0 = time.clock()\n",
        "\n",
        "import pathlib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "import torch\n",
        "import keras\n",
        "import scipy.io as io\n",
        "from keras import backend as K\n",
        "from keras import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import regularizers\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import Linear, ReLU\n",
        "\n",
        "from torch_geometric.nn import Sequential, GCNConv, GATv2Conv, GATConv, SAGEConv\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYtCdaAryZxC"
      },
      "outputs": [],
      "source": [
        "Edges_train = pd.read_excel('graph10.xlsx', sheet_name= 'Sheet2')\n",
        "Edges_test = pd.read_excel('graph10.xlsx', sheet_name= 'Sheet4')\n",
        "Edges_validation = pd.read_excel('graph10.xlsx', sheet_name= 'Sheet6')\n",
        "\n",
        "dataset_train = pd.read_excel('graph10.xlsx', sheet_name= 'Train2')\n",
        "dataset_test = pd.read_excel('graph10.xlsx', sheet_name = 'Test2')\n",
        "dataset_validation = pd.read_excel('graph10.xlsx', sheet_name= 'Validation')\n",
        "\n",
        "#one_hot_cols = pd.get_dummies(dataset_train['Flat Type'], prefix='Flat')\n",
        "#dataset_train = dataset_train.drop(columns=['Flat Type'])\n",
        "#dataset_train = pd.concat([one_hot_cols, dataset_train], axis=1)\n",
        "\n",
        "#one_hot_cols = pd.get_dummies(dataset_test['Flat Type'], prefix='Flat')\n",
        "#dataset_test = dataset_test.drop(columns=['Flat Type'])\n",
        "#dataset_test = pd.concat([one_hot_cols, dataset_test], axis=1)\n",
        "\n",
        "#one_hot_cols = pd.get_dummies(dataset_train['Room Type'], prefix='Room')\n",
        "#dataset_train = dataset_train.drop(columns=['Room Type'])\n",
        "#dataset_train = pd.concat([one_hot_cols, dataset_train], axis=1)\n",
        "\n",
        "#one_hot_cols = pd.get_dummies(dataset_test['Room Type'], prefix='Room')\n",
        "#dataset_test = dataset_test.drop(columns=['Room Type'])\n",
        "#dataset_test = pd.concat([one_hot_cols, dataset_test], axis=1)\n",
        "\n",
        "# one_hot_cols = pd.get_dummies(dataset_train['Wing Orientation'], prefix='Wing')\n",
        "# dataset_train = dataset_train.drop(columns=['Wing Orientation'])\n",
        "# dataset_train = pd.concat([one_hot_cols, dataset_train], axis=1)\n",
        "\n",
        "# one_hot_cols = pd.get_dummies(dataset_test['Wing Orientation'], prefix='Wing')\n",
        "# dataset_test = dataset_test.drop(columns=['Wing Orientation'])\n",
        "# dataset_test = pd.concat([one_hot_cols, dataset_test], axis=1)\n",
        "\n",
        "# one_hot_cols = pd.get_dummies(dataset_train['Flat Orientation'], prefix='Flat Orientation')\n",
        "# dataset_train = dataset_train.drop(columns=['Flat Orientation'])\n",
        "# dataset_train = pd.concat([one_hot_cols, dataset_train], axis=1)\n",
        "\n",
        "# one_hot_cols = pd.get_dummies(dataset_test['Flat Orientation'], prefix='Flat Orientation')\n",
        "# dataset_test = dataset_test.drop(columns=['Flat Orientation'])\n",
        "# dataset_test = pd.concat([one_hot_cols, dataset_test], axis=1)\n",
        "\n",
        "dataset_train.fillna(0, inplace=True)\n",
        "dataset_test.fillna(0, inplace=True)\n",
        "dataset_validation.fillna(0, inplace=True)\n",
        "\n",
        "train_X = dataset_train.iloc[0:, :15]\n",
        "train_Y = dataset_train.iloc[0:, 15:]\n",
        "\n",
        "test_X = dataset_test.iloc[0:, :15]\n",
        "test_Y = dataset_test.iloc[0:, 15:]\n",
        "\n",
        "validation_X = dataset_validation.iloc[0:, :15]\n",
        "validation_Y = dataset_validation.iloc[0:, 15:]\n",
        "\n",
        "scalerX = StandardScaler().fit(train_X)\n",
        "scalerY = StandardScaler().fit(train_Y)\n",
        "train_X = scalerX.transform(train_X)\n",
        "train_Y = scalerY.transform(train_Y)\n",
        "test_X = scalerX.transform(test_X)\n",
        "test_Y = scalerY.transform(test_Y)\n",
        "\n",
        "validation_X = scalerX.transform(validation_X)\n",
        "validation_Y = scalerY.transform(validation_Y)\n",
        "\n",
        "edges_train = Edges_train.values - 1\n",
        "edges_train = torch.from_numpy(edges_train).t()\n",
        "edges_train = edges_train.long()\n",
        "\n",
        "edges_test = Edges_test.values - 1\n",
        "edges_test = torch.from_numpy(edges_test).t()\n",
        "edges_test = edges_test.long()\n",
        "\n",
        "edges_validation = Edges_validation.values - 1\n",
        "edges_validation = torch.from_numpy(edges_validation).t()\n",
        "edges_validation = edges_validation.long()\n",
        "\n",
        "train = pd.concat([pd.DataFrame(train_X), pd.DataFrame(train_Y)], axis=1)\n",
        "test = pd.concat([pd.DataFrame(test_X), pd.DataFrame(test_Y)], axis=1)\n",
        "validation = pd.concat([pd.DataFrame(validation_X), pd.DataFrame(validation_Y)], axis=1)\n",
        "\n",
        "train_X = torch.tensor(np.array(train_X.astype('f')))\n",
        "train_Y = torch.tensor(np.array(train_Y.astype('f')))\n",
        "\n",
        "test_X = torch.tensor(np.array(test_X.astype('f')))\n",
        "test_Y = torch.tensor(np.array(test_Y.astype('f')))\n",
        "\n",
        "validation_X = torch.tensor(np.array(validation_X.astype('f')))\n",
        "validation_Y = torch.tensor(np.array(validation_Y.astype('f')))\n",
        "\n",
        "\n",
        "train_data = Data(x = train_X, edge_index = edges_train, y = train_Y)\n",
        "test_data = Data(x = test_X, edge_index = edges_test, y = test_Y)\n",
        "validation_data = Data(x = validation_X, edge_index = edges_validation, y = validation_Y)\n",
        "\n",
        "# ACH_MEAN = dataset_train.ACH.mean()\n",
        "# ACH_var = dataset_train.ACH.var()\n",
        "# ACH_Min = dataset_train.ACH.min()\n",
        "# ACH_Max = dataset_train.ACH.max()\n",
        "# CP_MEAN = dataset_train.CP.mean()\n",
        "# CP_var = dataset_train.CP.var()\n",
        "# CP_Min = dataset_train.CP.min()\n",
        "# CP_Max = dataset_train.CP.max()\n",
        "\n",
        "\n",
        "train_Y_mean = train_Y.mean()\n",
        "train_Y_var = train_Y.var()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mNasaxEoX0rM",
        "outputId": "19932ca9-aeb9-4a14-c34f-6a994ee95050"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001, Train_Loss: 0.1216, Test_Loss: 0.1096, Train_Acc:0.0090, Test_Acc: 0.0000\n",
            "Epoch: 002, Train_Loss: 0.2253, Test_Loss: 0.1110, Train_Acc:0.0867, Test_Acc: 0.1096\n",
            "Epoch: 003, Train_Loss: 0.3016, Test_Loss: 0.1110, Train_Acc:0.0553, Test_Acc: 0.0482\n",
            "Epoch: 004, Train_Loss: 0.1805, Test_Loss: 0.1066, Train_Acc:0.0484, Test_Acc: 0.0526\n",
            "Epoch: 005, Train_Loss: 0.1022, Test_Loss: 0.1014, Train_Acc:0.0069, Test_Acc: 0.0022\n",
            "Epoch: 006, Train_Loss: 0.0787, Test_Loss: 0.0973, Train_Acc:0.0149, Test_Acc: 0.0175\n",
            "Epoch: 007, Train_Loss: 0.0791, Test_Loss: 0.0907, Train_Acc:0.0484, Test_Acc: 0.0439\n",
            "Epoch: 008, Train_Loss: 0.0840, Test_Loss: 0.0832, Train_Acc:0.1027, Test_Acc: 0.1162\n",
            "Epoch: 009, Train_Loss: 0.0819, Test_Loss: 0.0808, Train_Acc:0.1995, Test_Acc: 0.2039\n",
            "Epoch: 010, Train_Loss: 0.0750, Test_Loss: 0.0790, Train_Acc:0.2314, Test_Acc: 0.2368\n",
            "Epoch: 011, Train_Loss: 0.0624, Test_Loss: 0.0731, Train_Acc:0.2963, Test_Acc: 0.3004\n",
            "Epoch: 012, Train_Loss: 0.0577, Test_Loss: 0.0687, Train_Acc:0.3718, Test_Acc: 0.3224\n",
            "Epoch: 013, Train_Loss: 0.0589, Test_Loss: 0.0652, Train_Acc:0.3729, Test_Acc: 0.3114\n",
            "Epoch: 014, Train_Loss: 0.0596, Test_Loss: 0.0599, Train_Acc:0.3782, Test_Acc: 0.3333\n",
            "Epoch: 015, Train_Loss: 0.0566, Test_Loss: 0.0526, Train_Acc:0.4043, Test_Acc: 0.3377\n",
            "Epoch: 016, Train_Loss: 0.0506, Test_Loss: 0.0507, Train_Acc:0.3122, Test_Acc: 0.2982\n",
            "Epoch: 017, Train_Loss: 0.0474, Test_Loss: 0.0512, Train_Acc:0.2415, Test_Acc: 0.1996\n",
            "Epoch: 018, Train_Loss: 0.0480, Test_Loss: 0.0494, Train_Acc:0.2457, Test_Acc: 0.2105\n",
            "Epoch: 019, Train_Loss: 0.0474, Test_Loss: 0.0444, Train_Acc:0.3941, Test_Acc: 0.3377\n",
            "Epoch: 020, Train_Loss: 0.0444, Test_Loss: 0.0379, Train_Acc:0.5117, Test_Acc: 0.4693\n",
            "Epoch: 021, Train_Loss: 0.0423, Test_Loss: 0.0331, Train_Acc:0.5872, Test_Acc: 0.5899\n",
            "Epoch: 022, Train_Loss: 0.0397, Test_Loss: 0.0335, Train_Acc:0.5920, Test_Acc: 0.5833\n",
            "Epoch: 023, Train_Loss: 0.0396, Test_Loss: 0.0356, Train_Acc:0.5628, Test_Acc: 0.5197\n",
            "Epoch: 024, Train_Loss: 0.0377, Test_Loss: 0.0349, Train_Acc:0.5154, Test_Acc: 0.4912\n",
            "Epoch: 025, Train_Loss: 0.0357, Test_Loss: 0.0325, Train_Acc:0.5128, Test_Acc: 0.4890\n",
            "Epoch: 026, Train_Loss: 0.0339, Test_Loss: 0.0305, Train_Acc:0.5410, Test_Acc: 0.5022\n",
            "Epoch: 027, Train_Loss: 0.0328, Test_Loss: 0.0292, Train_Acc:0.6101, Test_Acc: 0.5592\n",
            "Epoch: 028, Train_Loss: 0.0319, Test_Loss: 0.0279, Train_Acc:0.6612, Test_Acc: 0.6009\n",
            "Epoch: 029, Train_Loss: 0.0310, Test_Loss: 0.0275, Train_Acc:0.6878, Test_Acc: 0.6338\n",
            "Epoch: 030, Train_Loss: 0.0282, Test_Loss: 0.0290, Train_Acc:0.7005, Test_Acc: 0.6447\n",
            "Epoch: 031, Train_Loss: 0.0271, Test_Loss: 0.0297, Train_Acc:0.6851, Test_Acc: 0.6316\n",
            "Epoch: 032, Train_Loss: 0.0259, Test_Loss: 0.0284, Train_Acc:0.6899, Test_Acc: 0.6360\n",
            "Epoch: 033, Train_Loss: 0.0256, Test_Loss: 0.0261, Train_Acc:0.7383, Test_Acc: 0.6754\n",
            "Epoch: 034, Train_Loss: 0.0253, Test_Loss: 0.0239, Train_Acc:0.7521, Test_Acc: 0.6974\n",
            "Epoch: 035, Train_Loss: 0.0244, Test_Loss: 0.0221, Train_Acc:0.7511, Test_Acc: 0.7127\n",
            "Epoch: 036, Train_Loss: 0.0251, Test_Loss: 0.0207, Train_Acc:0.7723, Test_Acc: 0.7259\n",
            "Epoch: 037, Train_Loss: 0.0225, Test_Loss: 0.0210, Train_Acc:0.7718, Test_Acc: 0.7325\n",
            "Epoch: 038, Train_Loss: 0.0220, Test_Loss: 0.0216, Train_Acc:0.7606, Test_Acc: 0.7303\n",
            "Epoch: 039, Train_Loss: 0.0217, Test_Loss: 0.0212, Train_Acc:0.7798, Test_Acc: 0.7346\n",
            "Epoch: 040, Train_Loss: 0.0210, Test_Loss: 0.0206, Train_Acc:0.7830, Test_Acc: 0.7325\n",
            "Epoch: 041, Train_Loss: 0.0197, Test_Loss: 0.0206, Train_Acc:0.7660, Test_Acc: 0.7434\n",
            "Epoch: 042, Train_Loss: 0.0199, Test_Loss: 0.0205, Train_Acc:0.7447, Test_Acc: 0.7346\n",
            "Epoch: 043, Train_Loss: 0.0197, Test_Loss: 0.0199, Train_Acc:0.7484, Test_Acc: 0.7325\n",
            "Epoch: 044, Train_Loss: 0.0193, Test_Loss: 0.0201, Train_Acc:0.7670, Test_Acc: 0.7368\n",
            "Epoch: 045, Train_Loss: 0.0180, Test_Loss: 0.0201, Train_Acc:0.7670, Test_Acc: 0.7478\n",
            "Epoch: 046, Train_Loss: 0.0182, Test_Loss: 0.0196, Train_Acc:0.7707, Test_Acc: 0.7412\n",
            "Epoch: 047, Train_Loss: 0.0179, Test_Loss: 0.0184, Train_Acc:0.7612, Test_Acc: 0.7456\n",
            "Epoch: 048, Train_Loss: 0.0169, Test_Loss: 0.0176, Train_Acc:0.7606, Test_Acc: 0.7588\n",
            "Epoch: 049, Train_Loss: 0.0170, Test_Loss: 0.0173, Train_Acc:0.7654, Test_Acc: 0.7588\n",
            "Epoch: 050, Train_Loss: 0.0166, Test_Loss: 0.0178, Train_Acc:0.7750, Test_Acc: 0.7500\n",
            "Epoch: 051, Train_Loss: 0.0163, Test_Loss: 0.0177, Train_Acc:0.7777, Test_Acc: 0.7368\n",
            "Epoch: 052, Train_Loss: 0.0161, Test_Loss: 0.0176, Train_Acc:0.7564, Test_Acc: 0.7346\n",
            "Epoch: 053, Train_Loss: 0.0162, Test_Loss: 0.0173, Train_Acc:0.7468, Test_Acc: 0.7390\n",
            "Epoch: 054, Train_Loss: 0.0152, Test_Loss: 0.0173, Train_Acc:0.7463, Test_Acc: 0.7346\n",
            "Epoch: 055, Train_Loss: 0.0152, Test_Loss: 0.0173, Train_Acc:0.7447, Test_Acc: 0.7500\n",
            "Epoch: 056, Train_Loss: 0.0151, Test_Loss: 0.0179, Train_Acc:0.7394, Test_Acc: 0.7215\n",
            "Epoch: 057, Train_Loss: 0.0148, Test_Loss: 0.0175, Train_Acc:0.7564, Test_Acc: 0.7083\n",
            "Epoch: 058, Train_Loss: 0.0142, Test_Loss: 0.0181, Train_Acc:0.7452, Test_Acc: 0.6886\n",
            "Epoch: 059, Train_Loss: 0.0144, Test_Loss: 0.0174, Train_Acc:0.7351, Test_Acc: 0.7018\n",
            "Epoch: 060, Train_Loss: 0.0140, Test_Loss: 0.0178, Train_Acc:0.7117, Test_Acc: 0.7061\n",
            "Epoch: 061, Train_Loss: 0.0142, Test_Loss: 0.0180, Train_Acc:0.7149, Test_Acc: 0.7412\n",
            "Epoch: 062, Train_Loss: 0.0139, Test_Loss: 0.0178, Train_Acc:0.7293, Test_Acc: 0.7171\n",
            "Epoch: 063, Train_Loss: 0.0133, Test_Loss: 0.0179, Train_Acc:0.7484, Test_Acc: 0.6908\n",
            "Epoch: 064, Train_Loss: 0.0138, Test_Loss: 0.0177, Train_Acc:0.7500, Test_Acc: 0.7018\n",
            "Epoch: 065, Train_Loss: 0.0126, Test_Loss: 0.0179, Train_Acc:0.7564, Test_Acc: 0.6974\n",
            "Epoch: 066, Train_Loss: 0.0136, Test_Loss: 0.0179, Train_Acc:0.7447, Test_Acc: 0.6886\n",
            "Epoch: 067, Train_Loss: 0.0124, Test_Loss: 0.0184, Train_Acc:0.7378, Test_Acc: 0.6798\n",
            "Epoch: 068, Train_Loss: 0.0132, Test_Loss: 0.0181, Train_Acc:0.7367, Test_Acc: 0.6776\n",
            "Epoch: 069, Train_Loss: 0.0121, Test_Loss: 0.0176, Train_Acc:0.7410, Test_Acc: 0.6952\n",
            "Epoch: 070, Train_Loss: 0.0117, Test_Loss: 0.0171, Train_Acc:0.7601, Test_Acc: 0.7171\n",
            "Epoch: 071, Train_Loss: 0.0125, Test_Loss: 0.0178, Train_Acc:0.7463, Test_Acc: 0.6974\n",
            "Epoch: 072, Train_Loss: 0.0120, Test_Loss: 0.0178, Train_Acc:0.7473, Test_Acc: 0.7105\n",
            "Epoch: 073, Train_Loss: 0.0115, Test_Loss: 0.0180, Train_Acc:0.7245, Test_Acc: 0.7039\n",
            "Epoch: 074, Train_Loss: 0.0115, Test_Loss: 0.0180, Train_Acc:0.7388, Test_Acc: 0.7039\n",
            "Epoch: 075, Train_Loss: 0.0119, Test_Loss: 0.0179, Train_Acc:0.7255, Test_Acc: 0.6864\n",
            "Epoch: 076, Train_Loss: 0.0114, Test_Loss: 0.0181, Train_Acc:0.7335, Test_Acc: 0.6711\n",
            "Epoch: 077, Train_Loss: 0.0115, Test_Loss: 0.0176, Train_Acc:0.7548, Test_Acc: 0.6952\n",
            "Epoch: 078, Train_Loss: 0.0113, Test_Loss: 0.0176, Train_Acc:0.7457, Test_Acc: 0.6952\n",
            "Epoch: 079, Train_Loss: 0.0112, Test_Loss: 0.0178, Train_Acc:0.7250, Test_Acc: 0.7018\n",
            "Epoch: 080, Train_Loss: 0.0111, Test_Loss: 0.0177, Train_Acc:0.7383, Test_Acc: 0.7039\n",
            "Epoch: 081, Train_Loss: 0.0112, Test_Loss: 0.0180, Train_Acc:0.7170, Test_Acc: 0.7061\n",
            "Epoch: 082, Train_Loss: 0.0112, Test_Loss: 0.0184, Train_Acc:0.7197, Test_Acc: 0.7018\n",
            "Epoch: 083, Train_Loss: 0.0115, Test_Loss: 0.0196, Train_Acc:0.7165, Test_Acc: 0.6886\n",
            "Epoch: 084, Train_Loss: 0.0116, Test_Loss: 0.0191, Train_Acc:0.7170, Test_Acc: 0.6908\n",
            "Epoch: 085, Train_Loss: 0.0110, Test_Loss: 0.0187, Train_Acc:0.7027, Test_Acc: 0.6886\n",
            "Epoch: 086, Train_Loss: 0.0107, Test_Loss: 0.0184, Train_Acc:0.7239, Test_Acc: 0.7171\n",
            "Epoch: 087, Train_Loss: 0.0108, Test_Loss: 0.0188, Train_Acc:0.6814, Test_Acc: 0.6886\n",
            "Epoch: 088, Train_Loss: 0.0111, Test_Loss: 0.0186, Train_Acc:0.7330, Test_Acc: 0.7105\n",
            "Epoch: 089, Train_Loss: 0.0126, Test_Loss: 0.0193, Train_Acc:0.6691, Test_Acc: 0.6711\n",
            "Epoch: 090, Train_Loss: 0.0114, Test_Loss: 0.0194, Train_Acc:0.7213, Test_Acc: 0.6974\n",
            "Epoch: 091, Train_Loss: 0.0114, Test_Loss: 0.0193, Train_Acc:0.6920, Test_Acc: 0.6842\n",
            "Epoch: 092, Train_Loss: 0.0108, Test_Loss: 0.0194, Train_Acc:0.6846, Test_Acc: 0.6842\n",
            "Epoch: 093, Train_Loss: 0.0101, Test_Loss: 0.0184, Train_Acc:0.7250, Test_Acc: 0.6930\n",
            "Epoch: 094, Train_Loss: 0.0106, Test_Loss: 0.0177, Train_Acc:0.7410, Test_Acc: 0.7149\n",
            "Epoch: 095, Train_Loss: 0.0106, Test_Loss: 0.0196, Train_Acc:0.6830, Test_Acc: 0.6974\n",
            "Epoch: 096, Train_Loss: 0.0104, Test_Loss: 0.0188, Train_Acc:0.7473, Test_Acc: 0.7083\n",
            "Epoch: 097, Train_Loss: 0.0106, Test_Loss: 0.0173, Train_Acc:0.7314, Test_Acc: 0.7083\n",
            "Epoch: 098, Train_Loss: 0.0104, Test_Loss: 0.0170, Train_Acc:0.7734, Test_Acc: 0.7346\n",
            "Epoch: 099, Train_Loss: 0.0103, Test_Loss: 0.0178, Train_Acc:0.7362, Test_Acc: 0.7127\n",
            "Epoch: 100, Train_Loss: 0.0098, Test_Loss: 0.0182, Train_Acc:0.7569, Test_Acc: 0.7171\n",
            "Epoch: 101, Train_Loss: 0.0104, Test_Loss: 0.0180, Train_Acc:0.7069, Test_Acc: 0.6996\n",
            "Epoch: 102, Train_Loss: 0.0102, Test_Loss: 0.0170, Train_Acc:0.7516, Test_Acc: 0.7149\n",
            "Epoch: 103, Train_Loss: 0.0097, Test_Loss: 0.0167, Train_Acc:0.7319, Test_Acc: 0.7018\n",
            "Epoch: 104, Train_Loss: 0.0098, Test_Loss: 0.0174, Train_Acc:0.7160, Test_Acc: 0.6842\n",
            "Epoch: 105, Train_Loss: 0.0097, Test_Loss: 0.0170, Train_Acc:0.7782, Test_Acc: 0.7215\n",
            "Epoch: 106, Train_Loss: 0.0102, Test_Loss: 0.0178, Train_Acc:0.6995, Test_Acc: 0.6798\n",
            "Epoch: 107, Train_Loss: 0.0102, Test_Loss: 0.0175, Train_Acc:0.7420, Test_Acc: 0.7083\n",
            "Epoch: 108, Train_Loss: 0.0107, Test_Loss: 0.0171, Train_Acc:0.7144, Test_Acc: 0.7083\n",
            "Epoch: 109, Train_Loss: 0.0100, Test_Loss: 0.0170, Train_Acc:0.7574, Test_Acc: 0.7193\n",
            "Epoch: 110, Train_Loss: 0.0098, Test_Loss: 0.0174, Train_Acc:0.7383, Test_Acc: 0.7105\n",
            "Epoch: 111, Train_Loss: 0.0093, Test_Loss: 0.0172, Train_Acc:0.7282, Test_Acc: 0.7105\n",
            "Epoch: 112, Train_Loss: 0.0097, Test_Loss: 0.0173, Train_Acc:0.7601, Test_Acc: 0.7061\n",
            "Epoch: 113, Train_Loss: 0.0102, Test_Loss: 0.0176, Train_Acc:0.7202, Test_Acc: 0.6996\n",
            "Epoch: 114, Train_Loss: 0.0098, Test_Loss: 0.0170, Train_Acc:0.7415, Test_Acc: 0.7215\n",
            "Epoch: 115, Train_Loss: 0.0093, Test_Loss: 0.0162, Train_Acc:0.7654, Test_Acc: 0.7325\n",
            "Epoch: 116, Train_Loss: 0.0092, Test_Loss: 0.0157, Train_Acc:0.7920, Test_Acc: 0.7478\n",
            "Epoch: 117, Train_Loss: 0.0093, Test_Loss: 0.0172, Train_Acc:0.7463, Test_Acc: 0.7368\n",
            "Epoch: 118, Train_Loss: 0.0102, Test_Loss: 0.0188, Train_Acc:0.6511, Test_Acc: 0.6842\n",
            "Epoch: 119, Train_Loss: 0.0116, Test_Loss: 0.0166, Train_Acc:0.7707, Test_Acc: 0.7412\n",
            "Epoch: 120, Train_Loss: 0.0110, Test_Loss: 0.0168, Train_Acc:0.7968, Test_Acc: 0.7149\n",
            "Epoch: 121, Train_Loss: 0.0089, Test_Loss: 0.0185, Train_Acc:0.6851, Test_Acc: 0.6689\n",
            "Epoch: 122, Train_Loss: 0.0109, Test_Loss: 0.0177, Train_Acc:0.7452, Test_Acc: 0.7215\n",
            "Epoch: 123, Train_Loss: 0.0119, Test_Loss: 0.0171, Train_Acc:0.7872, Test_Acc: 0.6996\n",
            "Epoch: 124, Train_Loss: 0.0093, Test_Loss: 0.0186, Train_Acc:0.7383, Test_Acc: 0.6798\n",
            "Epoch: 125, Train_Loss: 0.0101, Test_Loss: 0.0186, Train_Acc:0.7553, Test_Acc: 0.7083\n",
            "Epoch: 126, Train_Loss: 0.0114, Test_Loss: 0.0164, Train_Acc:0.7532, Test_Acc: 0.7105\n",
            "Epoch: 127, Train_Loss: 0.0094, Test_Loss: 0.0154, Train_Acc:0.7521, Test_Acc: 0.7259\n",
            "Epoch: 128, Train_Loss: 0.0111, Test_Loss: 0.0155, Train_Acc:0.8117, Test_Acc: 0.7632\n",
            "Epoch: 129, Train_Loss: 0.0101, Test_Loss: 0.0169, Train_Acc:0.7830, Test_Acc: 0.7368\n",
            "Epoch: 130, Train_Loss: 0.0096, Test_Loss: 0.0173, Train_Acc:0.7404, Test_Acc: 0.7127\n",
            "Epoch: 131, Train_Loss: 0.0103, Test_Loss: 0.0148, Train_Acc:0.8303, Test_Acc: 0.7478\n",
            "Epoch: 132, Train_Loss: 0.0095, Test_Loss: 0.0150, Train_Acc:0.8218, Test_Acc: 0.7325\n",
            "Epoch: 133, Train_Loss: 0.0098, Test_Loss: 0.0153, Train_Acc:0.7739, Test_Acc: 0.7390\n",
            "Epoch: 134, Train_Loss: 0.0091, Test_Loss: 0.0147, Train_Acc:0.8213, Test_Acc: 0.7522\n",
            "Epoch: 135, Train_Loss: 0.0092, Test_Loss: 0.0146, Train_Acc:0.8457, Test_Acc: 0.7478\n",
            "Epoch: 136, Train_Loss: 0.0092, Test_Loss: 0.0150, Train_Acc:0.8293, Test_Acc: 0.7368\n",
            "Epoch: 137, Train_Loss: 0.0088, Test_Loss: 0.0145, Train_Acc:0.8473, Test_Acc: 0.7632\n",
            "Epoch: 138, Train_Loss: 0.0089, Test_Loss: 0.0142, Train_Acc:0.8335, Test_Acc: 0.7522\n",
            "Epoch: 139, Train_Loss: 0.0091, Test_Loss: 0.0160, Train_Acc:0.7989, Test_Acc: 0.7215\n",
            "Epoch: 140, Train_Loss: 0.0088, Test_Loss: 0.0146, Train_Acc:0.8191, Test_Acc: 0.7741\n",
            "Epoch: 141, Train_Loss: 0.0085, Test_Loss: 0.0134, Train_Acc:0.8505, Test_Acc: 0.7961\n",
            "Epoch: 142, Train_Loss: 0.0088, Test_Loss: 0.0152, Train_Acc:0.8574, Test_Acc: 0.7456\n",
            "Epoch: 143, Train_Loss: 0.0090, Test_Loss: 0.0150, Train_Acc:0.8495, Test_Acc: 0.7412\n",
            "Epoch: 144, Train_Loss: 0.0092, Test_Loss: 0.0149, Train_Acc:0.8521, Test_Acc: 0.7719\n",
            "Epoch: 145, Train_Loss: 0.0098, Test_Loss: 0.0154, Train_Acc:0.7840, Test_Acc: 0.7412\n",
            "Epoch: 146, Train_Loss: 0.0092, Test_Loss: 0.0156, Train_Acc:0.8122, Test_Acc: 0.7522\n",
            "Epoch: 147, Train_Loss: 0.0092, Test_Loss: 0.0154, Train_Acc:0.8596, Test_Acc: 0.7456\n",
            "Epoch: 148, Train_Loss: 0.0090, Test_Loss: 0.0140, Train_Acc:0.8628, Test_Acc: 0.7654\n",
            "Epoch: 149, Train_Loss: 0.0093, Test_Loss: 0.0150, Train_Acc:0.8404, Test_Acc: 0.7434\n",
            "Epoch: 150, Train_Loss: 0.0093, Test_Loss: 0.0157, Train_Acc:0.7899, Test_Acc: 0.7215\n",
            "Epoch: 151, Train_Loss: 0.0085, Test_Loss: 0.0143, Train_Acc:0.8367, Test_Acc: 0.7632\n",
            "Epoch: 152, Train_Loss: 0.0087, Test_Loss: 0.0149, Train_Acc:0.8505, Test_Acc: 0.7478\n",
            "Epoch: 153, Train_Loss: 0.0085, Test_Loss: 0.0150, Train_Acc:0.7904, Test_Acc: 0.7390\n",
            "Epoch: 154, Train_Loss: 0.0089, Test_Loss: 0.0144, Train_Acc:0.8436, Test_Acc: 0.7697\n",
            "Epoch: 155, Train_Loss: 0.0085, Test_Loss: 0.0135, Train_Acc:0.8516, Test_Acc: 0.7675\n",
            "Epoch: 156, Train_Loss: 0.0084, Test_Loss: 0.0146, Train_Acc:0.8463, Test_Acc: 0.7522\n",
            "Epoch: 157, Train_Loss: 0.0093, Test_Loss: 0.0156, Train_Acc:0.8191, Test_Acc: 0.7456\n",
            "Epoch: 158, Train_Loss: 0.0096, Test_Loss: 0.0146, Train_Acc:0.8032, Test_Acc: 0.7610\n",
            "Epoch: 159, Train_Loss: 0.0085, Test_Loss: 0.0146, Train_Acc:0.8234, Test_Acc: 0.7368\n",
            "Epoch: 160, Train_Loss: 0.0086, Test_Loss: 0.0146, Train_Acc:0.8559, Test_Acc: 0.7632\n",
            "Epoch: 161, Train_Loss: 0.0088, Test_Loss: 0.0129, Train_Acc:0.8495, Test_Acc: 0.7829\n",
            "Epoch: 162, Train_Loss: 0.0083, Test_Loss: 0.0131, Train_Acc:0.8144, Test_Acc: 0.7697\n",
            "Epoch: 163, Train_Loss: 0.0084, Test_Loss: 0.0141, Train_Acc:0.8516, Test_Acc: 0.7675\n",
            "Epoch: 164, Train_Loss: 0.0088, Test_Loss: 0.0137, Train_Acc:0.8495, Test_Acc: 0.7763\n",
            "Epoch: 165, Train_Loss: 0.0080, Test_Loss: 0.0130, Train_Acc:0.8149, Test_Acc: 0.7851\n",
            "Epoch: 166, Train_Loss: 0.0088, Test_Loss: 0.0139, Train_Acc:0.8590, Test_Acc: 0.7741\n",
            "Epoch: 167, Train_Loss: 0.0093, Test_Loss: 0.0150, Train_Acc:0.7888, Test_Acc: 0.7456\n",
            "Epoch: 168, Train_Loss: 0.0093, Test_Loss: 0.0141, Train_Acc:0.8122, Test_Acc: 0.7654\n",
            "Epoch: 169, Train_Loss: 0.0110, Test_Loss: 0.0138, Train_Acc:0.8394, Test_Acc: 0.7544\n",
            "Epoch: 170, Train_Loss: 0.0092, Test_Loss: 0.0130, Train_Acc:0.8718, Test_Acc: 0.8026\n",
            "Epoch: 171, Train_Loss: 0.0082, Test_Loss: 0.0126, Train_Acc:0.8803, Test_Acc: 0.8136\n",
            "Epoch: 172, Train_Loss: 0.0090, Test_Loss: 0.0129, Train_Acc:0.8532, Test_Acc: 0.7873\n",
            "Epoch: 173, Train_Loss: 0.0095, Test_Loss: 0.0137, Train_Acc:0.8436, Test_Acc: 0.8004\n",
            "Epoch: 174, Train_Loss: 0.0091, Test_Loss: 0.0109, Train_Acc:0.8989, Test_Acc: 0.8333\n",
            "Epoch: 175, Train_Loss: 0.0085, Test_Loss: 0.0109, Train_Acc:0.8899, Test_Acc: 0.8136\n",
            "Epoch: 176, Train_Loss: 0.0094, Test_Loss: 0.0123, Train_Acc:0.8723, Test_Acc: 0.8246\n",
            "Epoch: 177, Train_Loss: 0.0084, Test_Loss: 0.0125, Train_Acc:0.8644, Test_Acc: 0.8092\n",
            "Epoch: 178, Train_Loss: 0.0089, Test_Loss: 0.0112, Train_Acc:0.8739, Test_Acc: 0.8092\n",
            "Epoch: 179, Train_Loss: 0.0089, Test_Loss: 0.0117, Train_Acc:0.8883, Test_Acc: 0.8355\n",
            "Epoch: 180, Train_Loss: 0.0088, Test_Loss: 0.0132, Train_Acc:0.8851, Test_Acc: 0.7895\n",
            "Epoch: 181, Train_Loss: 0.0089, Test_Loss: 0.0137, Train_Acc:0.8521, Test_Acc: 0.8004\n",
            "Epoch: 182, Train_Loss: 0.0089, Test_Loss: 0.0122, Train_Acc:0.8878, Test_Acc: 0.8180\n",
            "Epoch: 183, Train_Loss: 0.0084, Test_Loss: 0.0110, Train_Acc:0.9096, Test_Acc: 0.8421\n",
            "Epoch: 184, Train_Loss: 0.0084, Test_Loss: 0.0125, Train_Acc:0.8920, Test_Acc: 0.7939\n",
            "Epoch: 185, Train_Loss: 0.0084, Test_Loss: 0.0133, Train_Acc:0.8686, Test_Acc: 0.8026\n",
            "Epoch: 186, Train_Loss: 0.0079, Test_Loss: 0.0124, Train_Acc:0.8814, Test_Acc: 0.8180\n",
            "Epoch: 187, Train_Loss: 0.0083, Test_Loss: 0.0113, Train_Acc:0.8910, Test_Acc: 0.8443\n",
            "Epoch: 188, Train_Loss: 0.0077, Test_Loss: 0.0114, Train_Acc:0.8878, Test_Acc: 0.8355\n",
            "Epoch: 189, Train_Loss: 0.0085, Test_Loss: 0.0119, Train_Acc:0.8654, Test_Acc: 0.8224\n",
            "Epoch: 190, Train_Loss: 0.0083, Test_Loss: 0.0132, Train_Acc:0.8463, Test_Acc: 0.7961\n",
            "Epoch: 191, Train_Loss: 0.0084, Test_Loss: 0.0135, Train_Acc:0.8537, Test_Acc: 0.7807\n",
            "Epoch: 192, Train_Loss: 0.0080, Test_Loss: 0.0124, Train_Acc:0.8814, Test_Acc: 0.8092\n",
            "Epoch: 193, Train_Loss: 0.0078, Test_Loss: 0.0126, Train_Acc:0.8803, Test_Acc: 0.8202\n",
            "Epoch: 194, Train_Loss: 0.0078, Test_Loss: 0.0143, Train_Acc:0.8431, Test_Acc: 0.7917\n",
            "Epoch: 195, Train_Loss: 0.0084, Test_Loss: 0.0141, Train_Acc:0.8239, Test_Acc: 0.7654\n",
            "Epoch: 196, Train_Loss: 0.0086, Test_Loss: 0.0119, Train_Acc:0.8941, Test_Acc: 0.8311\n",
            "Epoch: 197, Train_Loss: 0.0085, Test_Loss: 0.0123, Train_Acc:0.8734, Test_Acc: 0.7982\n",
            "Epoch: 198, Train_Loss: 0.0078, Test_Loss: 0.0128, Train_Acc:0.8516, Test_Acc: 0.7961\n",
            "Epoch: 199, Train_Loss: 0.0078, Test_Loss: 0.0124, Train_Acc:0.8697, Test_Acc: 0.8180\n",
            "Epoch: 200, Train_Loss: 0.0074, Test_Loss: 0.0118, Train_Acc:0.8676, Test_Acc: 0.7982\n",
            "Epoch: 201, Train_Loss: 0.0076, Test_Loss: 0.0134, Train_Acc:0.8564, Test_Acc: 0.8092\n",
            "Epoch: 202, Train_Loss: 0.0078, Test_Loss: 0.0123, Train_Acc:0.8739, Test_Acc: 0.8070\n",
            "Epoch: 203, Train_Loss: 0.0079, Test_Loss: 0.0119, Train_Acc:0.8941, Test_Acc: 0.8443\n",
            "Epoch: 204, Train_Loss: 0.0083, Test_Loss: 0.0121, Train_Acc:0.8904, Test_Acc: 0.8048\n",
            "Epoch: 205, Train_Loss: 0.0076, Test_Loss: 0.0133, Train_Acc:0.8489, Test_Acc: 0.7829\n",
            "Epoch: 206, Train_Loss: 0.0081, Test_Loss: 0.0126, Train_Acc:0.8718, Test_Acc: 0.8026\n",
            "Epoch: 207, Train_Loss: 0.0089, Test_Loss: 0.0123, Train_Acc:0.8734, Test_Acc: 0.7982\n",
            "Epoch: 208, Train_Loss: 0.0086, Test_Loss: 0.0143, Train_Acc:0.8404, Test_Acc: 0.7500\n",
            "Epoch: 209, Train_Loss: 0.0076, Test_Loss: 0.0131, Train_Acc:0.8622, Test_Acc: 0.8114\n",
            "Epoch: 210, Train_Loss: 0.0081, Test_Loss: 0.0126, Train_Acc:0.8883, Test_Acc: 0.8158\n",
            "Epoch: 211, Train_Loss: 0.0084, Test_Loss: 0.0116, Train_Acc:0.8867, Test_Acc: 0.8443\n",
            "Epoch: 212, Train_Loss: 0.0072, Test_Loss: 0.0123, Train_Acc:0.8649, Test_Acc: 0.8180\n",
            "Epoch: 213, Train_Loss: 0.0083, Test_Loss: 0.0137, Train_Acc:0.8574, Test_Acc: 0.7961\n",
            "Epoch: 214, Train_Loss: 0.0092, Test_Loss: 0.0113, Train_Acc:0.8968, Test_Acc: 0.8246\n",
            "Epoch: 215, Train_Loss: 0.0072, Test_Loss: 0.0116, Train_Acc:0.8867, Test_Acc: 0.8158\n",
            "Epoch: 216, Train_Loss: 0.0090, Test_Loss: 0.0118, Train_Acc:0.8894, Test_Acc: 0.8246\n",
            "Epoch: 217, Train_Loss: 0.0091, Test_Loss: 0.0117, Train_Acc:0.8814, Test_Acc: 0.8399\n",
            "Epoch: 218, Train_Loss: 0.0078, Test_Loss: 0.0113, Train_Acc:0.9059, Test_Acc: 0.8443\n",
            "Epoch: 219, Train_Loss: 0.0083, Test_Loss: 0.0132, Train_Acc:0.8809, Test_Acc: 0.8070\n",
            "Epoch: 220, Train_Loss: 0.0083, Test_Loss: 0.0133, Train_Acc:0.8766, Test_Acc: 0.8224\n",
            "Epoch: 221, Train_Loss: 0.0082, Test_Loss: 0.0114, Train_Acc:0.8633, Test_Acc: 0.8092\n",
            "Epoch: 222, Train_Loss: 0.0083, Test_Loss: 0.0113, Train_Acc:0.8798, Test_Acc: 0.8421\n",
            "Epoch: 223, Train_Loss: 0.0080, Test_Loss: 0.0114, Train_Acc:0.9059, Test_Acc: 0.8531\n",
            "Epoch: 224, Train_Loss: 0.0080, Test_Loss: 0.0107, Train_Acc:0.9122, Test_Acc: 0.8531\n",
            "Epoch: 225, Train_Loss: 0.0075, Test_Loss: 0.0118, Train_Acc:0.9000, Test_Acc: 0.8092\n",
            "Epoch: 226, Train_Loss: 0.0076, Test_Loss: 0.0120, Train_Acc:0.8894, Test_Acc: 0.8092\n",
            "Epoch: 227, Train_Loss: 0.0075, Test_Loss: 0.0119, Train_Acc:0.8968, Test_Acc: 0.8136\n",
            "Epoch: 228, Train_Loss: 0.0077, Test_Loss: 0.0104, Train_Acc:0.9144, Test_Acc: 0.8553\n",
            "Epoch: 229, Train_Loss: 0.0074, Test_Loss: 0.0127, Train_Acc:0.8809, Test_Acc: 0.8026\n",
            "Epoch: 230, Train_Loss: 0.0082, Test_Loss: 0.0111, Train_Acc:0.9229, Test_Acc: 0.8553\n",
            "Epoch: 231, Train_Loss: 0.0070, Test_Loss: 0.0105, Train_Acc:0.8899, Test_Acc: 0.8509\n",
            "Epoch: 232, Train_Loss: 0.0077, Test_Loss: 0.0102, Train_Acc:0.9101, Test_Acc: 0.8794\n",
            "Epoch: 233, Train_Loss: 0.0072, Test_Loss: 0.0101, Train_Acc:0.9277, Test_Acc: 0.8575\n",
            "Epoch: 234, Train_Loss: 0.0073, Test_Loss: 0.0103, Train_Acc:0.9133, Test_Acc: 0.8333\n",
            "Epoch: 235, Train_Loss: 0.0073, Test_Loss: 0.0115, Train_Acc:0.9149, Test_Acc: 0.8421\n",
            "Epoch: 236, Train_Loss: 0.0074, Test_Loss: 0.0109, Train_Acc:0.9149, Test_Acc: 0.8377\n",
            "Epoch: 237, Train_Loss: 0.0067, Test_Loss: 0.0113, Train_Acc:0.8931, Test_Acc: 0.8377\n",
            "Epoch: 238, Train_Loss: 0.0069, Test_Loss: 0.0110, Train_Acc:0.8915, Test_Acc: 0.8421\n",
            "Epoch: 239, Train_Loss: 0.0068, Test_Loss: 0.0110, Train_Acc:0.9069, Test_Acc: 0.8465\n",
            "Epoch: 240, Train_Loss: 0.0068, Test_Loss: 0.0118, Train_Acc:0.8580, Test_Acc: 0.7939\n",
            "Epoch: 241, Train_Loss: 0.0073, Test_Loss: 0.0108, Train_Acc:0.9016, Test_Acc: 0.8684\n",
            "Epoch: 242, Train_Loss: 0.0085, Test_Loss: 0.0123, Train_Acc:0.8420, Test_Acc: 0.8092\n",
            "Epoch: 243, Train_Loss: 0.0100, Test_Loss: 0.0113, Train_Acc:0.9064, Test_Acc: 0.8246\n",
            "Epoch: 244, Train_Loss: 0.0068, Test_Loss: 0.0116, Train_Acc:0.9016, Test_Acc: 0.8004\n",
            "Epoch: 245, Train_Loss: 0.0077, Test_Loss: 0.0132, Train_Acc:0.8521, Test_Acc: 0.7785\n",
            "Epoch: 246, Train_Loss: 0.0091, Test_Loss: 0.0106, Train_Acc:0.9170, Test_Acc: 0.8728\n",
            "Epoch: 247, Train_Loss: 0.0072, Test_Loss: 0.0114, Train_Acc:0.9037, Test_Acc: 0.8465\n",
            "Epoch: 248, Train_Loss: 0.0079, Test_Loss: 0.0112, Train_Acc:0.8968, Test_Acc: 0.8311\n",
            "Epoch: 249, Train_Loss: 0.0074, Test_Loss: 0.0102, Train_Acc:0.9218, Test_Acc: 0.8618\n",
            "Epoch: 250, Train_Loss: 0.0070, Test_Loss: 0.0110, Train_Acc:0.8851, Test_Acc: 0.8421\n",
            "Epoch: 251, Train_Loss: 0.0079, Test_Loss: 0.0118, Train_Acc:0.8846, Test_Acc: 0.8531\n",
            "Epoch: 252, Train_Loss: 0.0073, Test_Loss: 0.0112, Train_Acc:0.8771, Test_Acc: 0.8465\n",
            "Epoch: 253, Train_Loss: 0.0074, Test_Loss: 0.0113, Train_Acc:0.8947, Test_Acc: 0.8333\n",
            "Epoch: 254, Train_Loss: 0.0071, Test_Loss: 0.0112, Train_Acc:0.9064, Test_Acc: 0.8509\n",
            "Epoch: 255, Train_Loss: 0.0069, Test_Loss: 0.0107, Train_Acc:0.9138, Test_Acc: 0.8355\n",
            "Epoch: 256, Train_Loss: 0.0073, Test_Loss: 0.0106, Train_Acc:0.9160, Test_Acc: 0.8509\n",
            "Epoch: 257, Train_Loss: 0.0066, Test_Loss: 0.0104, Train_Acc:0.9202, Test_Acc: 0.8553\n",
            "Epoch: 258, Train_Loss: 0.0066, Test_Loss: 0.0113, Train_Acc:0.9117, Test_Acc: 0.8224\n",
            "Epoch: 259, Train_Loss: 0.0062, Test_Loss: 0.0117, Train_Acc:0.8995, Test_Acc: 0.8224\n",
            "Epoch: 260, Train_Loss: 0.0060, Test_Loss: 0.0116, Train_Acc:0.9101, Test_Acc: 0.8289\n",
            "Epoch: 261, Train_Loss: 0.0060, Test_Loss: 0.0116, Train_Acc:0.9133, Test_Acc: 0.8311\n",
            "Epoch: 262, Train_Loss: 0.0060, Test_Loss: 0.0111, Train_Acc:0.9117, Test_Acc: 0.8333\n",
            "Epoch: 263, Train_Loss: 0.0062, Test_Loss: 0.0106, Train_Acc:0.9287, Test_Acc: 0.8618\n",
            "Epoch: 264, Train_Loss: 0.0056, Test_Loss: 0.0112, Train_Acc:0.9202, Test_Acc: 0.8575\n",
            "Epoch: 265, Train_Loss: 0.0063, Test_Loss: 0.0116, Train_Acc:0.8973, Test_Acc: 0.8443\n",
            "Epoch: 266, Train_Loss: 0.0059, Test_Loss: 0.0117, Train_Acc:0.9106, Test_Acc: 0.8355\n",
            "Epoch: 267, Train_Loss: 0.0054, Test_Loss: 0.0120, Train_Acc:0.9170, Test_Acc: 0.8443\n",
            "Epoch: 268, Train_Loss: 0.0060, Test_Loss: 0.0116, Train_Acc:0.9037, Test_Acc: 0.8246\n",
            "Epoch: 269, Train_Loss: 0.0054, Test_Loss: 0.0121, Train_Acc:0.8888, Test_Acc: 0.8224\n",
            "Epoch: 270, Train_Loss: 0.0066, Test_Loss: 0.0131, Train_Acc:0.8670, Test_Acc: 0.7982\n",
            "Epoch: 271, Train_Loss: 0.0065, Test_Loss: 0.0126, Train_Acc:0.9048, Test_Acc: 0.8465\n",
            "Epoch: 272, Train_Loss: 0.0064, Test_Loss: 0.0115, Train_Acc:0.8941, Test_Acc: 0.8399\n",
            "Epoch: 273, Train_Loss: 0.0065, Test_Loss: 0.0115, Train_Acc:0.8803, Test_Acc: 0.8246\n",
            "Epoch: 274, Train_Loss: 0.0061, Test_Loss: 0.0114, Train_Acc:0.8894, Test_Acc: 0.8487\n",
            "Epoch: 275, Train_Loss: 0.0062, Test_Loss: 0.0118, Train_Acc:0.8394, Test_Acc: 0.8026\n",
            "Epoch: 276, Train_Loss: 0.0064, Test_Loss: 0.0124, Train_Acc:0.9080, Test_Acc: 0.8246\n",
            "Epoch: 277, Train_Loss: 0.0054, Test_Loss: 0.0125, Train_Acc:0.9011, Test_Acc: 0.8202\n",
            "Epoch: 278, Train_Loss: 0.0056, Test_Loss: 0.0135, Train_Acc:0.8266, Test_Acc: 0.7654\n",
            "Epoch: 279, Train_Loss: 0.0064, Test_Loss: 0.0138, Train_Acc:0.8362, Test_Acc: 0.7654\n",
            "Epoch: 280, Train_Loss: 0.0054, Test_Loss: 0.0131, Train_Acc:0.8883, Test_Acc: 0.8092\n",
            "Epoch: 281, Train_Loss: 0.0052, Test_Loss: 0.0140, Train_Acc:0.8245, Test_Acc: 0.7588\n",
            "Epoch: 282, Train_Loss: 0.0061, Test_Loss: 0.0148, Train_Acc:0.8532, Test_Acc: 0.7675\n",
            "Epoch: 283, Train_Loss: 0.0055, Test_Loss: 0.0137, Train_Acc:0.8559, Test_Acc: 0.7763\n",
            "Epoch: 284, Train_Loss: 0.0055, Test_Loss: 0.0128, Train_Acc:0.8574, Test_Acc: 0.7741\n",
            "Epoch: 285, Train_Loss: 0.0057, Test_Loss: 0.0126, Train_Acc:0.8766, Test_Acc: 0.8070\n",
            "Epoch: 286, Train_Loss: 0.0058, Test_Loss: 0.0127, Train_Acc:0.8819, Test_Acc: 0.8114\n",
            "Epoch: 287, Train_Loss: 0.0053, Test_Loss: 0.0130, Train_Acc:0.8835, Test_Acc: 0.8048\n",
            "Epoch: 288, Train_Loss: 0.0052, Test_Loss: 0.0118, Train_Acc:0.9043, Test_Acc: 0.8289\n",
            "Epoch: 289, Train_Loss: 0.0054, Test_Loss: 0.0116, Train_Acc:0.9117, Test_Acc: 0.8399\n",
            "Epoch: 290, Train_Loss: 0.0049, Test_Loss: 0.0128, Train_Acc:0.8676, Test_Acc: 0.8026\n",
            "Epoch: 291, Train_Loss: 0.0053, Test_Loss: 0.0119, Train_Acc:0.8931, Test_Acc: 0.8333\n",
            "Epoch: 292, Train_Loss: 0.0051, Test_Loss: 0.0115, Train_Acc:0.9160, Test_Acc: 0.8421\n",
            "Epoch: 293, Train_Loss: 0.0050, Test_Loss: 0.0107, Train_Acc:0.9064, Test_Acc: 0.8311\n",
            "Epoch: 294, Train_Loss: 0.0059, Test_Loss: 0.0135, Train_Acc:0.8824, Test_Acc: 0.7917\n",
            "Epoch: 295, Train_Loss: 0.0055, Test_Loss: 0.0120, Train_Acc:0.9117, Test_Acc: 0.8136\n",
            "Epoch: 296, Train_Loss: 0.0044, Test_Loss: 0.0110, Train_Acc:0.9351, Test_Acc: 0.8465\n",
            "Epoch: 297, Train_Loss: 0.0047, Test_Loss: 0.0120, Train_Acc:0.9016, Test_Acc: 0.8180\n",
            "Epoch: 298, Train_Loss: 0.0058, Test_Loss: 0.0114, Train_Acc:0.8888, Test_Acc: 0.8224\n",
            "Epoch: 299, Train_Loss: 0.0056, Test_Loss: 0.0100, Train_Acc:0.8989, Test_Acc: 0.8289\n",
            "Epoch: 300, Train_Loss: 0.0052, Test_Loss: 0.0102, Train_Acc:0.9314, Test_Acc: 0.8553\n",
            "Epoch: 301, Train_Loss: 0.0047, Test_Loss: 0.0101, Train_Acc:0.9293, Test_Acc: 0.8553\n",
            "Epoch: 302, Train_Loss: 0.0042, Test_Loss: 0.0105, Train_Acc:0.9085, Test_Acc: 0.8333\n",
            "Epoch: 303, Train_Loss: 0.0045, Test_Loss: 0.0101, Train_Acc:0.9335, Test_Acc: 0.8509\n",
            "Epoch: 304, Train_Loss: 0.0045, Test_Loss: 0.0103, Train_Acc:0.9191, Test_Acc: 0.8509\n",
            "Epoch: 305, Train_Loss: 0.0046, Test_Loss: 0.0118, Train_Acc:0.9043, Test_Acc: 0.8399\n",
            "Epoch: 306, Train_Loss: 0.0049, Test_Loss: 0.0115, Train_Acc:0.9229, Test_Acc: 0.8377\n",
            "Epoch: 307, Train_Loss: 0.0041, Test_Loss: 0.0118, Train_Acc:0.9335, Test_Acc: 0.8421\n",
            "Epoch: 308, Train_Loss: 0.0052, Test_Loss: 0.0126, Train_Acc:0.8771, Test_Acc: 0.8004\n",
            "Epoch: 309, Train_Loss: 0.0056, Test_Loss: 0.0136, Train_Acc:0.8527, Test_Acc: 0.7873\n",
            "Epoch: 310, Train_Loss: 0.0063, Test_Loss: 0.0108, Train_Acc:0.8936, Test_Acc: 0.8202\n",
            "Epoch: 311, Train_Loss: 0.0052, Test_Loss: 0.0110, Train_Acc:0.9191, Test_Acc: 0.8377\n",
            "Epoch: 312, Train_Loss: 0.0060, Test_Loss: 0.0105, Train_Acc:0.8920, Test_Acc: 0.8377\n",
            "Epoch: 313, Train_Loss: 0.0057, Test_Loss: 0.0125, Train_Acc:0.8559, Test_Acc: 0.8158\n",
            "Epoch: 314, Train_Loss: 0.0057, Test_Loss: 0.0134, Train_Acc:0.8713, Test_Acc: 0.8114\n",
            "Epoch: 315, Train_Loss: 0.0060, Test_Loss: 0.0115, Train_Acc:0.8798, Test_Acc: 0.8070\n",
            "Epoch: 316, Train_Loss: 0.0051, Test_Loss: 0.0111, Train_Acc:0.9048, Test_Acc: 0.8202\n",
            "Epoch: 317, Train_Loss: 0.0045, Test_Loss: 0.0115, Train_Acc:0.9016, Test_Acc: 0.8443\n",
            "Epoch: 318, Train_Loss: 0.0049, Test_Loss: 0.0099, Train_Acc:0.9176, Test_Acc: 0.8509\n",
            "Epoch: 319, Train_Loss: 0.0045, Test_Loss: 0.0109, Train_Acc:0.9271, Test_Acc: 0.8465\n",
            "Epoch: 320, Train_Loss: 0.0044, Test_Loss: 0.0115, Train_Acc:0.9160, Test_Acc: 0.8355\n",
            "Epoch: 321, Train_Loss: 0.0046, Test_Loss: 0.0112, Train_Acc:0.9112, Test_Acc: 0.8553\n",
            "Epoch: 322, Train_Loss: 0.0041, Test_Loss: 0.0107, Train_Acc:0.9080, Test_Acc: 0.8706\n",
            "Epoch: 323, Train_Loss: 0.0044, Test_Loss: 0.0107, Train_Acc:0.9165, Test_Acc: 0.8399\n",
            "Epoch: 324, Train_Loss: 0.0047, Test_Loss: 0.0120, Train_Acc:0.9011, Test_Acc: 0.7873\n",
            "Epoch: 325, Train_Loss: 0.0042, Test_Loss: 0.0122, Train_Acc:0.8798, Test_Acc: 0.7697\n",
            "Epoch: 326, Train_Loss: 0.0042, Test_Loss: 0.0131, Train_Acc:0.9202, Test_Acc: 0.7895\n",
            "Epoch: 327, Train_Loss: 0.0040, Test_Loss: 0.0117, Train_Acc:0.8824, Test_Acc: 0.7917\n",
            "Epoch: 328, Train_Loss: 0.0047, Test_Loss: 0.0117, Train_Acc:0.9324, Test_Acc: 0.8399\n",
            "Epoch: 329, Train_Loss: 0.0049, Test_Loss: 0.0124, Train_Acc:0.9090, Test_Acc: 0.8158\n",
            "Epoch: 330, Train_Loss: 0.0040, Test_Loss: 0.0103, Train_Acc:0.9207, Test_Acc: 0.8421\n",
            "Epoch: 331, Train_Loss: 0.0045, Test_Loss: 0.0102, Train_Acc:0.9457, Test_Acc: 0.8662\n",
            "Epoch: 332, Train_Loss: 0.0039, Test_Loss: 0.0104, Train_Acc:0.9378, Test_Acc: 0.8640\n",
            "Epoch: 333, Train_Loss: 0.0041, Test_Loss: 0.0105, Train_Acc:0.9410, Test_Acc: 0.8333\n",
            "Epoch: 334, Train_Loss: 0.0043, Test_Loss: 0.0112, Train_Acc:0.8910, Test_Acc: 0.8180\n",
            "Epoch: 335, Train_Loss: 0.0049, Test_Loss: 0.0099, Train_Acc:0.9223, Test_Acc: 0.8640\n",
            "Epoch: 336, Train_Loss: 0.0048, Test_Loss: 0.0089, Train_Acc:0.9447, Test_Acc: 0.8860\n",
            "Epoch: 337, Train_Loss: 0.0040, Test_Loss: 0.0091, Train_Acc:0.9426, Test_Acc: 0.8662\n",
            "Epoch: 338, Train_Loss: 0.0038, Test_Loss: 0.0107, Train_Acc:0.9287, Test_Acc: 0.8640\n",
            "Epoch: 339, Train_Loss: 0.0050, Test_Loss: 0.0095, Train_Acc:0.9287, Test_Acc: 0.8596\n",
            "Epoch: 340, Train_Loss: 0.0042, Test_Loss: 0.0090, Train_Acc:0.9388, Test_Acc: 0.8662\n",
            "Epoch: 341, Train_Loss: 0.0042, Test_Loss: 0.0091, Train_Acc:0.9314, Test_Acc: 0.8575\n",
            "Epoch: 342, Train_Loss: 0.0041, Test_Loss: 0.0096, Train_Acc:0.9181, Test_Acc: 0.8465\n",
            "Epoch: 343, Train_Loss: 0.0037, Test_Loss: 0.0093, Train_Acc:0.9239, Test_Acc: 0.8684\n",
            "Epoch: 344, Train_Loss: 0.0043, Test_Loss: 0.0085, Train_Acc:0.9351, Test_Acc: 0.8684\n",
            "Epoch: 345, Train_Loss: 0.0035, Test_Loss: 0.0087, Train_Acc:0.9335, Test_Acc: 0.8794\n",
            "Epoch: 346, Train_Loss: 0.0043, Test_Loss: 0.0088, Train_Acc:0.9489, Test_Acc: 0.8816\n",
            "Epoch: 347, Train_Loss: 0.0035, Test_Loss: 0.0096, Train_Acc:0.9234, Test_Acc: 0.8487\n",
            "Epoch: 348, Train_Loss: 0.0041, Test_Loss: 0.0099, Train_Acc:0.9426, Test_Acc: 0.8596\n",
            "Epoch: 349, Train_Loss: 0.0036, Test_Loss: 0.0097, Train_Acc:0.9441, Test_Acc: 0.8640\n",
            "Epoch: 350, Train_Loss: 0.0037, Test_Loss: 0.0091, Train_Acc:0.9367, Test_Acc: 0.8487\n",
            "Epoch: 351, Train_Loss: 0.0039, Test_Loss: 0.0106, Train_Acc:0.9351, Test_Acc: 0.8311\n",
            "Epoch: 352, Train_Loss: 0.0039, Test_Loss: 0.0111, Train_Acc:0.9250, Test_Acc: 0.8246\n",
            "Epoch: 353, Train_Loss: 0.0036, Test_Loss: 0.0104, Train_Acc:0.9309, Test_Acc: 0.8202\n",
            "Epoch: 354, Train_Loss: 0.0031, Test_Loss: 0.0100, Train_Acc:0.9457, Test_Acc: 0.8531\n",
            "Epoch: 355, Train_Loss: 0.0035, Test_Loss: 0.0104, Train_Acc:0.9298, Test_Acc: 0.8509\n",
            "Epoch: 356, Train_Loss: 0.0034, Test_Loss: 0.0099, Train_Acc:0.9372, Test_Acc: 0.8618\n",
            "Epoch: 357, Train_Loss: 0.0035, Test_Loss: 0.0103, Train_Acc:0.9383, Test_Acc: 0.8575\n",
            "Epoch: 358, Train_Loss: 0.0040, Test_Loss: 0.0104, Train_Acc:0.9351, Test_Acc: 0.8684\n",
            "Epoch: 359, Train_Loss: 0.0034, Test_Loss: 0.0100, Train_Acc:0.9394, Test_Acc: 0.8618\n",
            "Epoch: 360, Train_Loss: 0.0034, Test_Loss: 0.0106, Train_Acc:0.9426, Test_Acc: 0.8531\n",
            "Epoch: 361, Train_Loss: 0.0033, Test_Loss: 0.0109, Train_Acc:0.9404, Test_Acc: 0.8794\n",
            "Epoch: 362, Train_Loss: 0.0034, Test_Loss: 0.0097, Train_Acc:0.9112, Test_Acc: 0.8684\n",
            "Epoch: 363, Train_Loss: 0.0039, Test_Loss: 0.0104, Train_Acc:0.9282, Test_Acc: 0.8596\n",
            "Epoch: 364, Train_Loss: 0.0030, Test_Loss: 0.0115, Train_Acc:0.9165, Test_Acc: 0.8377\n",
            "Epoch: 365, Train_Loss: 0.0040, Test_Loss: 0.0099, Train_Acc:0.9128, Test_Acc: 0.8509\n",
            "Epoch: 366, Train_Loss: 0.0044, Test_Loss: 0.0096, Train_Acc:0.9356, Test_Acc: 0.8794\n",
            "Epoch: 367, Train_Loss: 0.0036, Test_Loss: 0.0106, Train_Acc:0.9351, Test_Acc: 0.8618\n",
            "Epoch: 368, Train_Loss: 0.0038, Test_Loss: 0.0099, Train_Acc:0.9420, Test_Acc: 0.8640\n",
            "Epoch: 369, Train_Loss: 0.0039, Test_Loss: 0.0106, Train_Acc:0.9277, Test_Acc: 0.8443\n",
            "Epoch: 370, Train_Loss: 0.0033, Test_Loss: 0.0108, Train_Acc:0.9324, Test_Acc: 0.8377\n",
            "Epoch: 371, Train_Loss: 0.0040, Test_Loss: 0.0101, Train_Acc:0.9505, Test_Acc: 0.8575\n",
            "Epoch: 372, Train_Loss: 0.0039, Test_Loss: 0.0101, Train_Acc:0.9441, Test_Acc: 0.8772\n",
            "Epoch: 373, Train_Loss: 0.0040, Test_Loss: 0.0105, Train_Acc:0.9457, Test_Acc: 0.8509\n",
            "Epoch: 374, Train_Loss: 0.0037, Test_Loss: 0.0099, Train_Acc:0.9319, Test_Acc: 0.8531\n",
            "Epoch: 375, Train_Loss: 0.0041, Test_Loss: 0.0091, Train_Acc:0.9420, Test_Acc: 0.8596\n",
            "Epoch: 376, Train_Loss: 0.0037, Test_Loss: 0.0091, Train_Acc:0.9378, Test_Acc: 0.8794\n",
            "Epoch: 377, Train_Loss: 0.0039, Test_Loss: 0.0088, Train_Acc:0.9388, Test_Acc: 0.8706\n",
            "Epoch: 378, Train_Loss: 0.0039, Test_Loss: 0.0100, Train_Acc:0.9372, Test_Acc: 0.8575\n",
            "Epoch: 379, Train_Loss: 0.0039, Test_Loss: 0.0103, Train_Acc:0.9441, Test_Acc: 0.8443\n",
            "Epoch: 380, Train_Loss: 0.0044, Test_Loss: 0.0093, Train_Acc:0.9468, Test_Acc: 0.8618\n",
            "Epoch: 381, Train_Loss: 0.0034, Test_Loss: 0.0093, Train_Acc:0.9404, Test_Acc: 0.8531\n",
            "Epoch: 382, Train_Loss: 0.0041, Test_Loss: 0.0086, Train_Acc:0.9426, Test_Acc: 0.8904\n",
            "Epoch: 383, Train_Loss: 0.0041, Test_Loss: 0.0085, Train_Acc:0.9441, Test_Acc: 0.8816\n",
            "Epoch: 384, Train_Loss: 0.0036, Test_Loss: 0.0081, Train_Acc:0.9516, Test_Acc: 0.8947\n",
            "Epoch: 385, Train_Loss: 0.0034, Test_Loss: 0.0091, Train_Acc:0.9468, Test_Acc: 0.8728\n",
            "Epoch: 386, Train_Loss: 0.0039, Test_Loss: 0.0100, Train_Acc:0.9410, Test_Acc: 0.8772\n",
            "Epoch: 387, Train_Loss: 0.0040, Test_Loss: 0.0089, Train_Acc:0.9495, Test_Acc: 0.8925\n",
            "Epoch: 388, Train_Loss: 0.0037, Test_Loss: 0.0092, Train_Acc:0.9569, Test_Acc: 0.8969\n",
            "Epoch: 389, Train_Loss: 0.0032, Test_Loss: 0.0091, Train_Acc:0.9564, Test_Acc: 0.8816\n",
            "Epoch: 390, Train_Loss: 0.0038, Test_Loss: 0.0090, Train_Acc:0.9548, Test_Acc: 0.8553\n",
            "Epoch: 391, Train_Loss: 0.0040, Test_Loss: 0.0092, Train_Acc:0.9479, Test_Acc: 0.8772\n",
            "Epoch: 392, Train_Loss: 0.0034, Test_Loss: 0.0093, Train_Acc:0.9457, Test_Acc: 0.8860\n",
            "Epoch: 393, Train_Loss: 0.0037, Test_Loss: 0.0091, Train_Acc:0.9543, Test_Acc: 0.8860\n",
            "Epoch: 394, Train_Loss: 0.0035, Test_Loss: 0.0104, Train_Acc:0.9495, Test_Acc: 0.8706\n",
            "Epoch: 395, Train_Loss: 0.0041, Test_Loss: 0.0100, Train_Acc:0.9606, Test_Acc: 0.8640\n",
            "Epoch: 396, Train_Loss: 0.0036, Test_Loss: 0.0097, Train_Acc:0.9457, Test_Acc: 0.8728\n",
            "Epoch: 397, Train_Loss: 0.0033, Test_Loss: 0.0094, Train_Acc:0.9436, Test_Acc: 0.8618\n",
            "Epoch: 398, Train_Loss: 0.0033, Test_Loss: 0.0104, Train_Acc:0.9452, Test_Acc: 0.8684\n",
            "Epoch: 399, Train_Loss: 0.0032, Test_Loss: 0.0112, Train_Acc:0.9330, Test_Acc: 0.8509\n",
            "Epoch: 400, Train_Loss: 0.0034, Test_Loss: 0.0100, Train_Acc:0.9404, Test_Acc: 0.8531\n",
            "Epoch: 401, Train_Loss: 0.0029, Test_Loss: 0.0099, Train_Acc:0.9388, Test_Acc: 0.8509\n",
            "Epoch: 402, Train_Loss: 0.0032, Test_Loss: 0.0102, Train_Acc:0.9441, Test_Acc: 0.8662\n",
            "Epoch: 403, Train_Loss: 0.0034, Test_Loss: 0.0089, Train_Acc:0.9335, Test_Acc: 0.8706\n",
            "Epoch: 404, Train_Loss: 0.0030, Test_Loss: 0.0097, Train_Acc:0.9463, Test_Acc: 0.8728\n",
            "Epoch: 405, Train_Loss: 0.0031, Test_Loss: 0.0095, Train_Acc:0.9271, Test_Acc: 0.8618\n",
            "Epoch: 406, Train_Loss: 0.0032, Test_Loss: 0.0091, Train_Acc:0.9335, Test_Acc: 0.8487\n",
            "Epoch: 407, Train_Loss: 0.0033, Test_Loss: 0.0096, Train_Acc:0.9452, Test_Acc: 0.8618\n",
            "Epoch: 408, Train_Loss: 0.0032, Test_Loss: 0.0090, Train_Acc:0.9293, Test_Acc: 0.8531\n",
            "Epoch: 409, Train_Loss: 0.0033, Test_Loss: 0.0100, Train_Acc:0.9532, Test_Acc: 0.8640\n",
            "Epoch: 410, Train_Loss: 0.0031, Test_Loss: 0.0106, Train_Acc:0.9436, Test_Acc: 0.8531\n",
            "Epoch: 411, Train_Loss: 0.0037, Test_Loss: 0.0100, Train_Acc:0.9255, Test_Acc: 0.8575\n",
            "Epoch: 412, Train_Loss: 0.0036, Test_Loss: 0.0085, Train_Acc:0.9543, Test_Acc: 0.8772\n",
            "Epoch: 413, Train_Loss: 0.0032, Test_Loss: 0.0092, Train_Acc:0.9564, Test_Acc: 0.8684\n",
            "Epoch: 414, Train_Loss: 0.0029, Test_Loss: 0.0090, Train_Acc:0.9495, Test_Acc: 0.8531\n",
            "Epoch: 415, Train_Loss: 0.0031, Test_Loss: 0.0093, Train_Acc:0.9548, Test_Acc: 0.8706\n",
            "Epoch: 416, Train_Loss: 0.0034, Test_Loss: 0.0092, Train_Acc:0.9585, Test_Acc: 0.8772\n",
            "Epoch: 417, Train_Loss: 0.0029, Test_Loss: 0.0093, Train_Acc:0.9612, Test_Acc: 0.8838\n",
            "Epoch: 418, Train_Loss: 0.0032, Test_Loss: 0.0094, Train_Acc:0.9585, Test_Acc: 0.8816\n",
            "Epoch: 419, Train_Loss: 0.0035, Test_Loss: 0.0102, Train_Acc:0.9426, Test_Acc: 0.8640\n",
            "Epoch: 420, Train_Loss: 0.0038, Test_Loss: 0.0091, Train_Acc:0.9473, Test_Acc: 0.8640\n",
            "Epoch: 421, Train_Loss: 0.0038, Test_Loss: 0.0090, Train_Acc:0.9564, Test_Acc: 0.8728\n",
            "Epoch: 422, Train_Loss: 0.0036, Test_Loss: 0.0081, Train_Acc:0.9574, Test_Acc: 0.8838\n",
            "Epoch: 423, Train_Loss: 0.0035, Test_Loss: 0.0088, Train_Acc:0.9452, Test_Acc: 0.8860\n",
            "Epoch: 424, Train_Loss: 0.0035, Test_Loss: 0.0090, Train_Acc:0.9479, Test_Acc: 0.8750\n",
            "Epoch: 425, Train_Loss: 0.0036, Test_Loss: 0.0086, Train_Acc:0.9420, Test_Acc: 0.8838\n",
            "Epoch: 426, Train_Loss: 0.0042, Test_Loss: 0.0096, Train_Acc:0.9457, Test_Acc: 0.8772\n",
            "Epoch: 427, Train_Loss: 0.0039, Test_Loss: 0.0090, Train_Acc:0.9537, Test_Acc: 0.8684\n",
            "Epoch: 428, Train_Loss: 0.0037, Test_Loss: 0.0081, Train_Acc:0.9707, Test_Acc: 0.8772\n",
            "Epoch: 429, Train_Loss: 0.0032, Test_Loss: 0.0082, Train_Acc:0.9564, Test_Acc: 0.8772\n",
            "Epoch: 430, Train_Loss: 0.0033, Test_Loss: 0.0092, Train_Acc:0.9622, Test_Acc: 0.8838\n",
            "Epoch: 431, Train_Loss: 0.0037, Test_Loss: 0.0083, Train_Acc:0.9447, Test_Acc: 0.8816\n",
            "Epoch: 432, Train_Loss: 0.0032, Test_Loss: 0.0097, Train_Acc:0.9399, Test_Acc: 0.8706\n",
            "Epoch: 433, Train_Loss: 0.0037, Test_Loss: 0.0108, Train_Acc:0.9457, Test_Acc: 0.8618\n",
            "Epoch: 434, Train_Loss: 0.0037, Test_Loss: 0.0102, Train_Acc:0.9335, Test_Acc: 0.8596\n",
            "Epoch: 435, Train_Loss: 0.0035, Test_Loss: 0.0093, Train_Acc:0.9255, Test_Acc: 0.8640\n",
            "Epoch: 436, Train_Loss: 0.0036, Test_Loss: 0.0110, Train_Acc:0.9351, Test_Acc: 0.8509\n",
            "Epoch: 437, Train_Loss: 0.0039, Test_Loss: 0.0098, Train_Acc:0.9468, Test_Acc: 0.8575\n",
            "Epoch: 438, Train_Loss: 0.0031, Test_Loss: 0.0103, Train_Acc:0.9447, Test_Acc: 0.8553\n",
            "Epoch: 439, Train_Loss: 0.0039, Test_Loss: 0.0089, Train_Acc:0.9479, Test_Acc: 0.8772\n",
            "Epoch: 440, Train_Loss: 0.0034, Test_Loss: 0.0089, Train_Acc:0.9287, Test_Acc: 0.8684\n",
            "Epoch: 441, Train_Loss: 0.0040, Test_Loss: 0.0096, Train_Acc:0.9239, Test_Acc: 0.8465\n",
            "Epoch: 442, Train_Loss: 0.0037, Test_Loss: 0.0101, Train_Acc:0.9229, Test_Acc: 0.8509\n",
            "Epoch: 443, Train_Loss: 0.0044, Test_Loss: 0.0090, Train_Acc:0.9500, Test_Acc: 0.8794\n",
            "Epoch: 444, Train_Loss: 0.0036, Test_Loss: 0.0083, Train_Acc:0.9447, Test_Acc: 0.8816\n",
            "Epoch: 445, Train_Loss: 0.0041, Test_Loss: 0.0083, Train_Acc:0.9489, Test_Acc: 0.8925\n",
            "Epoch: 446, Train_Loss: 0.0040, Test_Loss: 0.0077, Train_Acc:0.9537, Test_Acc: 0.8947\n",
            "Epoch: 447, Train_Loss: 0.0037, Test_Loss: 0.0085, Train_Acc:0.9298, Test_Acc: 0.8575\n",
            "Epoch: 448, Train_Loss: 0.0043, Test_Loss: 0.0088, Train_Acc:0.9234, Test_Acc: 0.8794\n",
            "Epoch: 449, Train_Loss: 0.0037, Test_Loss: 0.0083, Train_Acc:0.9500, Test_Acc: 0.8794\n",
            "Epoch: 450, Train_Loss: 0.0040, Test_Loss: 0.0088, Train_Acc:0.9388, Test_Acc: 0.8750\n",
            "Epoch: 451, Train_Loss: 0.0035, Test_Loss: 0.0092, Train_Acc:0.9527, Test_Acc: 0.8706\n",
            "Epoch: 452, Train_Loss: 0.0034, Test_Loss: 0.0103, Train_Acc:0.9394, Test_Acc: 0.8575\n",
            "Epoch: 453, Train_Loss: 0.0042, Test_Loss: 0.0095, Train_Acc:0.9394, Test_Acc: 0.8662\n",
            "Epoch: 454, Train_Loss: 0.0037, Test_Loss: 0.0087, Train_Acc:0.9479, Test_Acc: 0.8706\n",
            "Epoch: 455, Train_Loss: 0.0036, Test_Loss: 0.0082, Train_Acc:0.9537, Test_Acc: 0.8860\n",
            "Epoch: 456, Train_Loss: 0.0035, Test_Loss: 0.0075, Train_Acc:0.9649, Test_Acc: 0.8904\n",
            "Epoch: 457, Train_Loss: 0.0030, Test_Loss: 0.0085, Train_Acc:0.9590, Test_Acc: 0.8925\n",
            "Epoch: 458, Train_Loss: 0.0035, Test_Loss: 0.0084, Train_Acc:0.9505, Test_Acc: 0.8684\n",
            "Epoch: 459, Train_Loss: 0.0037, Test_Loss: 0.0079, Train_Acc:0.9590, Test_Acc: 0.9035\n",
            "Epoch: 460, Train_Loss: 0.0032, Test_Loss: 0.0090, Train_Acc:0.9505, Test_Acc: 0.8706\n",
            "Epoch: 461, Train_Loss: 0.0035, Test_Loss: 0.0099, Train_Acc:0.9484, Test_Acc: 0.8443\n",
            "Epoch: 462, Train_Loss: 0.0032, Test_Loss: 0.0103, Train_Acc:0.8931, Test_Acc: 0.8048\n",
            "Epoch: 463, Train_Loss: 0.0035, Test_Loss: 0.0099, Train_Acc:0.9452, Test_Acc: 0.8531\n",
            "Epoch: 464, Train_Loss: 0.0029, Test_Loss: 0.0089, Train_Acc:0.9574, Test_Acc: 0.8684\n",
            "Epoch: 465, Train_Loss: 0.0029, Test_Loss: 0.0085, Train_Acc:0.9606, Test_Acc: 0.8816\n",
            "Epoch: 466, Train_Loss: 0.0033, Test_Loss: 0.0104, Train_Acc:0.9441, Test_Acc: 0.8355\n",
            "Epoch: 467, Train_Loss: 0.0037, Test_Loss: 0.0099, Train_Acc:0.9473, Test_Acc: 0.8509\n",
            "Epoch: 468, Train_Loss: 0.0030, Test_Loss: 0.0097, Train_Acc:0.9553, Test_Acc: 0.8575\n",
            "Epoch: 469, Train_Loss: 0.0031, Test_Loss: 0.0100, Train_Acc:0.9505, Test_Acc: 0.8575\n",
            "Epoch: 470, Train_Loss: 0.0032, Test_Loss: 0.0093, Train_Acc:0.9521, Test_Acc: 0.8816\n",
            "Epoch: 471, Train_Loss: 0.0035, Test_Loss: 0.0093, Train_Acc:0.9606, Test_Acc: 0.8816\n",
            "Epoch: 472, Train_Loss: 0.0030, Test_Loss: 0.0091, Train_Acc:0.9543, Test_Acc: 0.8794\n",
            "Epoch: 473, Train_Loss: 0.0032, Test_Loss: 0.0104, Train_Acc:0.9553, Test_Acc: 0.8355\n",
            "Epoch: 474, Train_Loss: 0.0037, Test_Loss: 0.0087, Train_Acc:0.9516, Test_Acc: 0.8662\n",
            "Epoch: 475, Train_Loss: 0.0036, Test_Loss: 0.0087, Train_Acc:0.9601, Test_Acc: 0.9013\n",
            "Epoch: 476, Train_Loss: 0.0033, Test_Loss: 0.0088, Train_Acc:0.9585, Test_Acc: 0.8904\n",
            "Epoch: 477, Train_Loss: 0.0031, Test_Loss: 0.0083, Train_Acc:0.9574, Test_Acc: 0.8925\n",
            "Epoch: 478, Train_Loss: 0.0032, Test_Loss: 0.0095, Train_Acc:0.9351, Test_Acc: 0.8596\n",
            "Epoch: 479, Train_Loss: 0.0037, Test_Loss: 0.0084, Train_Acc:0.9564, Test_Acc: 0.8816\n",
            "Epoch: 480, Train_Loss: 0.0031, Test_Loss: 0.0083, Train_Acc:0.9489, Test_Acc: 0.8904\n",
            "Epoch: 481, Train_Loss: 0.0029, Test_Loss: 0.0085, Train_Acc:0.9633, Test_Acc: 0.8991\n",
            "Epoch: 482, Train_Loss: 0.0031, Test_Loss: 0.0074, Train_Acc:0.9564, Test_Acc: 0.9101\n",
            "Epoch: 483, Train_Loss: 0.0030, Test_Loss: 0.0079, Train_Acc:0.9473, Test_Acc: 0.8816\n",
            "Epoch: 484, Train_Loss: 0.0033, Test_Loss: 0.0096, Train_Acc:0.9426, Test_Acc: 0.8750\n",
            "Epoch: 485, Train_Loss: 0.0036, Test_Loss: 0.0084, Train_Acc:0.9612, Test_Acc: 0.8882\n",
            "Epoch: 486, Train_Loss: 0.0030, Test_Loss: 0.0078, Train_Acc:0.9553, Test_Acc: 0.8969\n",
            "Epoch: 487, Train_Loss: 0.0032, Test_Loss: 0.0085, Train_Acc:0.9612, Test_Acc: 0.8882\n",
            "Epoch: 488, Train_Loss: 0.0033, Test_Loss: 0.0085, Train_Acc:0.9617, Test_Acc: 0.8882\n",
            "Epoch: 489, Train_Loss: 0.0032, Test_Loss: 0.0084, Train_Acc:0.9580, Test_Acc: 0.8750\n",
            "Epoch: 490, Train_Loss: 0.0033, Test_Loss: 0.0087, Train_Acc:0.9511, Test_Acc: 0.8706\n",
            "Epoch: 491, Train_Loss: 0.0036, Test_Loss: 0.0088, Train_Acc:0.9628, Test_Acc: 0.8816\n",
            "Epoch: 492, Train_Loss: 0.0036, Test_Loss: 0.0093, Train_Acc:0.9564, Test_Acc: 0.8750\n",
            "Epoch: 493, Train_Loss: 0.0028, Test_Loss: 0.0088, Train_Acc:0.9596, Test_Acc: 0.8772\n",
            "Epoch: 494, Train_Loss: 0.0032, Test_Loss: 0.0085, Train_Acc:0.9532, Test_Acc: 0.8728\n",
            "Epoch: 495, Train_Loss: 0.0033, Test_Loss: 0.0085, Train_Acc:0.9521, Test_Acc: 0.8772\n",
            "Epoch: 496, Train_Loss: 0.0026, Test_Loss: 0.0081, Train_Acc:0.9441, Test_Acc: 0.8816\n",
            "Epoch: 497, Train_Loss: 0.0031, Test_Loss: 0.0080, Train_Acc:0.9521, Test_Acc: 0.8838\n",
            "Epoch: 498, Train_Loss: 0.0033, Test_Loss: 0.0089, Train_Acc:0.9511, Test_Acc: 0.8575\n",
            "Epoch: 499, Train_Loss: 0.0027, Test_Loss: 0.0093, Train_Acc:0.9457, Test_Acc: 0.8640\n",
            "Epoch: 500, Train_Loss: 0.0029, Test_Loss: 0.0083, Train_Acc:0.9564, Test_Acc: 0.8969\n",
            "Epoch: 501, Train_Loss: 0.0031, Test_Loss: 0.0082, Train_Acc:0.9580, Test_Acc: 0.8947\n",
            "Epoch: 502, Train_Loss: 0.0030, Test_Loss: 0.0083, Train_Acc:0.9463, Test_Acc: 0.8794\n",
            "Epoch: 503, Train_Loss: 0.0027, Test_Loss: 0.0081, Train_Acc:0.9495, Test_Acc: 0.8882\n",
            "Epoch: 504, Train_Loss: 0.0028, Test_Loss: 0.0083, Train_Acc:0.9521, Test_Acc: 0.8816\n",
            "Epoch: 505, Train_Loss: 0.0033, Test_Loss: 0.0083, Train_Acc:0.9633, Test_Acc: 0.8882\n",
            "Epoch: 506, Train_Loss: 0.0031, Test_Loss: 0.0086, Train_Acc:0.9553, Test_Acc: 0.8750\n",
            "Epoch: 507, Train_Loss: 0.0033, Test_Loss: 0.0087, Train_Acc:0.9569, Test_Acc: 0.8816\n",
            "Epoch: 508, Train_Loss: 0.0032, Test_Loss: 0.0083, Train_Acc:0.9537, Test_Acc: 0.8969\n",
            "Epoch: 509, Train_Loss: 0.0032, Test_Loss: 0.0080, Train_Acc:0.9644, Test_Acc: 0.9035\n",
            "Epoch: 510, Train_Loss: 0.0030, Test_Loss: 0.0081, Train_Acc:0.9548, Test_Acc: 0.8904\n",
            "Epoch: 511, Train_Loss: 0.0031, Test_Loss: 0.0076, Train_Acc:0.9495, Test_Acc: 0.8904\n",
            "Epoch: 512, Train_Loss: 0.0036, Test_Loss: 0.0085, Train_Acc:0.9553, Test_Acc: 0.8816\n",
            "Epoch: 513, Train_Loss: 0.0031, Test_Loss: 0.0080, Train_Acc:0.9559, Test_Acc: 0.9013\n",
            "Epoch: 514, Train_Loss: 0.0031, Test_Loss: 0.0082, Train_Acc:0.9426, Test_Acc: 0.8925\n",
            "Epoch: 515, Train_Loss: 0.0031, Test_Loss: 0.0092, Train_Acc:0.9489, Test_Acc: 0.8728\n",
            "Epoch: 516, Train_Loss: 0.0034, Test_Loss: 0.0087, Train_Acc:0.9548, Test_Acc: 0.8991\n",
            "Epoch: 517, Train_Loss: 0.0028, Test_Loss: 0.0088, Train_Acc:0.9580, Test_Acc: 0.8882\n",
            "Epoch: 518, Train_Loss: 0.0030, Test_Loss: 0.0084, Train_Acc:0.9606, Test_Acc: 0.8925\n",
            "Epoch: 519, Train_Loss: 0.0030, Test_Loss: 0.0088, Train_Acc:0.9580, Test_Acc: 0.8838\n",
            "Epoch: 520, Train_Loss: 0.0030, Test_Loss: 0.0087, Train_Acc:0.9569, Test_Acc: 0.8794\n",
            "Epoch: 521, Train_Loss: 0.0033, Test_Loss: 0.0083, Train_Acc:0.9660, Test_Acc: 0.8925\n",
            "Epoch: 522, Train_Loss: 0.0025, Test_Loss: 0.0080, Train_Acc:0.9612, Test_Acc: 0.8925\n",
            "Epoch: 523, Train_Loss: 0.0030, Test_Loss: 0.0081, Train_Acc:0.9622, Test_Acc: 0.8904\n",
            "Epoch: 524, Train_Loss: 0.0030, Test_Loss: 0.0089, Train_Acc:0.9537, Test_Acc: 0.8794\n",
            "Epoch: 525, Train_Loss: 0.0031, Test_Loss: 0.0082, Train_Acc:0.9516, Test_Acc: 0.8947\n",
            "Epoch: 526, Train_Loss: 0.0028, Test_Loss: 0.0095, Train_Acc:0.9527, Test_Acc: 0.8816\n",
            "Epoch: 527, Train_Loss: 0.0031, Test_Loss: 0.0087, Train_Acc:0.9532, Test_Acc: 0.8794\n",
            "Epoch: 528, Train_Loss: 0.0028, Test_Loss: 0.0086, Train_Acc:0.9330, Test_Acc: 0.8816\n",
            "Epoch: 529, Train_Loss: 0.0029, Test_Loss: 0.0091, Train_Acc:0.9410, Test_Acc: 0.8706\n",
            "Epoch: 530, Train_Loss: 0.0032, Test_Loss: 0.0079, Train_Acc:0.9521, Test_Acc: 0.9057\n",
            "Epoch: 531, Train_Loss: 0.0029, Test_Loss: 0.0076, Train_Acc:0.9638, Test_Acc: 0.8947\n",
            "Epoch: 532, Train_Loss: 0.0031, Test_Loss: 0.0078, Train_Acc:0.9617, Test_Acc: 0.9101\n",
            "Epoch: 533, Train_Loss: 0.0028, Test_Loss: 0.0072, Train_Acc:0.9638, Test_Acc: 0.9035\n",
            "Epoch: 534, Train_Loss: 0.0032, Test_Loss: 0.0077, Train_Acc:0.9633, Test_Acc: 0.8969\n",
            "Epoch: 535, Train_Loss: 0.0027, Test_Loss: 0.0077, Train_Acc:0.9628, Test_Acc: 0.8772\n",
            "Epoch: 536, Train_Loss: 0.0027, Test_Loss: 0.0078, Train_Acc:0.9622, Test_Acc: 0.8838\n",
            "Epoch: 537, Train_Loss: 0.0029, Test_Loss: 0.0080, Train_Acc:0.9676, Test_Acc: 0.8969\n",
            "Epoch: 538, Train_Loss: 0.0029, Test_Loss: 0.0086, Train_Acc:0.9516, Test_Acc: 0.8925\n",
            "Epoch: 539, Train_Loss: 0.0030, Test_Loss: 0.0089, Train_Acc:0.9617, Test_Acc: 0.8838\n",
            "Epoch: 540, Train_Loss: 0.0024, Test_Loss: 0.0090, Train_Acc:0.9521, Test_Acc: 0.8662\n",
            "Epoch: 541, Train_Loss: 0.0025, Test_Loss: 0.0086, Train_Acc:0.9585, Test_Acc: 0.8706\n",
            "Epoch: 542, Train_Loss: 0.0027, Test_Loss: 0.0082, Train_Acc:0.9644, Test_Acc: 0.8904\n",
            "Epoch: 543, Train_Loss: 0.0030, Test_Loss: 0.0079, Train_Acc:0.9495, Test_Acc: 0.8838\n",
            "Epoch: 544, Train_Loss: 0.0029, Test_Loss: 0.0081, Train_Acc:0.9633, Test_Acc: 0.9013\n",
            "Epoch: 545, Train_Loss: 0.0024, Test_Loss: 0.0085, Train_Acc:0.9660, Test_Acc: 0.8882\n",
            "Epoch: 546, Train_Loss: 0.0027, Test_Loss: 0.0074, Train_Acc:0.9601, Test_Acc: 0.9013\n",
            "Epoch: 547, Train_Loss: 0.0026, Test_Loss: 0.0074, Train_Acc:0.9606, Test_Acc: 0.9057\n",
            "Epoch: 548, Train_Loss: 0.0025, Test_Loss: 0.0084, Train_Acc:0.9601, Test_Acc: 0.8925\n",
            "Epoch: 549, Train_Loss: 0.0025, Test_Loss: 0.0095, Train_Acc:0.9511, Test_Acc: 0.8640\n",
            "Epoch: 550, Train_Loss: 0.0025, Test_Loss: 0.0099, Train_Acc:0.9511, Test_Acc: 0.8706\n",
            "Epoch: 551, Train_Loss: 0.0029, Test_Loss: 0.0088, Train_Acc:0.9622, Test_Acc: 0.8882\n",
            "Epoch: 552, Train_Loss: 0.0028, Test_Loss: 0.0085, Train_Acc:0.9559, Test_Acc: 0.8816\n",
            "Epoch: 553, Train_Loss: 0.0029, Test_Loss: 0.0076, Train_Acc:0.9633, Test_Acc: 0.8925\n",
            "Epoch: 554, Train_Loss: 0.0034, Test_Loss: 0.0083, Train_Acc:0.9617, Test_Acc: 0.8991\n",
            "Epoch: 555, Train_Loss: 0.0035, Test_Loss: 0.0073, Train_Acc:0.9649, Test_Acc: 0.9035\n",
            "Epoch: 556, Train_Loss: 0.0031, Test_Loss: 0.0074, Train_Acc:0.9654, Test_Acc: 0.8991\n",
            "Epoch: 557, Train_Loss: 0.0026, Test_Loss: 0.0083, Train_Acc:0.9612, Test_Acc: 0.8991\n",
            "Epoch: 558, Train_Loss: 0.0029, Test_Loss: 0.0081, Train_Acc:0.9670, Test_Acc: 0.8925\n",
            "Epoch: 559, Train_Loss: 0.0028, Test_Loss: 0.0085, Train_Acc:0.9638, Test_Acc: 0.8816\n",
            "Epoch: 560, Train_Loss: 0.0029, Test_Loss: 0.0087, Train_Acc:0.9527, Test_Acc: 0.8750\n",
            "Epoch: 561, Train_Loss: 0.0030, Test_Loss: 0.0083, Train_Acc:0.9612, Test_Acc: 0.8860\n",
            "Epoch: 562, Train_Loss: 0.0030, Test_Loss: 0.0091, Train_Acc:0.9638, Test_Acc: 0.8794\n",
            "Epoch: 563, Train_Loss: 0.0029, Test_Loss: 0.0096, Train_Acc:0.9564, Test_Acc: 0.8377\n",
            "Epoch: 564, Train_Loss: 0.0027, Test_Loss: 0.0096, Train_Acc:0.9484, Test_Acc: 0.8465\n",
            "Epoch: 565, Train_Loss: 0.0030, Test_Loss: 0.0087, Train_Acc:0.9590, Test_Acc: 0.8860\n",
            "Epoch: 566, Train_Loss: 0.0032, Test_Loss: 0.0086, Train_Acc:0.9617, Test_Acc: 0.8882\n",
            "Epoch: 567, Train_Loss: 0.0027, Test_Loss: 0.0087, Train_Acc:0.9468, Test_Acc: 0.8969\n",
            "Epoch: 568, Train_Loss: 0.0033, Test_Loss: 0.0094, Train_Acc:0.9649, Test_Acc: 0.8772\n",
            "Epoch: 569, Train_Loss: 0.0029, Test_Loss: 0.0089, Train_Acc:0.9585, Test_Acc: 0.8925\n",
            "Epoch: 570, Train_Loss: 0.0032, Test_Loss: 0.0086, Train_Acc:0.9574, Test_Acc: 0.8816\n",
            "Epoch: 571, Train_Loss: 0.0032, Test_Loss: 0.0077, Train_Acc:0.9585, Test_Acc: 0.8991\n",
            "Epoch: 572, Train_Loss: 0.0028, Test_Loss: 0.0091, Train_Acc:0.9574, Test_Acc: 0.8838\n",
            "Epoch: 573, Train_Loss: 0.0030, Test_Loss: 0.0100, Train_Acc:0.9410, Test_Acc: 0.8684\n",
            "Epoch: 574, Train_Loss: 0.0044, Test_Loss: 0.0085, Train_Acc:0.9324, Test_Acc: 0.8662\n",
            "Epoch: 575, Train_Loss: 0.0052, Test_Loss: 0.0093, Train_Acc:0.9298, Test_Acc: 0.8838\n",
            "Epoch: 576, Train_Loss: 0.0048, Test_Loss: 0.0094, Train_Acc:0.9436, Test_Acc: 0.8860\n",
            "Epoch: 577, Train_Loss: 0.0059, Test_Loss: 0.0110, Train_Acc:0.9314, Test_Acc: 0.8553\n",
            "Epoch: 578, Train_Loss: 0.0054, Test_Loss: 0.0119, Train_Acc:0.9074, Test_Acc: 0.8136\n",
            "Epoch: 579, Train_Loss: 0.0053, Test_Loss: 0.0107, Train_Acc:0.9319, Test_Acc: 0.8640\n",
            "Epoch: 580, Train_Loss: 0.0054, Test_Loss: 0.0099, Train_Acc:0.9346, Test_Acc: 0.8772\n",
            "Epoch: 581, Train_Loss: 0.0057, Test_Loss: 0.0108, Train_Acc:0.9213, Test_Acc: 0.8553\n",
            "Epoch: 582, Train_Loss: 0.0053, Test_Loss: 0.0095, Train_Acc:0.9378, Test_Acc: 0.8838\n",
            "Epoch: 583, Train_Loss: 0.0052, Test_Loss: 0.0095, Train_Acc:0.9410, Test_Acc: 0.8860\n",
            "Epoch: 584, Train_Loss: 0.0051, Test_Loss: 0.0091, Train_Acc:0.9394, Test_Acc: 0.8925\n",
            "Epoch: 585, Train_Loss: 0.0049, Test_Loss: 0.0088, Train_Acc:0.9335, Test_Acc: 0.8816\n",
            "Epoch: 586, Train_Loss: 0.0048, Test_Loss: 0.0092, Train_Acc:0.9090, Test_Acc: 0.8399\n",
            "Epoch: 587, Train_Loss: 0.0044, Test_Loss: 0.0094, Train_Acc:0.9053, Test_Acc: 0.8487\n",
            "Epoch: 588, Train_Loss: 0.0047, Test_Loss: 0.0085, Train_Acc:0.9250, Test_Acc: 0.8750\n",
            "Epoch: 589, Train_Loss: 0.0049, Test_Loss: 0.0081, Train_Acc:0.9138, Test_Acc: 0.8618\n",
            "Epoch: 590, Train_Loss: 0.0045, Test_Loss: 0.0092, Train_Acc:0.9043, Test_Acc: 0.8377\n",
            "Epoch: 591, Train_Loss: 0.0045, Test_Loss: 0.0101, Train_Acc:0.9074, Test_Acc: 0.8180\n",
            "Epoch: 592, Train_Loss: 0.0045, Test_Loss: 0.0110, Train_Acc:0.8952, Test_Acc: 0.8114\n",
            "Epoch: 593, Train_Loss: 0.0041, Test_Loss: 0.0101, Train_Acc:0.9043, Test_Acc: 0.8333\n",
            "Epoch: 594, Train_Loss: 0.0044, Test_Loss: 0.0087, Train_Acc:0.9404, Test_Acc: 0.8882\n",
            "Epoch: 595, Train_Loss: 0.0044, Test_Loss: 0.0087, Train_Acc:0.9319, Test_Acc: 0.8838\n",
            "Epoch: 596, Train_Loss: 0.0040, Test_Loss: 0.0097, Train_Acc:0.9303, Test_Acc: 0.8750\n",
            "Epoch: 597, Train_Loss: 0.0045, Test_Loss: 0.0094, Train_Acc:0.9410, Test_Acc: 0.8684\n",
            "Epoch: 598, Train_Loss: 0.0045, Test_Loss: 0.0093, Train_Acc:0.9436, Test_Acc: 0.8750\n",
            "Epoch: 599, Train_Loss: 0.0043, Test_Loss: 0.0094, Train_Acc:0.9505, Test_Acc: 0.8794\n",
            "Epoch: 600, Train_Loss: 0.0038, Test_Loss: 0.0087, Train_Acc:0.9532, Test_Acc: 0.8860\n",
            "Epoch: 601, Train_Loss: 0.0041, Test_Loss: 0.0092, Train_Acc:0.9394, Test_Acc: 0.8684\n",
            "Epoch: 602, Train_Loss: 0.0043, Test_Loss: 0.0090, Train_Acc:0.9431, Test_Acc: 0.8706\n",
            "Epoch: 603, Train_Loss: 0.0040, Test_Loss: 0.0088, Train_Acc:0.9452, Test_Acc: 0.8772\n",
            "Epoch: 604, Train_Loss: 0.0042, Test_Loss: 0.0101, Train_Acc:0.9378, Test_Acc: 0.8509\n",
            "Epoch: 605, Train_Loss: 0.0042, Test_Loss: 0.0095, Train_Acc:0.9383, Test_Acc: 0.8531\n",
            "Epoch: 606, Train_Loss: 0.0032, Test_Loss: 0.0101, Train_Acc:0.9351, Test_Acc: 0.8575\n",
            "Epoch: 607, Train_Loss: 0.0034, Test_Loss: 0.0104, Train_Acc:0.9473, Test_Acc: 0.8596\n",
            "Epoch: 608, Train_Loss: 0.0038, Test_Loss: 0.0098, Train_Acc:0.9457, Test_Acc: 0.8706\n",
            "Epoch: 609, Train_Loss: 0.0033, Test_Loss: 0.0098, Train_Acc:0.9489, Test_Acc: 0.8640\n",
            "Epoch: 610, Train_Loss: 0.0030, Test_Loss: 0.0097, Train_Acc:0.9410, Test_Acc: 0.8553\n",
            "Epoch: 611, Train_Loss: 0.0039, Test_Loss: 0.0100, Train_Acc:0.9511, Test_Acc: 0.8684\n",
            "Epoch: 612, Train_Loss: 0.0033, Test_Loss: 0.0102, Train_Acc:0.9543, Test_Acc: 0.8816\n",
            "Epoch: 613, Train_Loss: 0.0038, Test_Loss: 0.0092, Train_Acc:0.9564, Test_Acc: 0.8794\n",
            "Epoch: 614, Train_Loss: 0.0036, Test_Loss: 0.0086, Train_Acc:0.9601, Test_Acc: 0.8925\n",
            "Epoch: 615, Train_Loss: 0.0032, Test_Loss: 0.0094, Train_Acc:0.9479, Test_Acc: 0.8706\n",
            "Epoch: 616, Train_Loss: 0.0035, Test_Loss: 0.0092, Train_Acc:0.9543, Test_Acc: 0.8772\n",
            "Epoch: 617, Train_Loss: 0.0030, Test_Loss: 0.0080, Train_Acc:0.9574, Test_Acc: 0.8882\n",
            "Epoch: 618, Train_Loss: 0.0034, Test_Loss: 0.0083, Train_Acc:0.9564, Test_Acc: 0.8860\n",
            "Epoch: 619, Train_Loss: 0.0033, Test_Loss: 0.0084, Train_Acc:0.9484, Test_Acc: 0.8816\n",
            "Epoch: 620, Train_Loss: 0.0032, Test_Loss: 0.0087, Train_Acc:0.9441, Test_Acc: 0.8706\n",
            "Epoch: 621, Train_Loss: 0.0032, Test_Loss: 0.0090, Train_Acc:0.9537, Test_Acc: 0.8684\n",
            "Epoch: 622, Train_Loss: 0.0029, Test_Loss: 0.0086, Train_Acc:0.9601, Test_Acc: 0.8772\n",
            "Epoch: 623, Train_Loss: 0.0029, Test_Loss: 0.0085, Train_Acc:0.9537, Test_Acc: 0.8728\n",
            "Epoch: 624, Train_Loss: 0.0034, Test_Loss: 0.0087, Train_Acc:0.9612, Test_Acc: 0.8750\n",
            "Epoch: 625, Train_Loss: 0.0028, Test_Loss: 0.0082, Train_Acc:0.9543, Test_Acc: 0.8838\n",
            "Epoch: 626, Train_Loss: 0.0030, Test_Loss: 0.0081, Train_Acc:0.9585, Test_Acc: 0.8816\n",
            "Epoch: 627, Train_Loss: 0.0027, Test_Loss: 0.0083, Train_Acc:0.9606, Test_Acc: 0.8816\n",
            "Epoch: 628, Train_Loss: 0.0030, Test_Loss: 0.0092, Train_Acc:0.9596, Test_Acc: 0.8640\n",
            "Epoch: 629, Train_Loss: 0.0029, Test_Loss: 0.0095, Train_Acc:0.9500, Test_Acc: 0.8575\n",
            "Epoch: 630, Train_Loss: 0.0028, Test_Loss: 0.0089, Train_Acc:0.9580, Test_Acc: 0.8750\n",
            "Epoch: 631, Train_Loss: 0.0026, Test_Loss: 0.0087, Train_Acc:0.9559, Test_Acc: 0.8772\n",
            "Epoch: 632, Train_Loss: 0.0025, Test_Loss: 0.0090, Train_Acc:0.9484, Test_Acc: 0.8596\n",
            "Epoch: 633, Train_Loss: 0.0025, Test_Loss: 0.0097, Train_Acc:0.9463, Test_Acc: 0.8553\n",
            "Epoch: 634, Train_Loss: 0.0029, Test_Loss: 0.0099, Train_Acc:0.9266, Test_Acc: 0.8311\n",
            "Epoch: 635, Train_Loss: 0.0028, Test_Loss: 0.0094, Train_Acc:0.9447, Test_Acc: 0.8618\n",
            "Epoch: 636, Train_Loss: 0.0024, Test_Loss: 0.0083, Train_Acc:0.9447, Test_Acc: 0.8794\n",
            "Epoch: 637, Train_Loss: 0.0026, Test_Loss: 0.0080, Train_Acc:0.9527, Test_Acc: 0.8794\n",
            "Epoch: 638, Train_Loss: 0.0025, Test_Loss: 0.0078, Train_Acc:0.9654, Test_Acc: 0.8925\n",
            "Epoch: 639, Train_Loss: 0.0025, Test_Loss: 0.0079, Train_Acc:0.9633, Test_Acc: 0.8904\n",
            "Epoch: 640, Train_Loss: 0.0024, Test_Loss: 0.0080, Train_Acc:0.9548, Test_Acc: 0.8904\n",
            "Epoch: 641, Train_Loss: 0.0029, Test_Loss: 0.0083, Train_Acc:0.9654, Test_Acc: 0.8838\n",
            "Epoch: 642, Train_Loss: 0.0026, Test_Loss: 0.0083, Train_Acc:0.9596, Test_Acc: 0.8882\n",
            "Epoch: 643, Train_Loss: 0.0030, Test_Loss: 0.0083, Train_Acc:0.9548, Test_Acc: 0.8860\n",
            "Epoch: 644, Train_Loss: 0.0027, Test_Loss: 0.0080, Train_Acc:0.9601, Test_Acc: 0.8882\n",
            "Epoch: 645, Train_Loss: 0.0031, Test_Loss: 0.0088, Train_Acc:0.9420, Test_Acc: 0.8575\n",
            "Epoch: 646, Train_Loss: 0.0032, Test_Loss: 0.0081, Train_Acc:0.9628, Test_Acc: 0.8860\n",
            "Epoch: 647, Train_Loss: 0.0030, Test_Loss: 0.0076, Train_Acc:0.9676, Test_Acc: 0.8991\n",
            "Epoch: 648, Train_Loss: 0.0026, Test_Loss: 0.0077, Train_Acc:0.9622, Test_Acc: 0.9123\n",
            "Epoch: 649, Train_Loss: 0.0026, Test_Loss: 0.0076, Train_Acc:0.9543, Test_Acc: 0.9035\n",
            "Epoch: 650, Train_Loss: 0.0029, Test_Loss: 0.0076, Train_Acc:0.9665, Test_Acc: 0.9123\n",
            "Epoch: 651, Train_Loss: 0.0026, Test_Loss: 0.0076, Train_Acc:0.9644, Test_Acc: 0.9079\n",
            "Epoch: 652, Train_Loss: 0.0025, Test_Loss: 0.0076, Train_Acc:0.9638, Test_Acc: 0.8947\n",
            "Epoch: 653, Train_Loss: 0.0028, Test_Loss: 0.0082, Train_Acc:0.9638, Test_Acc: 0.8925\n",
            "Epoch: 654, Train_Loss: 0.0026, Test_Loss: 0.0084, Train_Acc:0.9553, Test_Acc: 0.8816\n",
            "Epoch: 655, Train_Loss: 0.0025, Test_Loss: 0.0078, Train_Acc:0.9585, Test_Acc: 0.8750\n",
            "Epoch: 656, Train_Loss: 0.0028, Test_Loss: 0.0082, Train_Acc:0.9543, Test_Acc: 0.8728\n",
            "Epoch: 657, Train_Loss: 0.0026, Test_Loss: 0.0086, Train_Acc:0.9676, Test_Acc: 0.8794\n",
            "Epoch: 658, Train_Loss: 0.0028, Test_Loss: 0.0085, Train_Acc:0.9521, Test_Acc: 0.8816\n",
            "Epoch: 659, Train_Loss: 0.0027, Test_Loss: 0.0081, Train_Acc:0.9654, Test_Acc: 0.8925\n",
            "Epoch: 660, Train_Loss: 0.0026, Test_Loss: 0.0083, Train_Acc:0.9601, Test_Acc: 0.8969\n",
            "Epoch: 661, Train_Loss: 0.0027, Test_Loss: 0.0080, Train_Acc:0.9617, Test_Acc: 0.8904\n",
            "Epoch: 662, Train_Loss: 0.0024, Test_Loss: 0.0074, Train_Acc:0.9574, Test_Acc: 0.9035\n",
            "Epoch: 663, Train_Loss: 0.0027, Test_Loss: 0.0079, Train_Acc:0.9601, Test_Acc: 0.8904\n",
            "Epoch: 664, Train_Loss: 0.0026, Test_Loss: 0.0080, Train_Acc:0.9676, Test_Acc: 0.8860\n",
            "Epoch: 665, Train_Loss: 0.0026, Test_Loss: 0.0078, Train_Acc:0.9665, Test_Acc: 0.8882\n",
            "Epoch: 666, Train_Loss: 0.0023, Test_Loss: 0.0078, Train_Acc:0.9537, Test_Acc: 0.8904\n",
            "Epoch: 667, Train_Loss: 0.0028, Test_Loss: 0.0075, Train_Acc:0.9612, Test_Acc: 0.8969\n",
            "Epoch: 668, Train_Loss: 0.0023, Test_Loss: 0.0073, Train_Acc:0.9660, Test_Acc: 0.8882\n",
            "Epoch: 669, Train_Loss: 0.0023, Test_Loss: 0.0074, Train_Acc:0.9628, Test_Acc: 0.8882\n",
            "Epoch: 670, Train_Loss: 0.0021, Test_Loss: 0.0078, Train_Acc:0.9628, Test_Acc: 0.8750\n",
            "Epoch: 671, Train_Loss: 0.0023, Test_Loss: 0.0076, Train_Acc:0.9527, Test_Acc: 0.8750\n",
            "Epoch: 672, Train_Loss: 0.0024, Test_Loss: 0.0078, Train_Acc:0.9601, Test_Acc: 0.8640\n",
            "Epoch: 673, Train_Loss: 0.0024, Test_Loss: 0.0083, Train_Acc:0.9553, Test_Acc: 0.8640\n",
            "Epoch: 674, Train_Loss: 0.0023, Test_Loss: 0.0085, Train_Acc:0.9521, Test_Acc: 0.8575\n",
            "Epoch: 675, Train_Loss: 0.0022, Test_Loss: 0.0074, Train_Acc:0.9559, Test_Acc: 0.8904\n",
            "Epoch: 676, Train_Loss: 0.0025, Test_Loss: 0.0085, Train_Acc:0.9606, Test_Acc: 0.8772\n",
            "Epoch: 677, Train_Loss: 0.0026, Test_Loss: 0.0077, Train_Acc:0.9495, Test_Acc: 0.9013\n",
            "Epoch: 678, Train_Loss: 0.0025, Test_Loss: 0.0072, Train_Acc:0.9617, Test_Acc: 0.9035\n",
            "Epoch: 679, Train_Loss: 0.0023, Test_Loss: 0.0075, Train_Acc:0.9601, Test_Acc: 0.8991\n",
            "Epoch: 680, Train_Loss: 0.0028, Test_Loss: 0.0073, Train_Acc:0.9622, Test_Acc: 0.9057\n",
            "Epoch: 681, Train_Loss: 0.0027, Test_Loss: 0.0074, Train_Acc:0.9622, Test_Acc: 0.9079\n",
            "Epoch: 682, Train_Loss: 0.0023, Test_Loss: 0.0081, Train_Acc:0.9601, Test_Acc: 0.8991\n",
            "Epoch: 683, Train_Loss: 0.0026, Test_Loss: 0.0084, Train_Acc:0.9553, Test_Acc: 0.8816\n",
            "Epoch: 684, Train_Loss: 0.0025, Test_Loss: 0.0080, Train_Acc:0.9585, Test_Acc: 0.9035\n",
            "Epoch: 685, Train_Loss: 0.0025, Test_Loss: 0.0081, Train_Acc:0.9553, Test_Acc: 0.8860\n",
            "Epoch: 686, Train_Loss: 0.0026, Test_Loss: 0.0083, Train_Acc:0.9505, Test_Acc: 0.8772\n",
            "Epoch: 687, Train_Loss: 0.0028, Test_Loss: 0.0080, Train_Acc:0.9489, Test_Acc: 0.8794\n",
            "Epoch: 688, Train_Loss: 0.0026, Test_Loss: 0.0079, Train_Acc:0.9436, Test_Acc: 0.8860\n",
            "Epoch: 689, Train_Loss: 0.0024, Test_Loss: 0.0077, Train_Acc:0.9463, Test_Acc: 0.9013\n",
            "Epoch: 690, Train_Loss: 0.0031, Test_Loss: 0.0078, Train_Acc:0.9612, Test_Acc: 0.8991\n",
            "Epoch: 691, Train_Loss: 0.0029, Test_Loss: 0.0083, Train_Acc:0.9505, Test_Acc: 0.8838\n",
            "Epoch: 692, Train_Loss: 0.0036, Test_Loss: 0.0079, Train_Acc:0.9543, Test_Acc: 0.8925\n",
            "Epoch: 693, Train_Loss: 0.0027, Test_Loss: 0.0086, Train_Acc:0.9532, Test_Acc: 0.8925\n",
            "Epoch: 694, Train_Loss: 0.0029, Test_Loss: 0.0085, Train_Acc:0.9622, Test_Acc: 0.8969\n",
            "Epoch: 695, Train_Loss: 0.0029, Test_Loss: 0.0081, Train_Acc:0.9532, Test_Acc: 0.8947\n",
            "Epoch: 696, Train_Loss: 0.0029, Test_Loss: 0.0085, Train_Acc:0.9564, Test_Acc: 0.8925\n",
            "Epoch: 697, Train_Loss: 0.0030, Test_Loss: 0.0077, Train_Acc:0.9633, Test_Acc: 0.8882\n",
            "Epoch: 698, Train_Loss: 0.0025, Test_Loss: 0.0074, Train_Acc:0.9628, Test_Acc: 0.8882\n",
            "Epoch: 699, Train_Loss: 0.0027, Test_Loss: 0.0080, Train_Acc:0.9505, Test_Acc: 0.8706\n",
            "Epoch: 700, Train_Loss: 0.0028, Test_Loss: 0.0083, Train_Acc:0.9590, Test_Acc: 0.8772\n",
            "Epoch: 701, Train_Loss: 0.0028, Test_Loss: 0.0087, Train_Acc:0.9585, Test_Acc: 0.8750\n",
            "Epoch: 702, Train_Loss: 0.0029, Test_Loss: 0.0095, Train_Acc:0.9617, Test_Acc: 0.8706\n",
            "Epoch: 703, Train_Loss: 0.0027, Test_Loss: 0.0089, Train_Acc:0.9580, Test_Acc: 0.8794\n",
            "Epoch: 704, Train_Loss: 0.0027, Test_Loss: 0.0083, Train_Acc:0.9633, Test_Acc: 0.8860\n",
            "Epoch: 705, Train_Loss: 0.0026, Test_Loss: 0.0084, Train_Acc:0.9617, Test_Acc: 0.8728\n",
            "Epoch: 706, Train_Loss: 0.0025, Test_Loss: 0.0081, Train_Acc:0.9654, Test_Acc: 0.8969\n",
            "Epoch: 707, Train_Loss: 0.0029, Test_Loss: 0.0081, Train_Acc:0.9644, Test_Acc: 0.8991\n",
            "Epoch: 708, Train_Loss: 0.0028, Test_Loss: 0.0079, Train_Acc:0.9431, Test_Acc: 0.8838\n",
            "Epoch: 709, Train_Loss: 0.0028, Test_Loss: 0.0085, Train_Acc:0.9543, Test_Acc: 0.8838\n",
            "Epoch: 710, Train_Loss: 0.0029, Test_Loss: 0.0087, Train_Acc:0.9362, Test_Acc: 0.8772\n",
            "Epoch: 711, Train_Loss: 0.0028, Test_Loss: 0.0082, Train_Acc:0.9426, Test_Acc: 0.8772\n",
            "Epoch: 712, Train_Loss: 0.0026, Test_Loss: 0.0081, Train_Acc:0.9644, Test_Acc: 0.8904\n",
            "Epoch: 713, Train_Loss: 0.0027, Test_Loss: 0.0086, Train_Acc:0.9580, Test_Acc: 0.8816\n",
            "Epoch: 714, Train_Loss: 0.0023, Test_Loss: 0.0080, Train_Acc:0.9346, Test_Acc: 0.8553\n",
            "Epoch: 715, Train_Loss: 0.0027, Test_Loss: 0.0088, Train_Acc:0.9298, Test_Acc: 0.8772\n",
            "Epoch: 716, Train_Loss: 0.0027, Test_Loss: 0.0097, Train_Acc:0.9441, Test_Acc: 0.8706\n",
            "Epoch: 717, Train_Loss: 0.0027, Test_Loss: 0.0094, Train_Acc:0.9441, Test_Acc: 0.8706\n",
            "Epoch: 718, Train_Loss: 0.0028, Test_Loss: 0.0088, Train_Acc:0.9293, Test_Acc: 0.8509\n",
            "Epoch: 719, Train_Loss: 0.0027, Test_Loss: 0.0092, Train_Acc:0.9431, Test_Acc: 0.8575\n",
            "Epoch: 720, Train_Loss: 0.0025, Test_Loss: 0.0096, Train_Acc:0.9537, Test_Acc: 0.8509\n",
            "Epoch: 721, Train_Loss: 0.0029, Test_Loss: 0.0095, Train_Acc:0.9574, Test_Acc: 0.8509\n",
            "Epoch: 722, Train_Loss: 0.0026, Test_Loss: 0.0102, Train_Acc:0.9532, Test_Acc: 0.8640\n",
            "Epoch: 723, Train_Loss: 0.0025, Test_Loss: 0.0099, Train_Acc:0.9484, Test_Acc: 0.8640\n",
            "Epoch: 724, Train_Loss: 0.0032, Test_Loss: 0.0089, Train_Acc:0.9601, Test_Acc: 0.8882\n",
            "Epoch: 725, Train_Loss: 0.0026, Test_Loss: 0.0088, Train_Acc:0.9564, Test_Acc: 0.8794\n",
            "Epoch: 726, Train_Loss: 0.0028, Test_Loss: 0.0087, Train_Acc:0.9628, Test_Acc: 0.8882\n",
            "Epoch: 727, Train_Loss: 0.0029, Test_Loss: 0.0090, Train_Acc:0.9628, Test_Acc: 0.8772\n",
            "Epoch: 728, Train_Loss: 0.0024, Test_Loss: 0.0108, Train_Acc:0.9404, Test_Acc: 0.8487\n",
            "Epoch: 729, Train_Loss: 0.0027, Test_Loss: 0.0094, Train_Acc:0.9569, Test_Acc: 0.8706\n",
            "Epoch: 730, Train_Loss: 0.0028, Test_Loss: 0.0091, Train_Acc:0.9495, Test_Acc: 0.8772\n",
            "Epoch: 731, Train_Loss: 0.0027, Test_Loss: 0.0084, Train_Acc:0.9601, Test_Acc: 0.8838\n",
            "Epoch: 732, Train_Loss: 0.0025, Test_Loss: 0.0086, Train_Acc:0.9516, Test_Acc: 0.8838\n",
            "Epoch: 733, Train_Loss: 0.0027, Test_Loss: 0.0091, Train_Acc:0.9452, Test_Acc: 0.8838\n",
            "Epoch: 734, Train_Loss: 0.0031, Test_Loss: 0.0077, Train_Acc:0.9596, Test_Acc: 0.8991\n",
            "Epoch: 735, Train_Loss: 0.0027, Test_Loss: 0.0088, Train_Acc:0.9351, Test_Acc: 0.8750\n",
            "Epoch: 736, Train_Loss: 0.0031, Test_Loss: 0.0093, Train_Acc:0.9511, Test_Acc: 0.8684\n",
            "Epoch: 737, Train_Loss: 0.0029, Test_Loss: 0.0097, Train_Acc:0.9500, Test_Acc: 0.8706\n",
            "Epoch: 738, Train_Loss: 0.0025, Test_Loss: 0.0078, Train_Acc:0.9484, Test_Acc: 0.8816\n",
            "Epoch: 739, Train_Loss: 0.0030, Test_Loss: 0.0086, Train_Acc:0.9601, Test_Acc: 0.8925\n",
            "Epoch: 740, Train_Loss: 0.0026, Test_Loss: 0.0082, Train_Acc:0.9580, Test_Acc: 0.8904\n",
            "Epoch: 741, Train_Loss: 0.0034, Test_Loss: 0.0079, Train_Acc:0.9532, Test_Acc: 0.8816\n",
            "Epoch: 742, Train_Loss: 0.0032, Test_Loss: 0.0087, Train_Acc:0.9596, Test_Acc: 0.8772\n",
            "Epoch: 743, Train_Loss: 0.0025, Test_Loss: 0.0089, Train_Acc:0.9601, Test_Acc: 0.8991\n",
            "Epoch: 744, Train_Loss: 0.0028, Test_Loss: 0.0091, Train_Acc:0.9590, Test_Acc: 0.8838\n",
            "Epoch: 745, Train_Loss: 0.0027, Test_Loss: 0.0097, Train_Acc:0.9479, Test_Acc: 0.8640\n",
            "Epoch: 746, Train_Loss: 0.0027, Test_Loss: 0.0083, Train_Acc:0.9660, Test_Acc: 0.8816\n",
            "Epoch: 747, Train_Loss: 0.0026, Test_Loss: 0.0082, Train_Acc:0.9612, Test_Acc: 0.8904\n",
            "Epoch: 748, Train_Loss: 0.0025, Test_Loss: 0.0089, Train_Acc:0.9516, Test_Acc: 0.8706\n",
            "Epoch: 749, Train_Loss: 0.0028, Test_Loss: 0.0091, Train_Acc:0.9527, Test_Acc: 0.8728\n",
            "Epoch: 750, Train_Loss: 0.0023, Test_Loss: 0.0096, Train_Acc:0.9532, Test_Acc: 0.8662\n",
            "Epoch: 751, Train_Loss: 0.0029, Test_Loss: 0.0086, Train_Acc:0.9596, Test_Acc: 0.8860\n",
            "Epoch: 752, Train_Loss: 0.0028, Test_Loss: 0.0083, Train_Acc:0.9644, Test_Acc: 0.8860\n",
            "Epoch: 753, Train_Loss: 0.0028, Test_Loss: 0.0079, Train_Acc:0.9676, Test_Acc: 0.8860\n",
            "Epoch: 754, Train_Loss: 0.0026, Test_Loss: 0.0084, Train_Acc:0.9601, Test_Acc: 0.8860\n",
            "Epoch: 755, Train_Loss: 0.0027, Test_Loss: 0.0081, Train_Acc:0.9622, Test_Acc: 0.8816\n",
            "Epoch: 756, Train_Loss: 0.0027, Test_Loss: 0.0078, Train_Acc:0.9596, Test_Acc: 0.8838\n",
            "Epoch: 757, Train_Loss: 0.0027, Test_Loss: 0.0078, Train_Acc:0.9612, Test_Acc: 0.9057\n",
            "Epoch: 758, Train_Loss: 0.0030, Test_Loss: 0.0079, Train_Acc:0.9670, Test_Acc: 0.8860\n",
            "Epoch: 759, Train_Loss: 0.0029, Test_Loss: 0.0078, Train_Acc:0.9628, Test_Acc: 0.8882\n",
            "Epoch: 760, Train_Loss: 0.0028, Test_Loss: 0.0080, Train_Acc:0.9553, Test_Acc: 0.8969\n",
            "Epoch: 761, Train_Loss: 0.0026, Test_Loss: 0.0084, Train_Acc:0.9479, Test_Acc: 0.8794\n",
            "Epoch: 762, Train_Loss: 0.0026, Test_Loss: 0.0086, Train_Acc:0.9441, Test_Acc: 0.8750\n",
            "Epoch: 763, Train_Loss: 0.0028, Test_Loss: 0.0079, Train_Acc:0.9596, Test_Acc: 0.8816\n",
            "Epoch: 764, Train_Loss: 0.0027, Test_Loss: 0.0075, Train_Acc:0.9617, Test_Acc: 0.9057\n",
            "Epoch: 765, Train_Loss: 0.0030, Test_Loss: 0.0078, Train_Acc:0.9426, Test_Acc: 0.8794\n",
            "Epoch: 766, Train_Loss: 0.0030, Test_Loss: 0.0082, Train_Acc:0.9426, Test_Acc: 0.8860\n",
            "Epoch: 767, Train_Loss: 0.0029, Test_Loss: 0.0085, Train_Acc:0.9606, Test_Acc: 0.8794\n",
            "Epoch: 768, Train_Loss: 0.0028, Test_Loss: 0.0087, Train_Acc:0.9484, Test_Acc: 0.8596\n",
            "Epoch: 769, Train_Loss: 0.0029, Test_Loss: 0.0089, Train_Acc:0.9239, Test_Acc: 0.8421\n",
            "Epoch: 770, Train_Loss: 0.0031, Test_Loss: 0.0080, Train_Acc:0.9564, Test_Acc: 0.8860\n",
            "Epoch: 771, Train_Loss: 0.0026, Test_Loss: 0.0080, Train_Acc:0.9590, Test_Acc: 0.8860\n",
            "Epoch: 772, Train_Loss: 0.0027, Test_Loss: 0.0079, Train_Acc:0.9654, Test_Acc: 0.8947\n",
            "Epoch: 773, Train_Loss: 0.0023, Test_Loss: 0.0084, Train_Acc:0.9574, Test_Acc: 0.8706\n",
            "Epoch: 774, Train_Loss: 0.0030, Test_Loss: 0.0083, Train_Acc:0.9548, Test_Acc: 0.8904\n",
            "Epoch: 775, Train_Loss: 0.0026, Test_Loss: 0.0088, Train_Acc:0.9569, Test_Acc: 0.8838\n",
            "Epoch: 776, Train_Loss: 0.0027, Test_Loss: 0.0089, Train_Acc:0.9665, Test_Acc: 0.8750\n",
            "Epoch: 777, Train_Loss: 0.0026, Test_Loss: 0.0084, Train_Acc:0.9574, Test_Acc: 0.8640\n",
            "Epoch: 778, Train_Loss: 0.0027, Test_Loss: 0.0090, Train_Acc:0.9548, Test_Acc: 0.8640\n",
            "Epoch: 779, Train_Loss: 0.0027, Test_Loss: 0.0088, Train_Acc:0.9516, Test_Acc: 0.8640\n",
            "Epoch: 780, Train_Loss: 0.0026, Test_Loss: 0.0076, Train_Acc:0.9644, Test_Acc: 0.9013\n",
            "Epoch: 781, Train_Loss: 0.0028, Test_Loss: 0.0079, Train_Acc:0.9559, Test_Acc: 0.8882\n",
            "Epoch: 782, Train_Loss: 0.0027, Test_Loss: 0.0094, Train_Acc:0.9479, Test_Acc: 0.8728\n",
            "Epoch: 783, Train_Loss: 0.0027, Test_Loss: 0.0093, Train_Acc:0.9564, Test_Acc: 0.8618\n",
            "Epoch: 784, Train_Loss: 0.0027, Test_Loss: 0.0083, Train_Acc:0.9527, Test_Acc: 0.8640\n",
            "Epoch: 785, Train_Loss: 0.0030, Test_Loss: 0.0080, Train_Acc:0.9697, Test_Acc: 0.8860\n",
            "Epoch: 786, Train_Loss: 0.0027, Test_Loss: 0.0085, Train_Acc:0.9548, Test_Acc: 0.8706\n",
            "Epoch: 787, Train_Loss: 0.0032, Test_Loss: 0.0076, Train_Acc:0.9601, Test_Acc: 0.9035\n",
            "Epoch: 788, Train_Loss: 0.0028, Test_Loss: 0.0080, Train_Acc:0.9495, Test_Acc: 0.8925\n",
            "Epoch: 789, Train_Loss: 0.0029, Test_Loss: 0.0091, Train_Acc:0.9335, Test_Acc: 0.8816\n",
            "Epoch: 790, Train_Loss: 0.0031, Test_Loss: 0.0082, Train_Acc:0.9543, Test_Acc: 0.8838\n",
            "Epoch: 791, Train_Loss: 0.0026, Test_Loss: 0.0073, Train_Acc:0.9468, Test_Acc: 0.8838\n",
            "Epoch: 792, Train_Loss: 0.0028, Test_Loss: 0.0069, Train_Acc:0.9617, Test_Acc: 0.8904\n",
            "Epoch: 793, Train_Loss: 0.0025, Test_Loss: 0.0075, Train_Acc:0.9622, Test_Acc: 0.8860\n",
            "Epoch: 794, Train_Loss: 0.0030, Test_Loss: 0.0074, Train_Acc:0.9543, Test_Acc: 0.8904\n",
            "Epoch: 795, Train_Loss: 0.0025, Test_Loss: 0.0080, Train_Acc:0.9335, Test_Acc: 0.8640\n",
            "Epoch: 796, Train_Loss: 0.0029, Test_Loss: 0.0085, Train_Acc:0.9596, Test_Acc: 0.8728\n",
            "Epoch: 797, Train_Loss: 0.0035, Test_Loss: 0.0085, Train_Acc:0.9473, Test_Acc: 0.8816\n",
            "Epoch: 798, Train_Loss: 0.0027, Test_Loss: 0.0078, Train_Acc:0.9564, Test_Acc: 0.8882\n",
            "Epoch: 799, Train_Loss: 0.0030, Test_Loss: 0.0091, Train_Acc:0.9463, Test_Acc: 0.8772\n",
            "Epoch: 800, Train_Loss: 0.0028, Test_Loss: 0.0088, Train_Acc:0.9559, Test_Acc: 0.8706\n",
            "Epoch: 801, Train_Loss: 0.0032, Test_Loss: 0.0080, Train_Acc:0.9548, Test_Acc: 0.8838\n",
            "Epoch: 802, Train_Loss: 0.0027, Test_Loss: 0.0084, Train_Acc:0.9516, Test_Acc: 0.8882\n",
            "Epoch: 803, Train_Loss: 0.0031, Test_Loss: 0.0083, Train_Acc:0.9569, Test_Acc: 0.8860\n",
            "Epoch: 804, Train_Loss: 0.0029, Test_Loss: 0.0084, Train_Acc:0.9527, Test_Acc: 0.8860\n",
            "Epoch: 805, Train_Loss: 0.0031, Test_Loss: 0.0079, Train_Acc:0.9489, Test_Acc: 0.8750\n",
            "Epoch: 806, Train_Loss: 0.0028, Test_Loss: 0.0082, Train_Acc:0.9495, Test_Acc: 0.8750\n",
            "Epoch: 807, Train_Loss: 0.0025, Test_Loss: 0.0090, Train_Acc:0.9473, Test_Acc: 0.8662\n",
            "Epoch: 808, Train_Loss: 0.0028, Test_Loss: 0.0084, Train_Acc:0.9590, Test_Acc: 0.8860\n",
            "Epoch: 809, Train_Loss: 0.0024, Test_Loss: 0.0077, Train_Acc:0.9532, Test_Acc: 0.8860\n",
            "Epoch: 810, Train_Loss: 0.0029, Test_Loss: 0.0076, Train_Acc:0.9447, Test_Acc: 0.8947\n",
            "Epoch: 811, Train_Loss: 0.0027, Test_Loss: 0.0078, Train_Acc:0.9473, Test_Acc: 0.8947\n",
            "Epoch: 812, Train_Loss: 0.0026, Test_Loss: 0.0081, Train_Acc:0.9548, Test_Acc: 0.8860\n",
            "Epoch: 813, Train_Loss: 0.0025, Test_Loss: 0.0081, Train_Acc:0.9484, Test_Acc: 0.8838\n",
            "Epoch: 814, Train_Loss: 0.0024, Test_Loss: 0.0079, Train_Acc:0.9410, Test_Acc: 0.8728\n",
            "Epoch: 815, Train_Loss: 0.0026, Test_Loss: 0.0075, Train_Acc:0.9590, Test_Acc: 0.8925\n",
            "Epoch: 816, Train_Loss: 0.0024, Test_Loss: 0.0081, Train_Acc:0.9649, Test_Acc: 0.9035\n",
            "Epoch: 817, Train_Loss: 0.0027, Test_Loss: 0.0080, Train_Acc:0.9617, Test_Acc: 0.8969\n",
            "Epoch: 818, Train_Loss: 0.0026, Test_Loss: 0.0080, Train_Acc:0.9628, Test_Acc: 0.8991\n",
            "Epoch: 819, Train_Loss: 0.0025, Test_Loss: 0.0078, Train_Acc:0.9585, Test_Acc: 0.9057\n",
            "Epoch: 820, Train_Loss: 0.0025, Test_Loss: 0.0077, Train_Acc:0.9543, Test_Acc: 0.9013\n",
            "Epoch: 821, Train_Loss: 0.0027, Test_Loss: 0.0080, Train_Acc:0.9394, Test_Acc: 0.8838\n",
            "Epoch: 822, Train_Loss: 0.0030, Test_Loss: 0.0084, Train_Acc:0.9463, Test_Acc: 0.8969\n",
            "Epoch: 823, Train_Loss: 0.0026, Test_Loss: 0.0086, Train_Acc:0.9420, Test_Acc: 0.8728\n",
            "Epoch: 824, Train_Loss: 0.0027, Test_Loss: 0.0081, Train_Acc:0.9468, Test_Acc: 0.8882\n",
            "Epoch: 825, Train_Loss: 0.0027, Test_Loss: 0.0080, Train_Acc:0.9415, Test_Acc: 0.8684\n",
            "Epoch: 826, Train_Loss: 0.0024, Test_Loss: 0.0082, Train_Acc:0.9521, Test_Acc: 0.8838\n",
            "Epoch: 827, Train_Loss: 0.0027, Test_Loss: 0.0093, Train_Acc:0.9505, Test_Acc: 0.8728\n",
            "Epoch: 828, Train_Loss: 0.0023, Test_Loss: 0.0094, Train_Acc:0.9372, Test_Acc: 0.8684\n",
            "Epoch: 829, Train_Loss: 0.0023, Test_Loss: 0.0085, Train_Acc:0.9559, Test_Acc: 0.8860\n",
            "Epoch: 830, Train_Loss: 0.0028, Test_Loss: 0.0086, Train_Acc:0.9612, Test_Acc: 0.8882\n",
            "Epoch: 831, Train_Loss: 0.0024, Test_Loss: 0.0087, Train_Acc:0.9537, Test_Acc: 0.8794\n",
            "Epoch: 832, Train_Loss: 0.0024, Test_Loss: 0.0092, Train_Acc:0.9527, Test_Acc: 0.8772\n",
            "Epoch: 833, Train_Loss: 0.0024, Test_Loss: 0.0092, Train_Acc:0.9532, Test_Acc: 0.8860\n",
            "Epoch: 834, Train_Loss: 0.0025, Test_Loss: 0.0089, Train_Acc:0.9553, Test_Acc: 0.8991\n",
            "Epoch: 835, Train_Loss: 0.0027, Test_Loss: 0.0086, Train_Acc:0.9633, Test_Acc: 0.9057\n",
            "Epoch: 836, Train_Loss: 0.0024, Test_Loss: 0.0086, Train_Acc:0.9649, Test_Acc: 0.8969\n",
            "Epoch: 837, Train_Loss: 0.0025, Test_Loss: 0.0081, Train_Acc:0.9606, Test_Acc: 0.8882\n",
            "Epoch: 838, Train_Loss: 0.0024, Test_Loss: 0.0081, Train_Acc:0.9596, Test_Acc: 0.8860\n",
            "Epoch: 839, Train_Loss: 0.0027, Test_Loss: 0.0079, Train_Acc:0.9665, Test_Acc: 0.8860\n",
            "Epoch: 840, Train_Loss: 0.0021, Test_Loss: 0.0081, Train_Acc:0.9638, Test_Acc: 0.8838\n",
            "Epoch: 841, Train_Loss: 0.0025, Test_Loss: 0.0087, Train_Acc:0.9527, Test_Acc: 0.8509\n",
            "Epoch: 842, Train_Loss: 0.0025, Test_Loss: 0.0089, Train_Acc:0.9622, Test_Acc: 0.8618\n",
            "Epoch: 843, Train_Loss: 0.0025, Test_Loss: 0.0083, Train_Acc:0.9532, Test_Acc: 0.8882\n",
            "Epoch: 844, Train_Loss: 0.0026, Test_Loss: 0.0082, Train_Acc:0.9574, Test_Acc: 0.8904\n",
            "Epoch: 845, Train_Loss: 0.0025, Test_Loss: 0.0085, Train_Acc:0.9564, Test_Acc: 0.8860\n",
            "Epoch: 846, Train_Loss: 0.0026, Test_Loss: 0.0077, Train_Acc:0.9585, Test_Acc: 0.8838\n",
            "Epoch: 847, Train_Loss: 0.0027, Test_Loss: 0.0077, Train_Acc:0.9532, Test_Acc: 0.8991\n",
            "Epoch: 848, Train_Loss: 0.0028, Test_Loss: 0.0075, Train_Acc:0.9559, Test_Acc: 0.9013\n",
            "Epoch: 849, Train_Loss: 0.0022, Test_Loss: 0.0076, Train_Acc:0.9665, Test_Acc: 0.9013\n",
            "Epoch: 850, Train_Loss: 0.0026, Test_Loss: 0.0076, Train_Acc:0.9665, Test_Acc: 0.9035\n",
            "Epoch: 851, Train_Loss: 0.0028, Test_Loss: 0.0086, Train_Acc:0.9644, Test_Acc: 0.9057\n",
            "Epoch: 852, Train_Loss: 0.0026, Test_Loss: 0.0082, Train_Acc:0.9644, Test_Acc: 0.8947\n",
            "Epoch: 853, Train_Loss: 0.0024, Test_Loss: 0.0080, Train_Acc:0.9527, Test_Acc: 0.8772\n",
            "Epoch: 854, Train_Loss: 0.0028, Test_Loss: 0.0085, Train_Acc:0.9532, Test_Acc: 0.8772\n",
            "Epoch: 855, Train_Loss: 0.0027, Test_Loss: 0.0086, Train_Acc:0.9564, Test_Acc: 0.8816\n",
            "Epoch: 856, Train_Loss: 0.0027, Test_Loss: 0.0088, Train_Acc:0.9617, Test_Acc: 0.8816\n",
            "Epoch: 857, Train_Loss: 0.0026, Test_Loss: 0.0093, Train_Acc:0.9580, Test_Acc: 0.8772\n",
            "Epoch: 858, Train_Loss: 0.0029, Test_Loss: 0.0093, Train_Acc:0.9559, Test_Acc: 0.8772\n",
            "Epoch: 859, Train_Loss: 0.0026, Test_Loss: 0.0091, Train_Acc:0.9633, Test_Acc: 0.8750\n",
            "Epoch: 860, Train_Loss: 0.0023, Test_Loss: 0.0091, Train_Acc:0.9612, Test_Acc: 0.8794\n",
            "Epoch: 861, Train_Loss: 0.0025, Test_Loss: 0.0088, Train_Acc:0.9617, Test_Acc: 0.8750\n",
            "Epoch: 862, Train_Loss: 0.0024, Test_Loss: 0.0088, Train_Acc:0.9649, Test_Acc: 0.8794\n",
            "Epoch: 863, Train_Loss: 0.0026, Test_Loss: 0.0083, Train_Acc:0.9601, Test_Acc: 0.8816\n",
            "Epoch: 864, Train_Loss: 0.0025, Test_Loss: 0.0084, Train_Acc:0.9606, Test_Acc: 0.8728\n",
            "Epoch: 865, Train_Loss: 0.0028, Test_Loss: 0.0077, Train_Acc:0.9638, Test_Acc: 0.8772\n",
            "Epoch: 866, Train_Loss: 0.0031, Test_Loss: 0.0088, Train_Acc:0.9644, Test_Acc: 0.8575\n",
            "Epoch: 867, Train_Loss: 0.0026, Test_Loss: 0.0087, Train_Acc:0.9580, Test_Acc: 0.8750\n",
            "Epoch: 868, Train_Loss: 0.0026, Test_Loss: 0.0081, Train_Acc:0.9612, Test_Acc: 0.8794\n",
            "Epoch: 869, Train_Loss: 0.0024, Test_Loss: 0.0084, Train_Acc:0.9590, Test_Acc: 0.8794\n",
            "Epoch: 870, Train_Loss: 0.0025, Test_Loss: 0.0077, Train_Acc:0.9638, Test_Acc: 0.8904\n",
            "Epoch: 871, Train_Loss: 0.0024, Test_Loss: 0.0077, Train_Acc:0.9495, Test_Acc: 0.8991\n",
            "Epoch: 872, Train_Loss: 0.0023, Test_Loss: 0.0073, Train_Acc:0.9574, Test_Acc: 0.8882\n",
            "Epoch: 873, Train_Loss: 0.0024, Test_Loss: 0.0071, Train_Acc:0.9601, Test_Acc: 0.9035\n",
            "Epoch: 874, Train_Loss: 0.0025, Test_Loss: 0.0076, Train_Acc:0.9367, Test_Acc: 0.8684\n",
            "Epoch: 875, Train_Loss: 0.0026, Test_Loss: 0.0076, Train_Acc:0.9505, Test_Acc: 0.8882\n",
            "Epoch: 876, Train_Loss: 0.0026, Test_Loss: 0.0075, Train_Acc:0.9622, Test_Acc: 0.8925\n",
            "Epoch: 877, Train_Loss: 0.0026, Test_Loss: 0.0076, Train_Acc:0.9574, Test_Acc: 0.8947\n",
            "Epoch: 878, Train_Loss: 0.0023, Test_Loss: 0.0076, Train_Acc:0.9564, Test_Acc: 0.8991\n",
            "Epoch: 879, Train_Loss: 0.0023, Test_Loss: 0.0077, Train_Acc:0.9638, Test_Acc: 0.8904\n",
            "Epoch: 880, Train_Loss: 0.0022, Test_Loss: 0.0079, Train_Acc:0.9628, Test_Acc: 0.8882\n",
            "Epoch: 881, Train_Loss: 0.0025, Test_Loss: 0.0079, Train_Acc:0.9638, Test_Acc: 0.8860\n",
            "Epoch: 882, Train_Loss: 0.0023, Test_Loss: 0.0085, Train_Acc:0.9516, Test_Acc: 0.8794\n",
            "Epoch: 883, Train_Loss: 0.0024, Test_Loss: 0.0088, Train_Acc:0.9431, Test_Acc: 0.8640\n",
            "Epoch: 884, Train_Loss: 0.0027, Test_Loss: 0.0081, Train_Acc:0.9585, Test_Acc: 0.8706\n",
            "Epoch: 885, Train_Loss: 0.0021, Test_Loss: 0.0085, Train_Acc:0.9612, Test_Acc: 0.8816\n",
            "Epoch: 886, Train_Loss: 0.0022, Test_Loss: 0.0093, Train_Acc:0.9543, Test_Acc: 0.8640\n",
            "Epoch: 887, Train_Loss: 0.0026, Test_Loss: 0.0093, Train_Acc:0.9585, Test_Acc: 0.8706\n",
            "Epoch: 888, Train_Loss: 0.0025, Test_Loss: 0.0094, Train_Acc:0.9410, Test_Acc: 0.8640\n",
            "Epoch: 889, Train_Loss: 0.0027, Test_Loss: 0.0086, Train_Acc:0.9574, Test_Acc: 0.8816\n",
            "Epoch: 890, Train_Loss: 0.0023, Test_Loss: 0.0077, Train_Acc:0.9564, Test_Acc: 0.8882\n",
            "Epoch: 891, Train_Loss: 0.0028, Test_Loss: 0.0080, Train_Acc:0.9596, Test_Acc: 0.8882\n",
            "Epoch: 892, Train_Loss: 0.0023, Test_Loss: 0.0081, Train_Acc:0.9500, Test_Acc: 0.8706\n",
            "Epoch: 893, Train_Loss: 0.0026, Test_Loss: 0.0079, Train_Acc:0.9489, Test_Acc: 0.8816\n",
            "Epoch: 894, Train_Loss: 0.0025, Test_Loss: 0.0082, Train_Acc:0.9601, Test_Acc: 0.8904\n",
            "Epoch: 895, Train_Loss: 0.0025, Test_Loss: 0.0083, Train_Acc:0.9638, Test_Acc: 0.8947\n",
            "Epoch: 896, Train_Loss: 0.0025, Test_Loss: 0.0089, Train_Acc:0.9564, Test_Acc: 0.8794\n",
            "Epoch: 897, Train_Loss: 0.0022, Test_Loss: 0.0089, Train_Acc:0.9580, Test_Acc: 0.8750\n",
            "Epoch: 898, Train_Loss: 0.0026, Test_Loss: 0.0079, Train_Acc:0.9601, Test_Acc: 0.8904\n",
            "Epoch: 899, Train_Loss: 0.0024, Test_Loss: 0.0086, Train_Acc:0.9537, Test_Acc: 0.8794\n",
            "Epoch: 900, Train_Loss: 0.0024, Test_Loss: 0.0094, Train_Acc:0.9548, Test_Acc: 0.8662\n",
            "Epoch: 901, Train_Loss: 0.0025, Test_Loss: 0.0087, Train_Acc:0.9548, Test_Acc: 0.8860\n",
            "Epoch: 902, Train_Loss: 0.0021, Test_Loss: 0.0079, Train_Acc:0.9628, Test_Acc: 0.8947\n",
            "Epoch: 903, Train_Loss: 0.0024, Test_Loss: 0.0074, Train_Acc:0.9660, Test_Acc: 0.9035\n",
            "Epoch: 904, Train_Loss: 0.0022, Test_Loss: 0.0078, Train_Acc:0.9628, Test_Acc: 0.8969\n",
            "Epoch: 905, Train_Loss: 0.0027, Test_Loss: 0.0083, Train_Acc:0.9457, Test_Acc: 0.8816\n",
            "Epoch: 906, Train_Loss: 0.0026, Test_Loss: 0.0078, Train_Acc:0.9559, Test_Acc: 0.8904\n",
            "Epoch: 907, Train_Loss: 0.0023, Test_Loss: 0.0083, Train_Acc:0.9617, Test_Acc: 0.8860\n",
            "Epoch: 908, Train_Loss: 0.0025, Test_Loss: 0.0086, Train_Acc:0.9537, Test_Acc: 0.8969\n",
            "Epoch: 909, Train_Loss: 0.0025, Test_Loss: 0.0080, Train_Acc:0.9644, Test_Acc: 0.8969\n",
            "Epoch: 910, Train_Loss: 0.0023, Test_Loss: 0.0079, Train_Acc:0.9606, Test_Acc: 0.8882\n",
            "Epoch: 911, Train_Loss: 0.0025, Test_Loss: 0.0085, Train_Acc:0.9452, Test_Acc: 0.8618\n",
            "Epoch: 912, Train_Loss: 0.0025, Test_Loss: 0.0089, Train_Acc:0.9447, Test_Acc: 0.8575\n",
            "Epoch: 913, Train_Loss: 0.0025, Test_Loss: 0.0075, Train_Acc:0.9410, Test_Acc: 0.8816\n",
            "Epoch: 914, Train_Loss: 0.0024, Test_Loss: 0.0071, Train_Acc:0.9574, Test_Acc: 0.8969\n",
            "Epoch: 915, Train_Loss: 0.0029, Test_Loss: 0.0073, Train_Acc:0.9516, Test_Acc: 0.9035\n",
            "Epoch: 916, Train_Loss: 0.0028, Test_Loss: 0.0070, Train_Acc:0.9649, Test_Acc: 0.8991\n",
            "Epoch: 917, Train_Loss: 0.0025, Test_Loss: 0.0069, Train_Acc:0.9590, Test_Acc: 0.9035\n",
            "Epoch: 918, Train_Loss: 0.0026, Test_Loss: 0.0075, Train_Acc:0.9628, Test_Acc: 0.8882\n",
            "Epoch: 919, Train_Loss: 0.0026, Test_Loss: 0.0075, Train_Acc:0.9622, Test_Acc: 0.8860\n",
            "Epoch: 920, Train_Loss: 0.0025, Test_Loss: 0.0075, Train_Acc:0.9644, Test_Acc: 0.8904\n",
            "Epoch: 921, Train_Loss: 0.0024, Test_Loss: 0.0081, Train_Acc:0.9654, Test_Acc: 0.8925\n",
            "Epoch: 922, Train_Loss: 0.0023, Test_Loss: 0.0082, Train_Acc:0.9622, Test_Acc: 0.8860\n",
            "Epoch: 923, Train_Loss: 0.0024, Test_Loss: 0.0085, Train_Acc:0.9649, Test_Acc: 0.8860\n",
            "Epoch: 924, Train_Loss: 0.0025, Test_Loss: 0.0088, Train_Acc:0.9612, Test_Acc: 0.8816\n",
            "Epoch: 925, Train_Loss: 0.0024, Test_Loss: 0.0086, Train_Acc:0.9574, Test_Acc: 0.8904\n",
            "Epoch: 926, Train_Loss: 0.0025, Test_Loss: 0.0077, Train_Acc:0.9559, Test_Acc: 0.8838\n",
            "Epoch: 927, Train_Loss: 0.0025, Test_Loss: 0.0073, Train_Acc:0.9559, Test_Acc: 0.8860\n",
            "Epoch: 928, Train_Loss: 0.0026, Test_Loss: 0.0070, Train_Acc:0.9596, Test_Acc: 0.8947\n",
            "Epoch: 929, Train_Loss: 0.0027, Test_Loss: 0.0080, Train_Acc:0.9585, Test_Acc: 0.8969\n",
            "Epoch: 930, Train_Loss: 0.0026, Test_Loss: 0.0076, Train_Acc:0.9644, Test_Acc: 0.8816\n",
            "Epoch: 931, Train_Loss: 0.0022, Test_Loss: 0.0074, Train_Acc:0.9713, Test_Acc: 0.8925\n",
            "Epoch: 932, Train_Loss: 0.0023, Test_Loss: 0.0077, Train_Acc:0.9521, Test_Acc: 0.8816\n",
            "Epoch: 933, Train_Loss: 0.0025, Test_Loss: 0.0075, Train_Acc:0.9436, Test_Acc: 0.8794\n",
            "Epoch: 934, Train_Loss: 0.0021, Test_Loss: 0.0074, Train_Acc:0.9553, Test_Acc: 0.8750\n",
            "Epoch: 935, Train_Loss: 0.0023, Test_Loss: 0.0087, Train_Acc:0.9559, Test_Acc: 0.8772\n",
            "Epoch: 936, Train_Loss: 0.0026, Test_Loss: 0.0083, Train_Acc:0.9511, Test_Acc: 0.8772\n",
            "Epoch: 937, Train_Loss: 0.0027, Test_Loss: 0.0082, Train_Acc:0.9543, Test_Acc: 0.8684\n",
            "Epoch: 938, Train_Loss: 0.0024, Test_Loss: 0.0079, Train_Acc:0.9521, Test_Acc: 0.8706\n",
            "Epoch: 939, Train_Loss: 0.0026, Test_Loss: 0.0074, Train_Acc:0.9590, Test_Acc: 0.8772\n",
            "Epoch: 940, Train_Loss: 0.0022, Test_Loss: 0.0070, Train_Acc:0.9606, Test_Acc: 0.8794\n",
            "Epoch: 941, Train_Loss: 0.0025, Test_Loss: 0.0070, Train_Acc:0.9702, Test_Acc: 0.9013\n",
            "Epoch: 942, Train_Loss: 0.0024, Test_Loss: 0.0074, Train_Acc:0.9569, Test_Acc: 0.8838\n",
            "Epoch: 943, Train_Loss: 0.0025, Test_Loss: 0.0077, Train_Acc:0.9596, Test_Acc: 0.8860\n",
            "Epoch: 944, Train_Loss: 0.0023, Test_Loss: 0.0091, Train_Acc:0.9516, Test_Acc: 0.8728\n",
            "Epoch: 945, Train_Loss: 0.0024, Test_Loss: 0.0091, Train_Acc:0.9495, Test_Acc: 0.8662\n",
            "Epoch: 946, Train_Loss: 0.0024, Test_Loss: 0.0083, Train_Acc:0.9511, Test_Acc: 0.8728\n",
            "Epoch: 947, Train_Loss: 0.0027, Test_Loss: 0.0078, Train_Acc:0.9622, Test_Acc: 0.8925\n",
            "Epoch: 948, Train_Loss: 0.0027, Test_Loss: 0.0078, Train_Acc:0.9644, Test_Acc: 0.8969\n",
            "Epoch: 949, Train_Loss: 0.0025, Test_Loss: 0.0077, Train_Acc:0.9457, Test_Acc: 0.8860\n",
            "Epoch: 950, Train_Loss: 0.0028, Test_Loss: 0.0078, Train_Acc:0.9527, Test_Acc: 0.8838\n",
            "Epoch: 951, Train_Loss: 0.0025, Test_Loss: 0.0086, Train_Acc:0.9495, Test_Acc: 0.8794\n",
            "Epoch: 952, Train_Loss: 0.0028, Test_Loss: 0.0093, Train_Acc:0.9441, Test_Acc: 0.8816\n",
            "Epoch: 953, Train_Loss: 0.0027, Test_Loss: 0.0089, Train_Acc:0.9319, Test_Acc: 0.8750\n",
            "Epoch: 954, Train_Loss: 0.0028, Test_Loss: 0.0097, Train_Acc:0.9410, Test_Acc: 0.8728\n",
            "Epoch: 955, Train_Loss: 0.0033, Test_Loss: 0.0090, Train_Acc:0.9436, Test_Acc: 0.8618\n",
            "Epoch: 956, Train_Loss: 0.0044, Test_Loss: 0.0094, Train_Acc:0.9356, Test_Acc: 0.8728\n",
            "Epoch: 957, Train_Loss: 0.0045, Test_Loss: 0.0095, Train_Acc:0.9096, Test_Acc: 0.8596\n",
            "Epoch: 958, Train_Loss: 0.0046, Test_Loss: 0.0087, Train_Acc:0.9335, Test_Acc: 0.8838\n",
            "Epoch: 959, Train_Loss: 0.0046, Test_Loss: 0.0096, Train_Acc:0.9324, Test_Acc: 0.8596\n",
            "Epoch: 960, Train_Loss: 0.0046, Test_Loss: 0.0092, Train_Acc:0.9160, Test_Acc: 0.8443\n",
            "Epoch: 961, Train_Loss: 0.0042, Test_Loss: 0.0092, Train_Acc:0.9186, Test_Acc: 0.8618\n",
            "Epoch: 962, Train_Loss: 0.0042, Test_Loss: 0.0092, Train_Acc:0.9181, Test_Acc: 0.8553\n",
            "Epoch: 963, Train_Loss: 0.0040, Test_Loss: 0.0097, Train_Acc:0.9314, Test_Acc: 0.8596\n",
            "Epoch: 964, Train_Loss: 0.0046, Test_Loss: 0.0096, Train_Acc:0.9394, Test_Acc: 0.8750\n",
            "Epoch: 965, Train_Loss: 0.0040, Test_Loss: 0.0105, Train_Acc:0.9277, Test_Acc: 0.8487\n",
            "Epoch: 966, Train_Loss: 0.0041, Test_Loss: 0.0125, Train_Acc:0.9176, Test_Acc: 0.8268\n",
            "Epoch: 967, Train_Loss: 0.0041, Test_Loss: 0.0111, Train_Acc:0.9154, Test_Acc: 0.8509\n",
            "Epoch: 968, Train_Loss: 0.0039, Test_Loss: 0.0106, Train_Acc:0.9223, Test_Acc: 0.8662\n",
            "Epoch: 969, Train_Loss: 0.0041, Test_Loss: 0.0084, Train_Acc:0.9356, Test_Acc: 0.8794\n",
            "Epoch: 970, Train_Loss: 0.0038, Test_Loss: 0.0091, Train_Acc:0.9255, Test_Acc: 0.8816\n",
            "Epoch: 971, Train_Loss: 0.0036, Test_Loss: 0.0098, Train_Acc:0.9181, Test_Acc: 0.8596\n",
            "Epoch: 972, Train_Loss: 0.0038, Test_Loss: 0.0093, Train_Acc:0.9378, Test_Acc: 0.8750\n",
            "Epoch: 973, Train_Loss: 0.0039, Test_Loss: 0.0103, Train_Acc:0.9282, Test_Acc: 0.8640\n",
            "Epoch: 974, Train_Loss: 0.0044, Test_Loss: 0.0091, Train_Acc:0.9431, Test_Acc: 0.8750\n",
            "Epoch: 975, Train_Loss: 0.0047, Test_Loss: 0.0082, Train_Acc:0.9447, Test_Acc: 0.8969\n",
            "Epoch: 976, Train_Loss: 0.0041, Test_Loss: 0.0092, Train_Acc:0.9388, Test_Acc: 0.8684\n",
            "Epoch: 977, Train_Loss: 0.0041, Test_Loss: 0.0092, Train_Acc:0.9165, Test_Acc: 0.8487\n",
            "Epoch: 978, Train_Loss: 0.0045, Test_Loss: 0.0073, Train_Acc:0.9447, Test_Acc: 0.8838\n",
            "Epoch: 979, Train_Loss: 0.0040, Test_Loss: 0.0080, Train_Acc:0.9410, Test_Acc: 0.8816\n",
            "Epoch: 980, Train_Loss: 0.0041, Test_Loss: 0.0083, Train_Acc:0.9473, Test_Acc: 0.8838\n",
            "Epoch: 981, Train_Loss: 0.0036, Test_Loss: 0.0083, Train_Acc:0.9394, Test_Acc: 0.8925\n",
            "Epoch: 982, Train_Loss: 0.0038, Test_Loss: 0.0080, Train_Acc:0.9378, Test_Acc: 0.8794\n",
            "Epoch: 983, Train_Loss: 0.0034, Test_Loss: 0.0084, Train_Acc:0.9303, Test_Acc: 0.8596\n",
            "Epoch: 984, Train_Loss: 0.0036, Test_Loss: 0.0091, Train_Acc:0.9580, Test_Acc: 0.8728\n",
            "Epoch: 985, Train_Loss: 0.0034, Test_Loss: 0.0089, Train_Acc:0.9628, Test_Acc: 0.8728\n",
            "Epoch: 986, Train_Loss: 0.0030, Test_Loss: 0.0082, Train_Acc:0.9351, Test_Acc: 0.8728\n",
            "Epoch: 987, Train_Loss: 0.0035, Test_Loss: 0.0081, Train_Acc:0.9548, Test_Acc: 0.8816\n",
            "Epoch: 988, Train_Loss: 0.0034, Test_Loss: 0.0089, Train_Acc:0.9473, Test_Acc: 0.8772\n",
            "Epoch: 989, Train_Loss: 0.0033, Test_Loss: 0.0090, Train_Acc:0.9516, Test_Acc: 0.8706\n",
            "Epoch: 990, Train_Loss: 0.0034, Test_Loss: 0.0086, Train_Acc:0.9495, Test_Acc: 0.8553\n",
            "Epoch: 991, Train_Loss: 0.0032, Test_Loss: 0.0082, Train_Acc:0.9479, Test_Acc: 0.8750\n",
            "Epoch: 992, Train_Loss: 0.0035, Test_Loss: 0.0077, Train_Acc:0.9580, Test_Acc: 0.9013\n",
            "Epoch: 993, Train_Loss: 0.0032, Test_Loss: 0.0072, Train_Acc:0.9484, Test_Acc: 0.8947\n",
            "Epoch: 994, Train_Loss: 0.0028, Test_Loss: 0.0075, Train_Acc:0.9431, Test_Acc: 0.8947\n",
            "Epoch: 995, Train_Loss: 0.0035, Test_Loss: 0.0084, Train_Acc:0.9585, Test_Acc: 0.8772\n",
            "Epoch: 996, Train_Loss: 0.0030, Test_Loss: 0.0090, Train_Acc:0.9559, Test_Acc: 0.8706\n",
            "Epoch: 997, Train_Loss: 0.0029, Test_Loss: 0.0087, Train_Acc:0.9484, Test_Acc: 0.8904\n",
            "Epoch: 998, Train_Loss: 0.0031, Test_Loss: 0.0093, Train_Acc:0.9527, Test_Acc: 0.8860\n",
            "Epoch: 999, Train_Loss: 0.0028, Test_Loss: 0.0094, Train_Acc:0.9516, Test_Acc: 0.8969\n",
            "Epoch: 1000, Train_Loss: 0.0030, Test_Loss: 0.0081, Train_Acc:0.9580, Test_Acc: 0.9167\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = {'batch': [], 'epoch': []}\n",
        "        self.accuracy = {'batch': [], 'epoch': []}\n",
        "        self.val_loss = {'batch': [], 'epoch': []}\n",
        "        self.val_acc = {'batch': [], 'epoch': []}\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses['batch'].append(logs.get('loss'))\n",
        "        self.accuracy['batch'].append(logs.get('acc'))\n",
        "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
        "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
        "\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        self.losses['epoch'].append(logs.get('loss'))\n",
        "        self.accuracy['epoch'].append(logs.get('acc'))\n",
        "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
        "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
        "\n",
        "    def loss_plot(self, loss_type):\n",
        "        iters = range(len(self.losses[loss_type]))\n",
        "        fig = plt.figure()\n",
        "        # acc\n",
        "        # plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
        "        # loss\n",
        "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
        "        if loss_type == 'epoch':\n",
        "            # val_acc\n",
        "            # plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
        "            # val_loss\n",
        "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
        "        plt.grid(True)\n",
        "        plt.xlabel(loss_type, FontSize=24)\n",
        "        plt.ylabel('Loss', FontSize=24)\n",
        "        plt.legend(loc=\"upper right\", fontsize = 24)\n",
        "        # plt.show()\n",
        "        fig.set_size_inches(16, 9)\n",
        "        plt.xticks(fontsize=20)\n",
        "        plt.yticks(fontsize=20)\n",
        "        fig.savefig('lossPlot', dpi=fig.dpi)\n",
        "\n",
        "batch_size = 100\n",
        "nb_classes = 1\n",
        "nb_epoch = 1000\n",
        "\n",
        "def r2_keras(y_true, y_pred):\n",
        "    SS_res =  K.sum(K.square(y_true - y_pred))\n",
        "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
        "\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        self.lin1 = Linear(33,256)\n",
        "        self.lin2 = Linear(256,512)\n",
        "        self.lin3 = Linear(512,256)\n",
        "        self.lin4 = Linear(256,1)\n",
        "\n",
        "        self.bnm1 = nn.BatchNorm1d(256, momentum=0.1)\n",
        "        self.bnm2 = nn.BatchNorm1d(512, momentum=0.1)\n",
        "        self.bnm3 = nn.BatchNorm1d(256, momentum=0.1)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = self.bnm1(x)\n",
        "        x = x.relu()\n",
        "        x = self.dropout(x)\n",
        "        x = self.lin2(x)\n",
        "        x = self.bnm2(x)\n",
        "        x = x.relu()\n",
        "        x = self.lin3(x)\n",
        "        x = self.bnm3(x)\n",
        "        x = x.relu()\n",
        "        x = self.lin4(x)      \n",
        "        return x     \n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        self.conv1 = GCNConv(21, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, 112)\n",
        "        # self.conv2 = GCNConv(hidden_channels, 50)\n",
        "        # self.lin1 = Linear(50,256)\n",
        "        self.lin2 = Linear(112,256)\n",
        "        self.lin3 = Linear(256,100)\n",
        "        self.lin4 = Linear(100,1)\n",
        "\n",
        "        self.bnm1 = nn.BatchNorm1d(112, momentum=0.1)\n",
        "        self.bnm2 = nn.BatchNorm1d(256, momentum=0.1)\n",
        "        self.bnm3 = nn.BatchNorm1d(100, momentum=0.1)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        # x = F.dropout(x, p=0.1, training=self.training)\n",
        "        # x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        # x = self.lin1(x)\n",
        "        x = self.bnm1(x)\n",
        "        x = x.relu()\n",
        "        x = self.dropout(x)\n",
        "        x = self.lin2(x)\n",
        "        x = self.bnm2(x)\n",
        "        x = x.relu()\n",
        "        x = self.lin3(x)\n",
        "        x = self.bnm3(x)\n",
        "        x = x.relu()\n",
        "        x = self.lin4(x)  \n",
        "        return x\n",
        "\n",
        "class GCN1(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        self.conv1 = GCNConv(15, 150)\n",
        "        self.conv2 = GCNConv(150, 200)\n",
        "        self.lin2 = Linear(200,400)\n",
        "        self.lin3 = Linear(400,200)\n",
        "        self.lin4 = Linear(200,1)\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(2,2)\n",
        "\n",
        "        self.bnm1 = nn.BatchNorm1d(200, momentum=0.1)\n",
        "        self.bnm2 = nn.BatchNorm1d(400, momentum=0.1)\n",
        "        self.bnm3 = nn.BatchNorm1d(200, momentum=0.1)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        # x = F.dropout(x, p=0.1, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        # x = self.lin1(x)\n",
        "        x = self.bnm1(x)\n",
        "        x = x.relu()\n",
        "        x = self.dropout(x)\n",
        "        x = self.lin2(x)\n",
        "        x = self.bnm2(x)\n",
        "        x = x.relu()\n",
        "        # x = self.dropout(x)\n",
        "        x = self.lin3(x)\n",
        "        x = self.bnm3(x)\n",
        "        x = x.relu()\n",
        "        x = self.lin4(x)  \n",
        "        return x\n",
        "\n",
        "\n",
        "# print(model)\n",
        "model = GCN1()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.025, weight_decay=1e-4)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.008, weight_decay=1e-4)\n",
        "# criterion = torch.nn.MSELoss()\n",
        "criterion = torch.nn.HuberLoss(reduction='mean',delta=0.15)\n",
        "\n",
        "def train():\n",
        "      model.to(\"cuda:0\")\n",
        "      train_data.to(\"cuda:0\")\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(train_data.x, train_data.edge_index)\n",
        "      #out = model(train_data.x)\n",
        "      loss = criterion(out, train_data.y)  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss\n",
        "\n",
        "# lmbd = 0.01\n",
        "# def train():\n",
        "#       model.to(\"cuda:0\")\n",
        "#       train_data.to(\"cuda:0\")\n",
        "#       model.train()\n",
        "#       optimizer.zero_grad()  # Clear gradients.\n",
        "#       out = model(train_data.x, train_data.edge_index)\n",
        "#       #out = model(train_data.x)\n",
        "#       loss = criterion(out, train_data.y)  # Compute the loss solely based on the training nodes.\n",
        "#       reg_loss = None\n",
        "#       for param in model.parameters():\n",
        "#           if reg_loss is None:\n",
        "#               reg_loss = 0.5 * torch.sum(param**2)\n",
        "#           else:\n",
        "#               reg_loss = reg_loss + 0.5 * param.norm(2)**2\n",
        "\n",
        "#       loss += lmbd * reg_loss\n",
        "#       loss.backward()  # Derive gradients.\n",
        "#       optimizer.step()  # Update parameters based on gradients.\n",
        "#       return loss\n",
        "\n",
        "def test():\n",
        "      model.eval()\n",
        "      test_data.to(\"cuda:0\")\n",
        "      #out = model(test_data.x)\n",
        "      out = model(test_data.x, test_data.edge_index)\n",
        "      loss = criterion(out, test_data.y)\n",
        "      pred = out  # Use the class with highest probability.\n",
        "      # train_correct = abs((pred-test_data.y/test_data.y))< 0.5  # Check against ground-truth labels.\n",
        "      # train_acc = int(train_correct.sum()) / int(len(test_data)) \n",
        "      # loss.backward() \n",
        "      # optimizer.step()\n",
        "      return loss\n",
        "\n",
        "Train_losses = []\n",
        "Test_losses = []\n",
        "\n",
        "for epoch in range(1, 1001):\n",
        "    Train_loss = train()\n",
        "    Test_loss = test()\n",
        "    model.eval()\n",
        "    test_data.to(\"cuda:0\")\n",
        "    train_data.to(\"cuda:0\")\n",
        "    #test_predictions = model(test_data.x)\n",
        "    test_predictions = model(test_data.x, test_data.edge_index)\n",
        "    train_predictions = model(train_data.x, train_data.edge_index)\n",
        "    # test_predictions_inverse = scalerY.inverse_transform(test_predictions.detach().numpy())\n",
        "    test_correct = abs((test_predictions-test_data.y)/test_predictions) < 0.25  # Check against ground-truth labels.\n",
        "    test_acc = int(test_correct.sum()) / len(test_predictions)\n",
        "    train_correct = abs((train_predictions-train_data.y)/train_predictions) < 0.25  # Check against ground-truth labels.\n",
        "    train_acc = int(train_correct.sum()) / len(train_predictions)\n",
        "    Train_losses.append(Train_loss.detach().cpu().numpy())\n",
        "    Test_losses.append(Test_loss.detach().cpu().numpy())\n",
        "    \n",
        "    print(f'Epoch: {epoch:03d}, Train_Loss: {Train_loss:.4f}, Test_Loss: {Test_loss:.4f}, Train_Acc:{train_acc:.4f}, Test_Acc: {test_acc:.4f}')\n",
        "    # print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "\n",
        "\n",
        "def draw_loss():\n",
        "  plt.figure()\n",
        "  plt.title('loss',fontsize=24)\n",
        "  plt.plot(Train_losses,'-')\n",
        "  plt.plot(Test_losses,'-')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['Train','Test'])\n",
        "  plt.show\n",
        "\n",
        "draw_loss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "McOBfXHqX5zi",
        "outputId": "46b4515b-6e9c-4dd8-a295-cefe8838bf01"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.eval()\n",
        "train_predictions = model(train_data.x, train_data.edge_index)\n",
        "# train_predictions = model(train_data.x)\n",
        "train_predictions_inverse = scalerY.inverse_transform(train_predictions.detach().cpu().numpy())\n",
        "train_Y_inverse = scalerY.inverse_transform(train_Y)\n",
        "prediction_frame = pd.DataFrame(train_predictions_inverse, index=dataset_train.index)\n",
        "test_2 = pd.concat([pd.DataFrame(test_X), pd.DataFrame(train_Y)], axis=1)\n",
        "result = pd.concat([test_2, prediction_frame], axis=1)\n",
        "result.rename(columns={0:'Prediction'},inplace = True)\n",
        "# result = test.join(prediction_frame)\n",
        "# pd.merge(test, prediction_frame, left_index=True)\n",
        "result.to_csv('result_Analysis_MAE_train.csv')\n",
        "\n",
        "writer = pd.ExcelWriter('result_analysis_train.xlsx')\n",
        "result.to_excel(writer, 'Sheet1')\n",
        "writer.save()\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.title('Training Set')\n",
        "plt.scatter(train_Y_inverse, train_predictions_inverse)\n",
        "plt.xlabel('True Values [ACH]')\n",
        "plt.ylabel('Predictions [ACH]')\n",
        "x = np.linspace(-1,1)\n",
        "y = x\n",
        "plt.plot(x, y, '-r', label='y=x')\n",
        "plt.axis('equal')\n",
        "plt.axis('square')\n",
        "# plt.show()\n",
        "fig.set_size_inches(6, 4)\n",
        "fig.savefig('Training set.png', dpi=fig.dpi)\n",
        "# plt.xlim([-0.1, 55])\n",
        "# plt.ylim([-0.1, 55])\n",
        "# _ = plt.plot([-100, 100], [-100, 100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "7dGmrwNlYBHF",
        "outputId": "e804b61f-6aef-415e-f40e-1dbb8dd266c1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEWCAYAAABWszP/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2de5hcVZW331+ahjSgdEIilwYSbgaBCIFGUEANKpFxgKAQifAJihecQUfRfIaLAg6M0Sh8OuOIURERDZcobRguMRBABw2S2IkhSCTghTQNBJPmIi10Ouv74+xqTlfX5VTXvWq9z1NP1dn7nFPrNKkfe6+191oyMxzHccrNmGob4DhOc+Bi4zhORXCxcRynIrjYOI5TEVxsHMepCC42juNUBBcbp6pIelHSPtW2wyk/LjZOVoIQpF5bJfXHjs8Yxf3ulfSReJuZ7Whmj5fO6qHvapd0jaSnJL0g6Y+S5ia89lpJl5fapmZnm2ob4NQuZrZj6rOkPwMfMbO7qmdRQVwF7AC8AXgOeD1wcFUtanJ8ZOMUjKQxkuZKekzS3yTdJGl86Bsr6frQ3ifpQUm7SLoCOBb4rzAy+q9wvknaL3y+VtK3JN0WRiMPSNo39r3HS1on6TlJ/y3pvvSRUowjgJ+Y2WYz22pmj5jZoti9DpC0VNKmcM9Zof1jwBnA/w123lqOv2Ez4mLjjIZPAjOBtwG7A5uBb4W+s4CdgD2BnYFzgX4zuwj4FXBemDqdl+XepwOXAeOA9cAVAJImAIuAC8J91wFvyWHjcuAKSR+StH+8Q9IOwFLgJ8Drwnf+t6QDzWwB8GPgq8HOE5P9SZx8uNg4o+Fc4CIz22BmLwOXAqdK2gYYIBKD/cxs0MxWmtnzBdz7FjP7rZltIfrRHxra/wlYa2Y/C33fBJ7KcZ9PhuvPAx6WtF7SCaHvn4E/m9kPzGyLmXUDPwVOK8BOp0BcbJzRMAm4JUyT+oA/AIPALsCPgCXADZKelPRVSa0F3DsuIC8BKb/R7sATqQ6LdhBvyHYTM+s3s/8ws8OJxO8m4OYw3ZsEHJmyPzzDGcCuBdjpFIiLjTMangBOMLP22GusmfWY2YCZXWZmBxJNc/4Z+GC4rpgUA73AHqkDSYof5yKMrP6DyGG8d7D/vjT7dzSzT5TATicLLjbOaLiayB8yCUDSREknh8/TJU2V1AI8TzSt2hquexoY7Zqa24CpkmaG6dq/kmMkIukLko6QtK2kscC/AX1Evp7/AV4v6f9Iag2vIyS9oQR2OllwsXFGwzeAxcAvJL1A5Iw9MvTtSuTIfZ5oenUf0dQqdd2pkjZL+mYhX2hmzxL5VL4K/A04EFgBvJztEuAHwLPAk8C7gPeY2Ytm9gJwPJFj+EmiqdtXgO3Ctd8HDgxTrK5C7HSyI0+e5dQjksYQ+WzOMLN7qm2Pkx8f2Th1g6QZYWXwdsCFgIhGVU4d4GLj1BNvBh4jmhqdCMw0s/7qmuQkxadRjuNUBB/ZOI5TEZpqI+aECRNs8uTJ1TbDcRqWlStXPmtmEzP1NZXYTJ48mRUrVlTbDMdpWCT9JVufT6Mcx6kILjaO41QEFxvHcSqCi43jOBXBxcZxnIpQVbEJCamfkfRQln5J+mZIfPR7SYfF+s6S9Gh4nVU5qx3HGQ3VHtlcC7w7R/8JwP7h9THg2wAhAdIlRDuN3wRcImlcWS11HKcoqio2ZvZLYFOOU04GrrOI5UC7pN2AGcBSM9tkZpuJ8snmEi3HcUZLby+cfz688kpRt6n2yCYfHcRSQRKlFOjI0T4CSR+TtELSio0bN5bNUMdpSHp7Yfp0WLAA1q0r6la1LjZFY2YLzKzTzDonTsy4itpxnEykhGbDBrjzTpg6tajb1brY9BCVBEmxR2jL1u44TilIF5pjjin6lrUuNouBD4ao1FHAc2bWS5S9/3hJ44Jj+PjQ5jhOsZRBaKDKGzElLQTeDkyQtIEowtQKYGZXA7cT1QtaT1TW40Ohb5OkfwceDLf6kpnlcjQ7jpOEMgkNVFlszGx2nn4jyqKfqe8a4Jpy2OU4TUlcaO64o6RCA02WYsJxmomu7h7mL1nHk3397N7exvQDJnLPIxuHjufMmMLMaSGIW8YRTQoXG8dpQC7uWsOPl/91qNpeT18/1y//61B/T18/F/xsDQAzdx1TdqGB2ncQO45TAF3dPUz70i+4PiY02egfGOSam39dEaEBH9k4TsNwxnd/w/2PJY+TTHxxE1ctvBBe3lx2oQEf2ThOQ3Bx15qCheaGhRey24vP8okzr2Dv/3mOo+cto6u7fMvVfGTjOHVOV3fPMH9MPlJCs+sLz/Lh93+J5e37AWl+nGkZd/8UhY9sHKeO6eruGRKIJMSF5qxZl7G848Bh/f0Dg8xfUtweqGy42DhOHXPp4rX0DwzmPKetNfqZpwvNij0Oynj+k33lKTLqYuM4dUpXdw99/QN5zxu/w3aJhQZg9/a2Upo5hIuN49Qply5em/ecjvY2Bjb0ZBUapZ3f1trCnBlTSmxphDuIHadOSF8RnGRU80oeoTnjqL2yryouMS42jlMHpBzBKf9MTwK/Sq6pU0poLp9ZXI6aQnCxcZw6YP6SdXkdwXFyCU1HmUcw2XCxcZw6oJAIUb4Rzf1zjxt2fvr0rFxC5A5ix6kDkkaI8kWd0u+Tmp719PVjvLqwrxwriX1k4zg1SnzEsVNbK60tYmAw+/bKfELTOkYjIk2ZpmephX2lHt1Uu0jduyWtC0Xo5mbov0rSqvD6o6S+WN9grG9xZS13nPKSPuLo6x8AgzHpsepAknU08087ZISAZJuelWNhX9VGNpJagG8B7yIqxfKgpMVm9nDqHDP7TOz8TwLTYrfoN7NDK2Wv41SSTCOOga2ZRzX5hGaM4MpZh2Ycqeze3pYxslWOhX3VHNm8CVhvZo+b2SvADURF6bIxG1hYEcscp8okCW1DZqE586i96GhvQ0SRp2xCAzBnxhTaWluGtZVrYV81xaaQQnOTgL2BZbHmsaH43HJJM7N9iRepc+qNru6eESt7U7S3tQ6JQ7YRzeUzp3L/3OO46v3RwP8zN67Kmj5i5rQOvvzeqcPE6cvvnVqWaFS9OIhPBxaZWXxcOcnMeiTtAyyTtMbMHku/0MwWAAsAOjs78yUvc5yqM3/JuqxZ9l7ZMsj7Dt+Du+5ezY8zCE1HmP5kWgSYLX3EzGkdFVlzU82RTSGF5k4nbQplZj3h/XHgXob7cxynbsnlnH1pYCtL71rFrbd8YYTQxKc/uaJM1aKaYvMgsL+kvSVtSyQoI6JKkg4AxgG/ibWNk7Rd+DwBOBp4OP1ax6lHcjlnJ764iZ8svJAdnnmKlVf/hN6DOzNOfyoZZUpK1aZRZrZF0nlElSxbgGvMbK2kLwErzCwlPKcDN4QaUineAHxH0lYiwZwXj2I5Tj0zZ8aUYVOgFHEfzQdnXcaiD5/C/VnuUckoU1KqXaTudqKql/G2L6YdX5rhul8DldtB5jhlINs2gdTo5LM3rWYw/D+2kHw0kFmwypk+Igm+XcFxqkBXdw9zbl49bJvAp29cxcVdrzpxvz7rECCz0IzbvjXn/SsZZUpKvUSjHKehuHTx2oyL9K5f/lc6J40fGuE80r2O0+YMF5rWFnHJiblHNlC5KFNSfGTjOFUgV+KroQx8vb3M/eq/MKl/E5/78DxW7nEQHe1tzD915LaDesBHNo5TY/T1D3DHL37HgWfOZELfRj734XnM+PipfLsOBSaOi43jVIFx27ey+aXMo5uJL25iygfOZcLzYerUvh/3lrGeU6XwaZTjVIH3vHG3jO0pZ/Auzw+POlV7QV4p8JGN45SQru4eLrt17dCopb2tlUtPOmjEiOS23/eOuDYedTr7tEtHhLeruSCvFLjYOE6J6OruYc6i1cMSXPX1DzDn5tXA8ClQ+hQqXWge3PPgEfev5oK8UuDTKMcpEfOXrMuYSW9gq+WcAmUSmkrWc6oULjaOUyJyTXPS+9rbokV52YQmLlkC3nd4ba2ZGQ0uNo5TInJNc9L7Lj3pIHbJMnVKHxsZcM8jUS6mru4ejp63jL3n3pY1R02t4j4bxymQi7vWsPCBJxg0o0Vi9pF7cvnMqcyZMWWEzwYyJxqfuesY3nHbZbSETZWpLQjZwuFP9vUXlKOmFnGxcZwCuLhrDdcv/+vQ8aDZ0HGqumTeaFRvL0yfzms2PgXLlrLomGOGuo6etyzrbu1KVkIoBy42jpOQru6eYUITZ+EDT3D5zKl59yONWBm8w97Ec9rm2q39mRtXZbxnvYTEXWwcJwGpKUw2Bm1kFCq97tNOfc9yzXWfZ8IL2VcGp94zpZ6Yv2RdzeWoKQQXG8fJQUow8lU7aNHwYHW6f6V149NckyEfTaZpULbRUS3mqCmEWi9Sd7akjbFidB+J9Z0l6dHwOquyljvNQLxQXD5mH7nnsOO4fyVf4quk06BazFFTCDVdpC5wo5mdl3bteOASoJMoMrgyXLu5AqY7TUImh2wmzjxqryHncIqUQCXJsFfINKjWctQUQjWnUUNF6gAkpYrUJcklPANYamabwrVLgXfjReycEpJvRNPW2pJxZJGq+zQhz16nFNMPmJjze7KlD6036qFI3fsk/V7SIkmpsWohBe68SJ1TMLkKxaUY25r55zN/yboRQpNpr1OK1IK9bHbEa36n1tbU02K+FLW+gvhWYLKZvRFYCvyw0BuY2QIz6zSzzokTc/8fxHFS5CoUl2LzSwMZf/gDG3oSCw3k9tnUYv2n0VLTRerM7G9m9nI4/B5weNJrHacYkjptR/zwe3u5+aaLEgsN5PbZ1GL9p9FS00XqJMUzDJ0E/CF8XgIcH4rVjQOOD22OUxJ2astdvSBOT18/R89bxpvO+xF/OeRIdn3hWT4++9+HCU22KZkgZ+g6mxDVy9qaOFUTGzPbAqSK1P0BuClVpE7SSeG0T0laK2k18Cng7HDtJuDfiQTrQeBLKWex4xRLV3cPf39lS+LzBbyyoYeFCy9kQt9GPjzrS0w+ZcawEHW2KZmRe1/TnBlTaGttGdZWT2tr4sgyrHxsVDo7O23FihXVNsOpcbLtT8pGpvB2R3sb9889Lu8908/LRD1FoyStNLPOTH2+gthx0ijEH5JtHU36PYpZ/VvPa2vi5BQbSe9NcI9/hDK6jlOzFDI6yFYnO52JL25iYZYFe0Y0mol/z9jWMUNi09Y6hrGtY/jMjauYv2RdTY9WSkXOaZSkvwE/J7t/C+CtZrZvqQ0rBz6Nak7S9ylB9gV5qfM/nWWHdYqU0OyWp/Z2W2sL7zu8g5+u7Mm5Grm1RXVbfC5OrmlUPrG53szOzHPzvOfUCi42zclo/CUHffFO/v5KZnFIKjSFMm77Vrq/eHxJ7lUtcolNzmhUEhGpF6FxmpfRrFW54pSpGdtTPppSCw2MrLjQaBTlszGzn5XWHMcpPdl8MJnWqsR9O+mJx/NtqmwdAwNbS2x8A5EvGnVi2udbY8cGuNg4NU/SSFAm306KXEIjwRlH7kXnpPFZr09CewELCeuRnGJjZh9KfZbUHT92nHohV/a7ONlSSuQb0ey+U9uwFBOfvWl1xsx9uRijqOJCI1PIOpvmWf3nNBxJ1qpkmmolyUcT9/2kvqPQEc5rx7bWfSQqH76oz3ECLdKwEUm+2tsp0vdRxUdSPX39I+6bief6G9s5DPkdxLfy6ohmH0nDNkqa2Ukjr3Kc2id9kd/0AybmFJpcu7f//soWurp7cuYRzrcFoh43VhZKvpHN12Kfv15OQxynWJKuEk6v/dTT1z/suBChARgYtLy1mzI5qVPU68bKQsknNg8DE9PzAks6EPC0d07ViVc/iIeqe/r6+fSNq/j0javoiAlPrtpPULjQpMi3nyrb1KqjxjdWlpJ8YvOfwH9naN8ZuBj4QMktcpyEpIeqs3lF4mVqL7t1bdb7jVZoINk0qFE2VI6WfPls9jOzX6Y3mtmvgDeWxyTHSUbS6gcQZdSbc/OqrKt006NOhQgN5E6A5UTkE5vX5OgregVSgrpR50t6OCQ8v1vSpFjfYKye1OL0a53Gp9DUmNlW9yYJb+dj/pJ1dZmEvJLkE5v1kv4pvVHSCcDjxXxxrG7UCcCBwOzgC4rTDXSGhOeLgK/G+vrN7NDw8qhYE1KKCE4Sodlh25YMVw6nnqseVIp8YvNp4P9JulbSJ8Prh8A3gH8r8ruH6kaZ2StAqm7UEGZ2j5m9FA6XEyU2dxwgmrq0tuQruJKdXPlo4rRvv22i+9Vr1YNKkW/X96PAVOA+YHJ43Qe80cz+WOR3J679FDgHuCN2PDbUg1ouaWa2i7xuVIMzynXthaSJKCRFaD1WPagUeVcQh1IqP0gdS9oBOFXSbDN7TzmNi33nmUSldt8Wa55kZj2S9gGWSVpjZo+lX2tmC4AFEOWzqYS9TmW4dPFaBrYW/p+0XPlooDkW542WRNsVQqmV9xCFumcAPwWuLvK7E9V+kvRO4CLgbbEaUphZT3h/XNK9wDRghNg4jUV8Xc1oKIUzOBvNsjhvtOTbrnA8MJuoLtM9wHXAESXa/T1UN4pIZE4nbd2OpGnAd4B3m9kzsfZxwEtm9rKkCcDRDHceO3VE0pW/Xd09zFm0moHB0Q1QSyk0HWGLwz2PbKyLqge1QL6RzZ3Ar4BjzOxPAJK+UYovNrMtklJ1o1qAa1J1o4AVZrYYmA/sCNwsCeCvIfL0BuA7krYS+Z3mpa9yduqD9IV58QV46T/cy25dW3ahSU+YlU6u3MVObvKJzWFEI467JD1OFDHKHwdMSKjKcHta2xdjn9+Z5bpfEzmunTonVy3r9B/0aNNmJhWaVHLy+GjFRy+lI1/yrFXAKmCupLcQTalaJd0B3BKcr44zarJFb3r6+kfspB4NhUydfMRSXhKX3zWzX5vZJ4kcuVcBR5XNKqdpyBW9SV8kV2jazKT5aCDywbjQlJecYiNp1/Q2M9tqZr8wsw9nO8dxkpKplnWK/oFBLl386sbJQtJmFrKp0qNIlSHfyCZJpUuvhumMmpnTOvjye7O73/r6B4ZGNzOndTBu+/yjmyRCM277VkQ0ovHpU2XI5yA+RNLzOfoF5Op3nLzMnNaRc+1M3Fl8yYkH5Qx/JxGaM4/aa1iCcqcy5HMQlyzy5Di5mDNjStaStyOcyFli09mEpkViq5lHk6qMJzx3aoKZ0zq47Na1GcPbcSfy/CXrMm5RyBV12mrGn+ZVZGeNk4PE0SjHKTeXnHjQCGdxuvN2NOVWfL9SbeAjG6dmSFJMLle5lUxC45Gm2iHpRsx9gQ1hL9LbiVKCXmdmfeU0zmk+8uXpTReaXPlomimZeD2QdGTzU6BT0n5E6Rp+DvwEGJHFz3HKSUd7Gz19/VnTREhw1axDXWBqkKQ+m61mtgU4BfhPM5sD7FY+sxwnM3NmTGGvfzyXUWhax8iFpoZJOrIZkDQbOAs4MbQVnfDccQpl5q5jeMfPL6HlxWf54KzL6N7zYGiy+kv1SlKx+RBwLnCFmf0p5KD5UfnMcpwM9PbC9Om8ZmMv3L2URcccU22LnAJIJDYhV8ynYsd/Ar5SLqMcZwRBaNiwAe68E1xo6o6k0aijgUuBSeEaAWZm+5TPNMcJuNA0BEkdxN8HrgSOAY4gSj5+RLFfnqBI3XaSbgz9D0iaHOu7ILSvkzSjWFucGsWFpmFI6rN5zszuyH9acmJF6t5FVMblQUmL09J7ngNsNrP9JJ1ONHV7fyhmdzpwELA7USbB15tZslqsTn0QF5o77nChqXOSjmzukTRf0pslHZZ6FfndeYvUheMfhs+LgHcoSkZ8MnCDmb0c/Efrw/2cRiFdaI49ttoWOUWSdGRzZHjvjLUZcFwR352pSN2R2c4JCdKfA3YO7cvTrs0Y85T0MeBjAHvttVcR5joVw4WmIUkajZpebkPKhRepqzNcaBqWRNMoSTtJujJVxlbS1yXtVOR3JylSN3SOpG2AnYC/JbzWqTdcaBqapD6ba4AXgFnh9TyxkryjZKhIXai4eTqwOO2cxUSrlgFOBZaZmYX200O0am9gf+C3RdrjVJP0qJMLTcOR1Gezr5m9L3Z8maTMadUSkrBI3feBH0laD2wiEiTCeTcBDwNbgH/1SFQd4+HtpiCp2PRLOsbM/heGFvmNrthyjARF6v4BnJbl2iuAK4q1wakyLjRNQ1Kx+QTww+CnEdEo4+xyGeU0Cb29cNxxLjRNQtJo1CqiSguvDcdeUcEpjpTQPPGEC02TkFNsJJ1pZtdLOj+tHQAzu7KMtjmNigtNU5JvZLNDeH9Nhj5fs+IUjgtN05KvbtR3wse7zOz+eF9wEjtOctwZ3NQkXWfznwnbHCczLjRNTz6fzZuBtwAT0/w2ryVaG+M4+XGhccjvs9kW2DGcF/fbPE+0otdxcuNC4wTy+WzuA+6TdK2Z/aVCNjmNQh0KTVd3T84iec7oSeqz+Z6k9tSBpHGSlpTJJqcRqMPEV13dPVzwszX09PVjRKV+L/jZGrq6fY9vKUgqNhPi1S/NbDPwuvKY5NQ9dbp7e/6SdfQPDN9i1z8wyPwl66pkUWORuEidpKHMU5Im4etsnEzUqdAAPNmXebtftnanMJLujboI+F9J9xHtjTqWkP3OcYaoY6EB2D2U9s3U7hRPopGNmd0JHAbcSJQr+HAzc5+N8yoNkI9mzowptLUOX9HR1trCnBlTqmRRY5Fvnc0BZvZILLn5k+F9L0l7mdnvymueUxfUYdQpE6mok0ejykO+adRngY8CX8/QV2zCc6cRyCI09RpCnjmtoy7srEfyrbP5aHgvacJzSeOJpmSTgT8Ds0KEK37OocC3iVYrDxLVGb8x9F0LvA14Lpx+dkiD4ZSJM777G+5/bNPQ8dH7jufH/zw5Yz6aVAg5FdlJhZAB/yE3MfmmUe/N1W9mPxvl984F7jazeaES5lzg82nnvAR80MwelbQ7sFLSklgIfo6ZLRrl9zsF8K4r7+XRZ/4+rO2Pq9fT88XZdLzw7IipU64QsotN85JvGnVieH8d0R6pZeF4OvBrYLRiczLw9vD5h8C9pImNmf0x9vlJSc8AE4E+nIpxxnd/M0JoJr64iYULL6T9hWdh2dIRPpp6CiHX63SvHsk3jfoQgKRfAAeaWW843g24tojv3SV1L+ApYJdcJ0t6E9E+rcdizVdI+iJwNzDXzF7Ocq0XqUtAph/dir9sGjZ1gleFZrcXnuWsWZexKIMzuF5CyD7dqyxJF/XtGRMHgKeBnL9cSXdJeijDa1iJ3VCaJesCwSBsPwI+ZGZbQ/MFwAHAEcB4Rk7B4vdfYGadZtY5ceLEXCY3LV3dPcxZtHrYMv3zb1rF9cv/Ouy8iS9u4oaY0KzY46CM96uXELKvGK4sSRf13R32Qi0Mx+8H7sp1gZm9M1ufpKcl7WZmvUFMnsly3muB24CLzGyo3G5M+F6W9APgcwmfw8nAZbeuZWBwuN5vTZP/lNDsmiY0k+fexrjtW7nkxIOGRgP1EkKup+leI5A04fl5kk4B3hqaFpjZLUV8b6r43Lzw/vP0E0LhuluA69IdwTGhEjATeKgIW5qezS8N5OzPJjTx6+csWg0wTHBqTVzSqZfpXqOQdBoF8DvgNjP7DLBEUqa8xEmZB7xL0qPAO8MxkjolfS+cM4tI3M6WtCq8Dg19P5a0BlgDTAAuL8KWpubirjU5+/MJTYqBQau76Ue9TPcaBUUukzwnSR8lcrKON7N9Je0PXG1m7yi3gaWks7PTVqxYUW0zaop9L7idwSz/BpIKTRxBzU6bMuHRqNIiaaWZdWbsSyg2q4A3AQ+Y2bTQtsbMppbU0jLjYjOSyXNvy9geF5qzT7uUB/c8eFT3b5EYNKPDf8hNQS6xSeogftnMXknVi5K0DZ5iomEpldAAQ6OmbGFlH1k0D0l9NvdJuhBok/Qu4Gbg1vKZ5VSCTP6aUgpNOulhZc+M11wkFZvPAxuJHLIfB24HLi6XUU75ubhrTdZ1NOUQmhTxsLKvc2ku8k6jJLUAa83sAOC75TfJKRepKUumcO9onMGjIR5W9nUuzUXekY2ZDQLr4mlBnfojPmVJpxih6ShgTUp6WDnbehZf59KYJHUQjwPWSvotMLQrz8xOKotVTsm57Na1I6YsULzQ3D83SmkUd/S2b9+KGfT1D+SMRs2ZMWXY3iTwdS6NTFKx+UJZrXDKSld3T8ZVwsUITboojGbFcL1sa3BKQ758NmOBc4H9iJzD3zezLZUwzCkdmRyuhQrN/q/bgZde2VpyUaiHbQ1Oacg3svkhMAD8CjgBOBD4t3Ib5ZSWdIdrIUIzBrjy/Ye6IDhFk09sDkytEpb0feC35TfJKTXxDYdJhaZF4uuzDnGRcUpGvmjU0ETfp0/1S8q3UsiIxoXGKTX5xOYQSc+H1wvAG1OfJT1fCQOd4pk5rYPXpS3YyyU07W2tLjROycmXFrQlV79Tm6TvNzrxdXDDDReyS8KVwX39Axw9b5lHhpySkjT07dQJ6Xl1X9nQw2nzkwtNCs/H65SaQpJnlQxJ4yUtlfRoeB+X5bzBWOKsxbH2vSU9IGm9pBtDVj+H4fuNit3r5PuUnFJSFbHh1bpR+xOqI2Q5r9/MDg2v+GrlrwBXmdl+wGbgnPKaWz88mSHqVMymSt+n5JSKaonNyURreAjvM5NeGPIOHwek8hIXdH2js3t724ioUzG7t32fklMqquWzSVo3aqykFcAWYJ6ZdQE7A32xUPwGIKtToZHrRl3ctYaFDzzBoBktErOP3JOLD2tnytfOYpcCtiC0tY6hf2ArYnhGNN+n5JSSsomNpLuAXTN0XRQ/MDOTlC3r3yQz65G0D7AsJDl/Lsu5GTGzBcACiNKCFnJtrZCtgFw8H82gGUvuWsWnui5h/Eub+OSH57GyfT862tuYfsBE7nlkI0/29TO2dQwvb9nKVmNIoC6fOTXr97hz2CkVZRObUtSNMrOe8P64pHuBacBPgXZJ24TRzR5Aw6Z2y1a18eUtw3dwpypV7vDCs2yzbCnfzlCpMh++T8kpJ9Xy2aTqRkH2ulHjJG0XPk8AjgYeDhU07wFOzXV9o5Atm128iFx6Sdz02tuOUwtUSxwjLEoAAArQSURBVGyS1I16A7BC0moicZlnZg+Hvs8D50taT+TD+X5Fra8g+aJB6ULTXYZUno5TCqriIDazvwEjak6Z2QrgI+Hzr4GMpWLM7HGi0jINRSafSbaqjW2tY9hx87Mj9jqdeeSeVbDccfJTrZGNk0a2SgPTD5iYsWrjlcfuwu1dXxwSmu49D+bMo/YacvY6Tq3h2xVqhGy+mXse2ciX3zt12Ijn4sPaOeFTH4C+jbBsKYvcR+PUAS42NUKuSgPDokS9vTB9OmzYAHfe6c5gp27waVSNkKjSgAuNU8e42NQIc2ZMyeibGVrBGxeaO+5woXHqDp9G1Qg5Kw2kC82xx1bZWscpHBebGiLjCl4XGqdB8GlULeNC4zQQLja1iguN02D4NKoW8aiT04D4yKbWcKFxGhQXm1qitxeOO86FxmlIXGxqhZTQPPGEC43TkLjY1AIuNE4T4GJTbVxonCbBo1HVxJ3BThNRs0XqJE2PFahbJekfkmaGvmsl/SnWd2jln6JIXGicJqNmi9SZ2T2pAnVEdaJeAn4RO2VOrIDdqopYXSpcaJwmpF6K1J0K3GFmL5XVqkrgQuM0KdUSm6RF6lKcDixMa7tC0u8lXZWqwpAJSR+TtELSio0bNxZhcglwoXGamLKJjaS7JD2U4XVy/LxQmiVr8bhQV2oqsCTWfAFwAHAEMJ6o2kJGzGyBmXWaWefEiROLeaTi8Hw0TpNT00XqArOAW8xsIHbv1KjoZUk/AD5XEqPLhW+qdJzaLVIXYzZpU6ggUEgSkb/noTLYWBpcaBwHqO0idUiaDOwJ3Jd2/Y9D3e81wATg8grYXDguNI4zRM0WqQvHfwZGFJ82s+PKaV9JcKFxnGH4CuJy4FEnxxmB740qNS40jpMRF5tS4kLjOFlxsSkVnvjKcXLiYlMKPE2E4+TFxaZYXGgcJxEejSoG99E4TmJ8ZDNaXGgcpyBcbEaDC43jFIyLTaG40DjOqHCxKQQXGscZNS42SfF8NI5TFC42SfBNlY5TNC42+XChcZyS4GKTCxcaxykZLjbZcKFxnJJSrSJ1p0laK2mrpM4c571b0jpJ6yXNjbXvLemB0H6jpG1LamB61MmFxnGKplojm4eA9wK/zHaCpBbgW8AJwIHAbEkHhu6vAFeZ2X7AZuCcklnm4W3HKQtVERsz+4OZrctz2puA9Wb2uJm9AtwAnBySnB8HLArnJSlyl4ynn3ahcZwyUcs+mw7gidjxhtC2M9BnZlvS2jNSUJG617wGDjjAhcZxykDZdn1LugvYNUPXRWaWq3RLSTGzBcACgM7OzqzF8ADYfnvo6qqEWY7TdFSlSF1CeojKuKTYI7T9DWiXtE0Y3aTaHcepYWp5GvUgsH+IPG1LVO97cSjXew9wajgvX5E7x3FqgGqFvk+RtAF4M3CbpCWhfXdJtwOEUct5RDW+/wDcZGZrwy0+D5wvaT2RD+f7lX4Gx3EKQ9FAoTno7Oy0FStWVNsMx2lYJK00s4xr52p5GuU4TgPhYuM4TkVwsXEcpyK42DiOUxGaykEsaSPwlwSnTgCeLbM55aYRngEa4zma6RkmmdnETB1NJTZJkbQim0e9XmiEZ4DGeA5/hgifRjmOUxFcbBzHqQguNplZUG0DSkAjPAM0xnP4M+A+G8dxKoSPbBzHqQguNo7jVAQXG4pPwF4LSBovaamkR8P7uCznDUpaFV6LK21nJvL9XSVtFxLbrw+J7idX3sr8JHiOsyVtjP39P1INO3Mh6RpJz0h6KEu/JH0zPOPvJR2W+OZm1vQv4A3AFOBeoDPLOS3AY8A+wLbAauDAatses++rwNzweS7wlSznvVhtWwv9uwL/AlwdPp8O3Fhtu0f5HGcD/1VtW/M8x1uBw4CHsvT/E3AHIOAo4IGk9/aRDcUlYC+/dYk5mSj5O5QyCXz5SfJ3jT/bIuAdIfF9LVHr/z4SYWa/BDblOOVk4DqLWE6UNXO3JPd2sUlOtgTstcIuZtYbPj8F7JLlvLEhAfxySbUgSEn+rkPnWJRU7TmipGm1RNJ/H+8L049FkvbM0F/rjPp3ULYcxLVGrSRgL4ZczxA/MDOTlG1NwyQz65G0D7BM0hoze6zUtjoZuRVYaGYvS/o40WjtuCrbVDGaRmysfAnYK0auZ5D0tKTdzKw3DGufyXKPnvD+uKR7gWlEvoZqkeTvmjpng6RtgJ2IEt/XEnmfw8ziNn+PyM9Wb4z6d+DTqORkTMBeZZviLCZK/g5ZksBLGidpu/B5AnA08HDFLMxMkr9r/NlOBZZZ8FbWEHmfI823cRJRbu16YzHwwRCVOgp4LjZ9z021vd+18AJOIZp7vgw8DSwJ7bsDt6d54v9INBK4qNp2pz3DzsDdwKPAXcD40N4JfC98fguwhihSsgY4p9p2Z/u7Al8CTgqfxwI3A+uB3wL7VNvmUT7Hl4G14e9/D3BAtW3O8AwLgV5gIPwmzgHOBc4N/SIqi/1Y+DeUMXqb6eXbFRzHqQg+jXIcpyK42DiOUxFcbBzHqQguNo7jVAQXG8dxKoKLjeM4FcHFpomQtHMsvcFTknpix9uW4P6XSPpyWtuhkrIuXpN0qaTPFfvdOe7/Z0lr4qlDJE2QNCDp3LRzd5V0g6THJK2UdLuk10uanJ5yIW63pPnh71m252gEmma7gjO0XP5QiH4sROkmvpbql7SNRRsdR8tC4E7ggljb6aG9mkw3s3jNo9OA5cBs4GqI8rQAtwA/NLPTQ9shRBtanyAHZjZH0t/LYXgj4SObJkfStZKulvQA8NX0kYakh1LJqiSdKem3YST0HUkt8XuZ2R+BzZKOjDXPAhZK+qikByWtlvRTSdtnsOXe1AgkjD7+HD63hNHDg2HH9MdD+26SfhnseUjSsQkfezbwWaBD0h6hbTowYGZXx55ntZn9KuE9nTy42DgQbaZ7i5mdn+0ESW8A3g8cbWaHAoPAGRlOXUg0miHsndlkZo8CPzOzI8zsEKI9QecUYN85RHtwjgCOAD4qaW/gA0RbSw4FDgFW5btRSOuwm5n9FrgpPBPAwcDKHJfuG5tyriJawu8UgE+jHICbzWwwzznvAA4HHgx5q9rIvLP8RuDXkj7L8CnUwZIuB9qBHYElBdh3PPBGSaeG452A/Yk2P14jqRXoMrO8YkMkLjeFzzcA1wBfT3DdY0HUgKFpqFMALjYOQNzfsIXhI96x4V1E/oy4P2YEZvaEpD8BbwPeB7w5dF0LzDSz1ZLOBt6e4fL4d4+NtQv4pJmNEChJbwXeA1wr6Uozuy6XfURTqF0lpUZlu0van2iD5KnZL3OKxadRTjp/JspBi6Jk1nuH9ruBUyW9LvSNlzQpyz0WAlcBj5vZhtD2GqA3jEIyTb9S3314+Bz/4S8BPhGuJUSIdgjf/7SZfZcoP0zO5NuSXg/saGYdZjbZzCYT7cSeDSwDtpP0sdj5byzAD+TkwcXGSeenwHhJa4HziFImYGYPAxcDv5D0e2ApkC337M3AQQyPQn0BeAC4H3gky3VfIxKVbmBCrP17RHl3fhdC0N8hGpW/HVgdzn8/8I08zzabKOKU/ryzLUp/cArwzhD6XkskRE/luaeTEE8x4TQ0IaLVmRb6Lsf3XEraUgJnOD6ycRqdjcDdylEPrFgkzQfOZLjvy0nDRzaO41QEH9k4jlMRXGwcx6kILjaO41QEFxvHcSrC/wcsDJameYclgwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.eval()\n",
        "test_predictions = model(test_data.x, test_data.edge_index)\n",
        "# test_predictions = model(test_data.x)\n",
        "test_predictions_inverse = scalerY.inverse_transform(test_predictions.detach().cpu().numpy())\n",
        "test_Y_inverse = scalerY.inverse_transform(test_Y)\n",
        "prediction_frame = pd.DataFrame(test_predictions_inverse, index=dataset_test.index)\n",
        "# test_2 = pd.concat([pd.DataFrame(test_X.detach().numpy()), pd.DataFrame(test_Y.detach().numpy())], axis=1)\n",
        "result = pd.concat([dataset_test, prediction_frame], axis=1)\n",
        "result.rename(columns={0:'Prediction'},inplace = True)\n",
        "# result = test.join(prediction_frame)\n",
        "# pd.merge(test, prediction_frame, left_index=True)\n",
        "result.to_csv('result_Analysis_MAE.csv')\n",
        "\n",
        "writer = pd.ExcelWriter('result_analysis.xlsx')\n",
        "result.to_excel(writer, 'Sheet1')\n",
        "writer.save()\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.title('Testing Set')\n",
        "plt.scatter(test_Y_inverse, test_predictions_inverse)\n",
        "plt.xlabel('True Values [ACH]')\n",
        "plt.ylabel('Predictions [ACH]')\n",
        "x = np.linspace(-1,1)\n",
        "y = x\n",
        "plt.plot(x, y, '-r', label='y=x')\n",
        "plt.axis('equal')\n",
        "plt.axis('square')\n",
        "# plt.show()\n",
        "fig.set_size_inches(6, 4)\n",
        "fig.savefig('MAE.png', dpi=fig.dpi)\n",
        "# plt.xlim([-0.1, 55])\n",
        "# plt.ylim([-0.1, 55])\n",
        "# _ = plt.plot([-100, 100], [-100, 100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wyhma1NmlRc"
      },
      "outputs": [],
      "source": [
        "def train_and_pred(validation_data, validation_labels):\n",
        "  model.cpu()\n",
        "  preds = model(validation_data.x, validation_data.edge_index)\n",
        "  preds = scalerY.inverse_transform(preds.detach().numpy())\n",
        "  validation_labels['Cp'] = pd.Series(preds[:,0].reshape(1, -1)[0])\n",
        "  submission = pd.concat([validation_labels['Id'], validation_labels['Cp']], axis=1)\n",
        "  submission.to_csv('./Validation Results.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XZzqwbrmvnW"
      },
      "outputs": [],
      "source": [
        "validation_labels = pd.DataFrame(scalerY.inverse_transform(validation_Y.detach().numpy()))\n",
        "validation_labels.columns = ['Id']\n",
        "train_and_pred(validation_data, validation_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "Pi30P1_QYD6Z",
        "outputId": "03e74689-0f5d-43e3-b73a-9fe5414794fa"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'W1 Cp'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-845a3b11e011>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mMAE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'result_Analysis_MAE.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# ACH_MAE = MAE['ACH']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mACH_MAE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1 Cp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mPRED_MAE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mabsolute_diff_MAE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mACH_MAE\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mPRED_MAE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'W1 Cp'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def drawComparison (ACH, PRED):\n",
        "    fig = plt.figure()\n",
        "    plt.title('Testing Set', FontSize=24)\n",
        "    plt.scatter(ACH, PRED)\n",
        "    plt.xlabel('True Values [ACH]', FontSize=24)\n",
        "    plt.ylabel('Predictions [ACH]', FontSize=24)\n",
        "    plt.xticks(fontsize=20)\n",
        "    plt.yticks(fontsize=20)\n",
        "    x = np.linspace(-5,60,20)\n",
        "    y = x\n",
        "    plt.plot(x, y, '-r', label='y=x')\n",
        "    plt.axis('equal')\n",
        "    plt.axis('square')\n",
        "\n",
        "    fig.set_size_inches(16, 9)\n",
        "    fig.savefig('comparison', dpi=fig.dpi)\n",
        "\n",
        "MAE = pd.read_csv('result_Analysis_MAE.csv')\n",
        "# ACH_MAE = MAE['ACH']\n",
        "ACH_MAE = MAE['W1 Cp']\n",
        "PRED_MAE = MAE['Prediction']\n",
        "absolute_diff_MAE = abs(ACH_MAE-PRED_MAE)\n",
        "absolute_percentage_diff_MAE = abs(ACH_MAE-PRED_MAE)/abs(ACH_MAE)\n",
        "drawComparison(ACH_MAE, PRED_MAE)\n",
        "\n",
        "# Rating_GT = MAE['ACH']\n",
        "# Rating_Pred = MAE['ACH']\n",
        "# is_equal = MAE['ACH']\n",
        "Rating_GT = MAE['W1 Cp']\n",
        "Rating_Pred = MAE['W1 Cp']\n",
        "is_equal = MAE['W1 Cp']\n",
        "\n",
        "Output = pd.concat([ACH_MAE, PRED_MAE, absolute_diff_MAE, \n",
        "                    absolute_percentage_diff_MAE, Rating_GT, Rating_Pred, is_equal],axis=1,ignore_index=True)\n",
        "# Output.columns=['ACH','PRED_MAE','absolute_diff_MAE',\n",
        "                              # 'absolute_percentage_diff_MAE','Rating_GT', 'Rating_Pred']\n",
        "\n",
        "Output.columns=['ACH','PRED_MAE','absolute_diff_MAE',\n",
        "                              'absolute_percentage_diff_MAE','Rating_GT', 'Rating_Pred','is_equal']\n",
        "\n",
        "# Output.loc[Output['ACH'] <=5, 'Rating_GT'] = 0\n",
        "# Output.loc[(Output['ACH'] >5) & (Output['ACH'] <=10), 'Rating_GT'] = 1\n",
        "# Output.loc[(Output['ACH'] >10) & (Output['ACH'] <=30), 'Rating_GT'] = 2\n",
        "# Output.loc[(Output['ACH'] >30) & (Output['ACH'] <=70), 'Rating_GT'] = 3\n",
        "# # Output.loc[(Output['ACH'] >44) & (Output['ACH'] <=70), 'Rating_GT'] = 4\n",
        "# Output.loc[Output['ACH'] >70, 'Rating_GT'] = 4\n",
        "\n",
        "# Output.loc[Output['PRED_MAE'] <=5, 'Rating_Pred'] = 0\n",
        "# Output.loc[(Output['PRED_MAE'] >5) & (Output['PRED_MAE'] <=10), 'Rating_Pred'] = 1\n",
        "# Output.loc[(Output['PRED_MAE'] >10) & (Output['PRED_MAE'] <=30), 'Rating_Pred'] = 2\n",
        "# Output.loc[(Output['PRED_MAE'] >30) & (Output['PRED_MAE'] <=70), 'Rating_Pred'] = 3\n",
        "# # Output.loc[(Output['PRED_MAE'] >44) & (Output['PRED_MAE'] <=70), 'Rating_Pred'] = 4\n",
        "# Output.loc[Output['PRED_MAE'] >70, 'Rating_Pred'] = 4\n",
        "\n",
        "# Output.loc[Output['ACH'] <=4, 'Rating_GT'] = 0\n",
        "# Output.loc[(Output['ACH'] >4) & (Output['ACH'] <=8), 'Rating_GT'] = 1\n",
        "# Output.loc[(Output['ACH'] >8) & (Output['ACH'] <=16), 'Rating_GT'] = 2\n",
        "# Output.loc[Output['ACH'] >16, 'Rating_GT'] = 3\n",
        "# Output.loc[Output['PRED_MAE'] <=4, 'Rating_Pred'] = 0\n",
        "# Output.loc[(Output['PRED_MAE'] >4) & (Output['PRED_MAE'] <=8), 'Rating_Pred'] = 1\n",
        "# Output.loc[(Output['PRED_MAE'] >8) & (Output['PRED_MAE'] <=16), 'Rating_Pred'] = 2\n",
        "# Output.loc[Output['PRED_MAE'] >16, 'Rating_Pred'] = 3\n",
        "\n",
        "# Output.loc[Output['Rating_GT'] == Output['Rating_Pred'], 'is equal'] = 1\n",
        "# count_Pred = Output['Rating_Pred'].value_counts()\n",
        "# count_GT = Output['Rating_GT'].value_counts()\n",
        "\n",
        "Output.to_csv('PaperResult.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "phu-LiQNbEIR",
        "outputId": "af40b5e3-15e5-4cf1-8a53-3d13c52ab9aa"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-62ab9ad6b08e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrue_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtotal_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Rating_GT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mOutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Rating_GT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Rating_Pred'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrue_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Output' is not defined"
          ]
        }
      ],
      "source": [
        "true_count = 0\n",
        "total_count = len(Output['Rating_GT'])\n",
        "for i in range(total_count):\n",
        "  if Output['Rating_GT'].iloc[i] == Output['Rating_Pred'].iloc[i]:\n",
        "    true_count += 1\n",
        "true_percent = true_count/total_count\n",
        "\n",
        "print(true_percent)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "history_visible": true,
      "name": "GCN 17 May_.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.0 ('mlds')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "8eb7b885f2208ae6b54ba7042d7c707c6e996b7bcbec7b4929157dd301d245e1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
